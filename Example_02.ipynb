{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Example_02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxn-jyAPdW8l"
      },
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqyi5e3sePDl"
      },
      "source": [
        "# Import libraries\n",
        "from numpy import linalg\n",
        "import pandas as pd\n",
        "# !pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn.init as init\n",
        "from torch import autograd\n",
        "\n",
        "from torch import nn, optim\n",
        "from time import time\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBN2LBiGxH2M"
      },
      "source": [
        "# Discritize the interval\n",
        "a = 0\n",
        "b = 1\n",
        "step = (b-a)/1000\n",
        "x_init_np = np.arange(start=a, stop=b+step, step=step)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBs4v4BS5U66"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y21HCpj9Itcd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def draw_result(lst_iter, lst_loss):\n",
        "    plt.plot(lst_iter, lst_loss, '-b', label='loss')\n",
        "    \n",
        "    plt.xlabel(\"n iteration\")\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title(\"Loss\")\n",
        "\n",
        "    # save image\n",
        "    plt.savefig(\"Loss\"+\".png\")  # should before show method\n",
        "\n",
        "    # show\n",
        "    plt.show()\n",
        "\n",
        "# plt.plot(test_error_vec)\n",
        "# plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXFtdOzWCDPj"
      },
      "source": [
        "def draw_graph(lst_iter, lst_loss, lst_acc, title):\n",
        "    plt.plot(lst_iter, lst_loss, '-b', label='true')\n",
        "    plt.plot(lst_iter, lst_acc, '-r', label='neural network')\n",
        "\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title(title)\n",
        "\n",
        "    # save image\n",
        "    plt.savefig(title+\".png\")  # should before show method\n",
        "\n",
        "    # show\n",
        "    plt.show()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjOZQK1vxHVO"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f5m6GiEWx8d"
      },
      "source": [
        "def Loss3(u_hat, second_derivative_u_hat, x_data):\n",
        "    # u_hat, u_star are n-dim pytorch tensor\n",
        "    # second_derivative_u_hat is also a n-dim pytorch tensor\n",
        "    result = torch.pow(torch.linalg.norm(second_derivative_u_hat[1:-1] + u_hat[1:-1] + torch.exp(x_data[1:-1]),ord = 2),2)/len(u_hat[1:-1]) + 1/2*(torch.pow(u_hat[0],2) + torch.pow(u_hat[-1],2))\n",
        "    # result = 1/len(x_data)*torch.pow(torch.linalg.norm(second_derivative_u_hat + u_hat + torch.exp(x_data)), 2) + 1000*1/2*(torch.pow(u_hat[0],2) + torch.pow(u_hat[-1],2))\n",
        "    return result"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh_YMgtuXJra"
      },
      "source": [
        "def run_train(lr = 0.001, num_e = 1000, Loss = Loss3, true_sol = None):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    time0 = time()\n",
        "    num_e = num_e\n",
        "    iter = []\n",
        "    test_error_vec = []\n",
        "    # print(model.parameters())\n",
        "    scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.1)\n",
        "    scheduler2 = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
        "    for e in range(num_e):\n",
        "        running_loss = 0\n",
        "        # call zero_Grad\n",
        "        optimizer.zero_grad() \n",
        "        # Compute u_hat, d2u_hat\n",
        "        u_hat = torch.tensor(np.array([]))\n",
        "        d2u_hat = torch.tensor(np.array([]))\n",
        "        for x in x_init_np:\n",
        "            x_tensor = torch.tensor(np.array([x]), requires_grad= True)\n",
        "            temp = model(x_tensor.float())\n",
        "            # u_hat.append(temp.clone().item())\n",
        "            u_hat = torch.hstack([u_hat, temp])\n",
        "            # first_derivative = autograd.grad(model(x_tensor.float()), x_tensor, create_graph=True)[0]\n",
        "            # second_derivative = autograd.grad(first_derivative, x_tensor)[0]\n",
        "            # # d2u_hat.append(second_derivative.detach().numpy()[0])\n",
        "            # d2u_hat = torch.hstack([d2u_hat, second_derivative])\n",
        "            # d2u_hat.append(second_derivative.clone().item())\n",
        "        d2u_hat = u_hat.clone()    \n",
        "        d2u_hat[1:-1] = (u_hat[0:-2] + u_hat[2:] - 2*u_hat[1:-1])/step**2     \n",
        "        # loss and backward\n",
        "        # print(u_hat)\n",
        "        # print(d2u_hat)\n",
        "        # u_hat = torch.tensor(np.array(u_hat))\n",
        "        # d2u_hat = torch.tensor(np.array(d2u_hat))\n",
        "        loss = Loss(u_hat,d2u_hat,x_init)\n",
        "        loss.backward()\n",
        "        # print(type(list(model.parameters())[0]))\n",
        "        # print(x_init.grad)\n",
        "        # optimizer.step()\n",
        "        # torch.nn.utils.clip_grad_norm(model.parameters(), 1)\n",
        "        optimizer.step() \n",
        "\n",
        "        # print(type(list(model.parameters())[0]))\n",
        "\n",
        "        running_loss += loss.clone().item()    \n",
        "        if (e % 100 == 0):\n",
        "            print(f\"The running loss at {e} iteration is: {running_loss}\")\n",
        "        test_error_vec.append(running_loss)\n",
        "        iter.append(e)\n",
        "        # print(\"----------------------------------------------------\")\n",
        "        # if (num_e % 2 == 0):\n",
        "        # scheduler1.step()\n",
        "        #     scheduler2.step()\n",
        "\n",
        "    draw_result(iter, test_error_vec)\n",
        "    # print(f\"The running loss at {num_e} iteration is: {test_error_vec[-1]}\")\n",
        "\n",
        "    if(true_sol):\n",
        "      y_true = true_sol(x_init_np)\n",
        "      draw_graph(x_init_np, y_true, u_hat.clone().detach().numpy(), 'Solutions')\n",
        "      print(np.mean(np.square(y_true -  u_hat.clone().detach().numpy())))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsl5RK-STnpf"
      },
      "source": [
        "def Loss4(u_hat, second_derivative_u_hat, x_data):\n",
        "    # u_hat, u_star are n-dim pytorch tensor\n",
        "    # second_derivative_u_hat is also a n-dim pytorch tensor\n",
        "    result = torch.pow(torch.linalg.norm(-second_derivative_u_hat[1:-1] + u_hat[1:-1] - torch.sin(4.0*np.pi*x_data[1:-1]),ord = 2),2)/len(u_hat[1:-1]) + 1/2*(torch.pow(u_hat[0],2) + torch.pow(u_hat[-1],2))\n",
        "    # result = 1/len(x_data)*torch.pow(torch.linalg.norm(second_derivative_u_hat + u_hat + torch.exp(x_data)), 2) + 1000*1/2*(torch.pow(u_hat[0],2) + torch.pow(u_hat[-1],2))\n",
        "    return result"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8ZuuzPhUmcl"
      },
      "source": [
        "def u_star_h(x):\n",
        "  return 1/(16*np.pi**2 + 1 )*np.sin(4*np.pi*x)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di0o5rUDUw61"
      },
      "source": [
        "# Discritize the interval\n",
        "a = 0\n",
        "b = 1\n",
        "step = (b-a)/1000\n",
        "x_init_np = np.arange(start=a, stop=b+step, step=step)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3n_c20aVD5t",
        "outputId": "95dca7d3-cf6c-4c48-dd05-61db4159c9bd"
      },
      "source": [
        "x_init_np = np.arange(start=a, stop=b, step=step)\n",
        "x_init = torch.tensor(x_init_np, requires_grad= True)\n",
        "# print(x_init)\n",
        "print(x_init.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCGssHpwVFDn",
        "outputId": "3799f047-83b0-44ed-c160-9ece26e38cb4"
      },
      "source": [
        "# u = sigma(W4(sigma(W3... x)))\n",
        "\n",
        "input_size = 1\n",
        "output_size = 1\n",
        "k = 10\n",
        "# hidden_sizes = [5, 5, 5, 5, 5, 5, 5]\n",
        "\n",
        "\n",
        "# module1 = nn.Linear(input_size, hidden_sizes[0])\n",
        "# init.xavier_normal_(module1.weight)\n",
        "\n",
        "# module2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
        "# init.xavier_normal_(module2.weight)\n",
        "\n",
        "# module3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
        "# init.xavier_normal_(module3.weight)\n",
        "\n",
        "# module4 = nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
        "# init.xavier_normal_(module4.weight)\n",
        "\n",
        "# module5 = nn.Linear(hidden_sizes[3], hidden_sizes[4])\n",
        "# init.xavier_normal_(module5.weight)\n",
        "\n",
        "model = nn.Sequential(nn.Linear(input_size, k),\n",
        "                      nn.Tanh(),\n",
        "\n",
        "                    #   nn.Linear(k, k),\n",
        "                    #   nn.Tanh(),\n",
        "\n",
        "                    #   nn.Linear(k, k),\n",
        "                    #   nn.SELU(),\n",
        "                      \n",
        "                    #   nn.Linear(k, k),\n",
        "                    #   nn.SELU(),\n",
        "\n",
        "                    #   nn.Linear(k, k),\n",
        "                    #   nn.Tanh(),\n",
        "\n",
        "                    #   nn.Linear(k, k),\n",
        "                    #   nn.Tanh(),\n",
        "                      \n",
        "                    #   nn.Linear(k, k),\n",
        "                    #   nn.Tanh(),\n",
        "\n",
        "                    #   nn.Linear(k, k),\n",
        "                    #   nn.Tanh(),\n",
        "\n",
        "                      nn.Linear(k, k),\n",
        "                      nn.Tanh(),\n",
        "\n",
        "                      nn.Linear(k, k),\n",
        "                      nn.Tanh(),\n",
        "\n",
        "                      nn.Linear(k, output_size),\n",
        "                      nn.Tanh(),\n",
        "\n",
        "                      )\n",
        "\n",
        "print(model)\n",
        "\n",
        "# criterion = nn.MSELoss(size_average=False)\n",
        "# criterion = nn.HingeEmbeddingLoss()\n",
        "run_train( lr = 0.01, num_e = 2000,Loss = Loss4,true_sol=u_star_h)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=1, out_features=10, bias=True)\n",
            "  (1): Tanh()\n",
            "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (3): Tanh()\n",
            "  (4): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (5): Tanh()\n",
            "  (6): Linear(in_features=10, out_features=1, bias=True)\n",
            "  (7): Tanh()\n",
            ")\n",
            "The running loss at 0 iteration is: 0.5935919909674743\n",
            "The running loss at 100 iteration is: 0.46080284981769104\n",
            "The running loss at 200 iteration is: 0.4530754720151159\n",
            "The running loss at 300 iteration is: 0.4474995555077421\n"
          ]
        }
      ]
    }
  ]
}