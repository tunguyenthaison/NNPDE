{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nxn-jyAPdW8l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je95UkAAeSz3"
      },
      "source": [
        "The equation we are testing is\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "u''(x) +u(x) = e^{-x}, \\qquad\\text{for}\\; 0<x<1,\\\\\n",
        "u(0) = 1,  \\qquad u\\left(1\\right) = \\frac{1}{2}\\cos(1) + \\frac{1}{2}\\sin(1) + \\frac{e^{-1}}{2}.\n",
        "\\end{cases}\n",
        "$$ \n",
        "\n",
        "The exact solution is\n",
        "$$ u^*(x) = \\frac{\\cos x+\\sin x + e^{-x}}{2} $$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The idea of the paper is, assume $$u = \\mathrm{NN}(x).$$\n",
        "We minimize the loss function\n",
        "$$ L(u^*, \\hat{u}) = \\mathrm{Mean}\\left(\\left\\Vert \\frac{d^2\\hat{u}}{dx^2} + \\hat{u}(x) - \\Big(- e^{x}\\Big)\\right\\Vert^2\\right) + \\frac{|\\hat{u}(0)|^2 + |\\hat{u}\\left(1\\right)|^2}{2}$$\n",
        "where all the derivatives are computed via \n",
        "* back-propagation - we tried this idea but it does not work well, maybe autograd in Pytorch does not perform it well, or the idea behind it is problematic, in the sense that the derivative obtained by back-propagation with second-order is not very nice.\n",
        "* finite difference scheme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oqyi5e3sePDl"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from numpy import linalg\n",
        "import pandas as pd\n",
        "# !pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn.init as init\n",
        "from torch import autograd\n",
        "\n",
        "from torch import nn, optim\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UMTkPFHHFihK"
      },
      "outputs": [],
      "source": [
        "def u_star_func(x):\n",
        "    result = 1/2*(np.exp(-x) + np.cos(x) + np.sin(x))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LBN2LBiGxH2M"
      },
      "outputs": [],
      "source": [
        "# Discritize the interval\n",
        "a = 0\n",
        "b = 1\n",
        "step = (b-a)/100\n",
        "x_init_np = np.arange(start=a, stop=b+step, step=step)\n",
        "x_init = torch.tensor(x_init_np, requires_grad= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TKC-9hQRFVL2"
      },
      "outputs": [],
      "source": [
        "bb = u_star_func(1)\n",
        "def Loss(u_hat, second_derivative_u_hat, x_data):\n",
        "    loss = nn.MSELoss()\n",
        "    result = loss(second_derivative_u_hat[1:-1] +u_hat[1:-1], torch.exp(-x_data[1:-1])) + 1/2*(torch.square(u_hat[0]-1) + torch.square(u_hat[-1]-bb))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxN0oTjcd76g",
        "outputId": "0b38cb3a-625e-4e9b-d86d-d912c2e836b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=1, out_features=5, bias=True)\n",
            "  (1): Tanh()\n",
            "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
            "  (3): Tanh()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "input_size = 1\n",
        "output_size = 1\n",
        "k = 5\n",
        "\n",
        "module1 = nn.Linear(input_size, k)\n",
        "init.xavier_normal_(module1.weight)\n",
        "\n",
        "module5 = nn.Linear(k, output_size)\n",
        "init.xavier_normal_(module5.weight)\n",
        "\n",
        "model = nn.Sequential(module1,\n",
        "                      nn.Tanh(),\n",
        "\n",
        "                      module5,\n",
        "                      nn.Tanh(),\n",
        "\n",
        "                      )\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "y21HCpj9Itcd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def draw_result(lst_iter, lst_loss, title='Loss'):\n",
        "    plt.plot(lst_iter, lst_loss, '-b', label='loss')\n",
        "    \n",
        "    plt.xlabel(\"n iteration\")\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title(title)\n",
        "\n",
        "    # save image\n",
        "    plt.savefig(title +\".png\")  # should before show method\n",
        "\n",
        "    # show\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tXFtdOzWCDPj"
      },
      "outputs": [],
      "source": [
        "def draw_graph(lst_iter, lst_loss, lst_acc, title):\n",
        "    plt.plot(lst_iter, lst_loss, '-b', label='true')\n",
        "    plt.plot(lst_iter, lst_acc, '-r', label='neural network')\n",
        "\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.title(title)\n",
        "\n",
        "    # save image\n",
        "    plt.savefig(title+\".png\")  # should before show method\n",
        "\n",
        "    # show\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7sqtXd41_1KQ"
      },
      "outputs": [],
      "source": [
        "# optimizer = optim.SGD(model.parameters(), lr=0.07, momentum=3.0)\n",
        "def run_train(lr = 0.001, num_e = 1000, isOn = True):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    time0 = time()\n",
        "    num_e = num_e\n",
        "    iter = []\n",
        "    test_error_vec = []\n",
        "    # print(model.parameters())\n",
        "    scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.1)\n",
        "    scheduler2 = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
        "    for e in range(num_e):\n",
        "        running_loss = 0\n",
        "        \n",
        "        optimizer.zero_grad() # call zero_Grad\n",
        "        u_hat = torch.tensor(np.array([])) \n",
        "        d2u_hat = torch.tensor(np.array([]))\n",
        "        for x in x_init_np:\n",
        "            x_tensor = torch.tensor(np.array([x]), requires_grad= True)\n",
        "            temp = model(x_tensor.float())\n",
        "            u_hat = torch.hstack([u_hat, temp])\n",
        "        u_hat[0] = 1\n",
        "        u_hat[-1] = bb   \n",
        "        d2u_hat = u_hat.clone()    \n",
        "        d2u_hat[1:-1] = (u_hat[0:-2] + u_hat[2:] - 2*u_hat[1:-1])/step**2     \n",
        "\n",
        "        loss = Loss(u_hat,d2u_hat,x_init)\n",
        "        loss.backward()\n",
        "      \n",
        "        optimizer.step() \n",
        "\n",
        "\n",
        "        running_loss += loss.clone().item()    \n",
        "        if isOn == True:\n",
        "            print(f\"The running loss at {e+1} iteration is: {running_loss}\")\n",
        "        test_error_vec.append(running_loss)\n",
        "        iter.append(e)\n",
        "\n",
        "    draw_result(iter, test_error_vec)\n",
        "    y_true = u_star_func(x_init_np)\n",
        "    draw_graph(x_init_np, y_true, u_hat.clone().detach().numpy(), 'Solutions')\n",
        "    print(np.mean(np.square(y_true -  u_hat.clone().detach().numpy())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9iLfRcetCPX_",
        "outputId": "89412faa-d65a-4601-bd73-03708469da9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The running loss at 1 iteration is: 85156.35345967334\n",
            "The running loss at 2 iteration is: 16280.942827651197\n",
            "The running loss at 3 iteration is: 11413.665877208563\n",
            "The running loss at 4 iteration is: 11726.34471902191\n",
            "The running loss at 5 iteration is: 12416.254482392045\n",
            "The running loss at 6 iteration is: 12949.921254041268\n",
            "The running loss at 7 iteration is: 13312.652691614161\n",
            "The running loss at 8 iteration is: 13546.665026677125\n",
            "The running loss at 9 iteration is: 13687.225964270578\n",
            "The running loss at 10 iteration is: 13757.272939341041\n",
            "The running loss at 11 iteration is: 13770.411676486834\n",
            "The running loss at 12 iteration is: 13733.467856986787\n",
            "The running loss at 13 iteration is: 13648.229555173746\n",
            "The running loss at 14 iteration is: 13512.102450158045\n",
            "The running loss at 15 iteration is: 13318.069851627672\n",
            "The running loss at 16 iteration is: 13054.001554397864\n",
            "The running loss at 17 iteration is: 12701.004160650124\n",
            "The running loss at 18 iteration is: 12230.844122161147\n",
            "The running loss at 19 iteration is: 11602.176218486125\n",
            "The running loss at 20 iteration is: 10755.897243966292\n",
            "The running loss at 21 iteration is: 9612.096269693657\n",
            "The running loss at 22 iteration is: 8079.54470813321\n",
            "The running loss at 23 iteration is: 6113.4390160343655\n",
            "The running loss at 24 iteration is: 3921.2338145621775\n",
            "The running loss at 25 iteration is: 2470.3889681670776\n",
            "The running loss at 26 iteration is: 3689.368572032421\n",
            "The running loss at 27 iteration is: 5962.148009363739\n",
            "The running loss at 28 iteration is: 4250.810659314159\n",
            "The running loss at 29 iteration is: 1723.7074195069354\n",
            "The running loss at 30 iteration is: 1056.852451060856\n",
            "The running loss at 31 iteration is: 1546.305886607575\n",
            "The running loss at 32 iteration is: 2150.3365994436303\n",
            "The running loss at 33 iteration is: 2412.4798286659643\n",
            "The running loss at 34 iteration is: 2225.075665151441\n",
            "The running loss at 35 iteration is: 1638.6656716025936\n",
            "The running loss at 36 iteration is: 843.4678519306127\n",
            "The running loss at 37 iteration is: 226.96012337303722\n",
            "The running loss at 38 iteration is: 275.9702053693503\n",
            "The running loss at 39 iteration is: 945.5423669026369\n",
            "The running loss at 40 iteration is: 1212.957715519632\n",
            "The running loss at 41 iteration is: 645.679151933283\n",
            "The running loss at 42 iteration is: 113.85281107547587\n",
            "The running loss at 43 iteration is: 121.47891548243847\n",
            "The running loss at 44 iteration is: 418.2675661315737\n",
            "The running loss at 45 iteration is: 645.3064494030252\n",
            "The running loss at 46 iteration is: 634.5273924144473\n",
            "The running loss at 47 iteration is: 407.80614881948145\n",
            "The running loss at 48 iteration is: 129.92223339852126\n",
            "The running loss at 49 iteration is: 28.096948800838934\n",
            "The running loss at 50 iteration is: 190.41252946614213\n",
            "The running loss at 51 iteration is: 378.69788459455424\n",
            "The running loss at 52 iteration is: 309.45302884409546\n",
            "The running loss at 53 iteration is: 98.82040769159626\n",
            "The running loss at 54 iteration is: 18.18335084982399\n",
            "The running loss at 55 iteration is: 100.05560683130982\n",
            "The running loss at 56 iteration is: 204.27531710016407\n",
            "The running loss at 57 iteration is: 217.90187693522313\n",
            "The running loss at 58 iteration is: 134.379601181525\n",
            "The running loss at 59 iteration is: 36.33292330393855\n",
            "The running loss at 60 iteration is: 20.028343737587125\n",
            "The running loss at 61 iteration is: 89.44769227771394\n",
            "The running loss at 62 iteration is: 136.98268412256374\n",
            "The running loss at 63 iteration is: 93.39572976070623\n",
            "The running loss at 64 iteration is: 25.602633250859757\n",
            "The running loss at 65 iteration is: 16.05346045144355\n",
            "The running loss at 66 iteration is: 55.261538370471115\n",
            "The running loss at 67 iteration is: 83.59347516120948\n",
            "The running loss at 68 iteration is: 68.27936229520353\n",
            "The running loss at 69 iteration is: 29.001548843765033\n",
            "The running loss at 70 iteration is: 10.235801526059577\n",
            "The running loss at 71 iteration is: 28.98553817580832\n",
            "The running loss at 72 iteration is: 52.47402641391604\n",
            "The running loss at 73 iteration is: 44.91263264898678\n",
            "The running loss at 74 iteration is: 19.069778610270596\n",
            "The running loss at 75 iteration is: 9.8403641649479\n",
            "The running loss at 76 iteration is: 22.734174168303483\n",
            "The running loss at 77 iteration is: 35.14334954663772\n",
            "The running loss at 78 iteration is: 30.36262655921004\n",
            "The running loss at 79 iteration is: 15.425625308050874\n",
            "The running loss at 80 iteration is: 9.038812885360294\n",
            "The running loss at 81 iteration is: 16.861907835863313\n",
            "The running loss at 82 iteration is: 24.725226010965144\n",
            "The running loss at 83 iteration is: 20.38721410634015\n",
            "The running loss at 84 iteration is: 10.901128615627082\n",
            "The running loss at 85 iteration is: 9.13350605507688\n",
            "The running loss at 86 iteration is: 14.875769028941036\n",
            "The running loss at 87 iteration is: 18.233941491778968\n",
            "The running loss at 88 iteration is: 14.481077052004897\n",
            "The running loss at 89 iteration is: 9.07906660125661\n",
            "The running loss at 90 iteration is: 8.959741196721504\n",
            "The running loss at 91 iteration is: 12.827942784463687\n",
            "The running loss at 92 iteration is: 13.927286814787434\n",
            "The running loss at 93 iteration is: 10.621199249469289\n",
            "The running loss at 94 iteration is: 7.975330578888178\n",
            "The running loss at 95 iteration is: 9.150726079683983\n",
            "The running loss at 96 iteration is: 11.346521301169016\n",
            "The running loss at 97 iteration is: 10.936922907462607\n",
            "The running loss at 98 iteration is: 8.617017851172909\n",
            "The running loss at 99 iteration is: 7.688792043140693\n",
            "The running loss at 100 iteration is: 8.96362516499532\n",
            "The running loss at 101 iteration is: 9.907125897896293\n",
            "The running loss at 102 iteration is: 8.907705473775964\n",
            "The running loss at 103 iteration is: 7.564556199317546\n",
            "The running loss at 104 iteration is: 7.702486995561203\n",
            "The running loss at 105 iteration is: 8.624958895856226\n",
            "The running loss at 106 iteration is: 8.633430606537173\n",
            "The running loss at 107 iteration is: 7.697114595794855\n",
            "The running loss at 108 iteration is: 7.202483120239095\n",
            "The running loss at 109 iteration is: 7.65523490966312\n",
            "The running loss at 110 iteration is: 8.04123247950695\n",
            "The running loss at 111 iteration is: 7.628810771338545\n",
            "The running loss at 112 iteration is: 7.058928890940768\n",
            "The running loss at 113 iteration is: 7.105223147217368\n",
            "The running loss at 114 iteration is: 7.454498749883407\n",
            "The running loss at 115 iteration is: 7.3941066204142825\n",
            "The running loss at 116 iteration is: 6.976856141433689\n",
            "The running loss at 117 iteration is: 6.804963971296572\n",
            "The running loss at 118 iteration is: 6.998324461483745\n",
            "The running loss at 119 iteration is: 7.082829127021615\n",
            "The running loss at 120 iteration is: 6.843782559463989\n",
            "The running loss at 121 iteration is: 6.626276513722559\n",
            "The running loss at 122 iteration is: 6.6808492622421465\n",
            "The running loss at 123 iteration is: 6.7805464586141895\n",
            "The running loss at 124 iteration is: 6.67127922777607\n",
            "The running loss at 125 iteration is: 6.4817803955946705\n",
            "The running loss at 126 iteration is: 6.451006078335129\n",
            "The running loss at 127 iteration is: 6.519281943646742\n",
            "The running loss at 128 iteration is: 6.478184300040225\n",
            "The running loss at 129 iteration is: 6.337645551706096\n",
            "The running loss at 130 iteration is: 6.27215052020132\n",
            "The running loss at 131 iteration is: 6.3022550147826975\n",
            "The running loss at 132 iteration is: 6.287951051060052\n",
            "The running loss at 133 iteration is: 6.19031022876923\n",
            "The running loss at 134 iteration is: 6.116645349197789\n",
            "The running loss at 135 iteration is: 6.117390730905053\n",
            "The running loss at 136 iteration is: 6.109058965814059\n",
            "The running loss at 137 iteration is: 6.039762324070358\n",
            "The running loss at 138 iteration is: 5.971790654094933\n",
            "The running loss at 139 iteration is: 5.954947572452649\n",
            "The running loss at 140 iteration is: 5.9434137842161725\n",
            "The running loss at 141 iteration is: 5.892144116305131\n",
            "The running loss at 142 iteration is: 5.832803904870001\n",
            "The running loss at 143 iteration is: 5.80693914106303\n",
            "The running loss at 144 iteration is: 5.790499347914599\n",
            "The running loss at 145 iteration is: 5.7490489517686445\n",
            "The running loss at 146 iteration is: 5.698192973833039\n",
            "The running loss at 147 iteration is: 5.668348005601091\n",
            "The running loss at 148 iteration is: 5.647726171015764\n",
            "The running loss at 149 iteration is: 5.611837827883089\n",
            "The running loss at 150 iteration is: 5.567349121952204\n",
            "The running loss at 151 iteration is: 5.5364402253659755\n",
            "The running loss at 152 iteration is: 5.513325321903917\n",
            "The running loss at 153 iteration is: 5.480041258499799\n",
            "The running loss at 154 iteration is: 5.440789580839613\n",
            "The running loss at 155 iteration is: 5.410371727886158\n",
            "The running loss at 156 iteration is: 5.385172053164854\n",
            "The running loss at 157 iteration is: 5.35417051196561\n",
            "The running loss at 158 iteration is: 5.318238124247719\n",
            "The running loss at 159 iteration is: 5.288651715238177\n",
            "The running loss at 160 iteration is: 5.26295057543111\n",
            "The running loss at 161 iteration is: 5.232813940635843\n",
            "The running loss at 162 iteration is: 5.199928976716393\n",
            "The running loss at 163 iteration is: 5.171325331360242\n",
            "The running loss at 164 iteration is: 5.145283610193541\n",
            "The running loss at 165 iteration is: 5.116066884601822\n",
            "The running loss at 166 iteration is: 5.085654064718138\n",
            "The running loss at 167 iteration is: 5.057747532534819\n",
            "The running loss at 168 iteration is: 5.031817681840804\n",
            "The running loss at 169 iteration is: 5.00353486608296\n",
            "The running loss at 170 iteration is: 4.974919334344166\n",
            "The running loss at 171 iteration is: 4.948163074112557\n",
            "The running loss at 172 iteration is: 4.922185242261618\n",
            "The running loss at 173 iteration is: 4.895134972861914\n",
            "The running loss at 174 iteration is: 4.86798772168389\n",
            "The running loss at 175 iteration is: 4.842288227906484\n",
            "The running loss at 176 iteration is: 4.816734724929003\n",
            "The running loss at 177 iteration is: 4.790516128051509\n",
            "The running loss at 178 iteration is: 4.764689192408651\n",
            "The running loss at 179 iteration is: 4.739728477235648\n",
            "The running loss at 180 iteration is: 4.714951834738183\n",
            "The running loss at 181 iteration is: 4.689292416460344\n",
            "The running loss at 182 iteration is: 4.664253044418327\n",
            "The running loss at 183 iteration is: 4.640262341914834\n",
            "The running loss at 184 iteration is: 4.616167529373375\n",
            "The running loss at 185 iteration is: 4.591751014314377\n",
            "The running loss at 186 iteration is: 4.567328960319351\n",
            "The running loss at 187 iteration is: 4.543909971059692\n",
            "The running loss at 188 iteration is: 4.520568435318176\n",
            "The running loss at 189 iteration is: 4.4968941049836975\n",
            "The running loss at 190 iteration is: 4.473160935602922\n",
            "The running loss at 191 iteration is: 4.450618533917088\n",
            "The running loss at 192 iteration is: 4.427816572727598\n",
            "The running loss at 193 iteration is: 4.404778415442028\n",
            "The running loss at 194 iteration is: 4.3823114057450185\n",
            "The running loss at 195 iteration is: 4.360389535474904\n",
            "The running loss at 196 iteration is: 4.3380565561664675\n",
            "The running loss at 197 iteration is: 4.316307159083118\n",
            "The running loss at 198 iteration is: 4.294588973885779\n",
            "The running loss at 199 iteration is: 4.27286662972885\n",
            "The running loss at 200 iteration is: 4.251361363596975\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.00017228966269093184\n",
            "The running loss at 1 iteration is: 4.230279589899785\n",
            "The running loss at 2 iteration is: 217.9127895815039\n",
            "The running loss at 3 iteration is: 3238.9130712951883\n",
            "The running loss at 4 iteration is: 6.801994739643446\n",
            "The running loss at 5 iteration is: 1031.0174921311154\n",
            "The running loss at 6 iteration is: 1994.7646625886464\n",
            "The running loss at 7 iteration is: 1947.4740059564956\n",
            "The running loss at 8 iteration is: 1160.5196675967773\n",
            "The running loss at 9 iteration is: 238.9292642595031\n",
            "The running loss at 10 iteration is: 91.70214518775292\n",
            "The running loss at 11 iteration is: 926.3066000061318\n",
            "The running loss at 12 iteration is: 1047.706445945474\n",
            "The running loss at 13 iteration is: 316.0991044749299\n",
            "The running loss at 14 iteration is: 3.235243396560528\n",
            "The running loss at 15 iteration is: 250.04746021344985\n",
            "The running loss at 16 iteration is: 562.4475322321706\n",
            "The running loss at 17 iteration is: 629.6269226530391\n",
            "The running loss at 18 iteration is: 428.44367362292235\n",
            "The running loss at 19 iteration is: 131.6402703707129\n",
            "The running loss at 20 iteration is: 3.336869873374886\n",
            "The running loss at 21 iteration is: 168.75335041578376\n",
            "The running loss at 22 iteration is: 365.39563381441883\n",
            "The running loss at 23 iteration is: 278.4752090861442\n",
            "The running loss at 24 iteration is: 64.70170814489421\n",
            "The running loss at 25 iteration is: 7.861954367358925\n",
            "The running loss at 26 iteration is: 108.31022368991529\n",
            "The running loss at 27 iteration is: 206.1249590593502\n",
            "The running loss at 28 iteration is: 197.20108430992303\n",
            "The running loss at 29 iteration is: 98.27025217151159\n",
            "The running loss at 30 iteration is: 10.88530654285806\n",
            "The running loss at 31 iteration is: 24.746778128321193\n",
            "The running loss at 32 iteration is: 105.45899468784145\n",
            "The running loss at 33 iteration is: 125.43419149796601\n",
            "The running loss at 34 iteration is: 58.034786257022375\n",
            "The running loss at 35 iteration is: 4.5583111248940105\n",
            "The running loss at 36 iteration is: 22.067603240416734\n",
            "The running loss at 37 iteration is: 66.77249365422837\n",
            "The running loss at 38 iteration is: 77.26066401090597\n",
            "The running loss at 39 iteration is: 43.55442334234503\n",
            "The running loss at 40 iteration is: 7.154684677856702\n",
            "The running loss at 41 iteration is: 10.008021647819088\n",
            "The running loss at 42 iteration is: 40.18881277656377\n",
            "The running loss at 43 iteration is: 47.82490974697304\n",
            "The running loss at 44 iteration is: 22.35512699713315\n",
            "The running loss at 45 iteration is: 3.088473677107827\n",
            "The running loss at 46 iteration is: 12.02279818620481\n",
            "The running loss at 47 iteration is: 29.048547418061588\n",
            "The running loss at 48 iteration is: 28.7721751863408\n",
            "The running loss at 49 iteration is: 12.680984946980344\n",
            "The running loss at 50 iteration is: 2.7679477292520436\n",
            "The running loss at 51 iteration is: 10.394718186547488\n",
            "The running loss at 52 iteration is: 20.479377302628443\n",
            "The running loss at 53 iteration is: 16.1457559684709\n",
            "The running loss at 54 iteration is: 5.151523739694347\n",
            "The running loss at 55 iteration is: 3.7655272462107914\n",
            "The running loss at 56 iteration is: 10.957421326719496\n",
            "The running loss at 57 iteration is: 13.958362218704744\n",
            "The running loss at 58 iteration is: 8.452122263071344\n",
            "The running loss at 59 iteration is: 2.978098765141953\n",
            "The running loss at 60 iteration is: 4.941200025485489\n",
            "The running loss at 61 iteration is: 9.533524864566923\n",
            "The running loss at 62 iteration is: 8.530573603893469\n",
            "The running loss at 63 iteration is: 3.934411809768701\n",
            "The running loss at 64 iteration is: 3.0681305276120443\n",
            "The running loss at 65 iteration is: 6.0705864633487705\n",
            "The running loss at 66 iteration is: 7.218818561201928\n",
            "The running loss at 67 iteration is: 4.718409166239486\n",
            "The running loss at 68 iteration is: 2.7168732709673855\n",
            "The running loss at 69 iteration is: 3.9874099884903447\n",
            "The running loss at 70 iteration is: 5.646012116728527\n",
            "The running loss at 71 iteration is: 4.635738286751107\n",
            "The running loss at 72 iteration is: 2.8517736236778015\n",
            "The running loss at 73 iteration is: 3.1320659738557097\n",
            "The running loss at 74 iteration is: 4.425104571966333\n",
            "The running loss at 75 iteration is: 4.262648136714836\n",
            "The running loss at 76 iteration is: 2.999238816459226\n",
            "The running loss at 77 iteration is: 2.7586475740744474\n",
            "The running loss at 78 iteration is: 3.6296870130695718\n",
            "The running loss at 79 iteration is: 3.8039927272956864\n",
            "The running loss at 80 iteration is: 2.9852167990177167\n",
            "The running loss at 81 iteration is: 2.6483061719744465\n",
            "The running loss at 82 iteration is: 3.1776373134083347\n",
            "The running loss at 83 iteration is: 3.4152799840424146\n",
            "The running loss at 84 iteration is: 2.9204686069063226\n",
            "The running loss at 85 iteration is: 2.5990402396497605\n",
            "The running loss at 86 iteration is: 2.910042517600563\n",
            "The running loss at 87 iteration is: 3.124162868817461\n",
            "The running loss at 88 iteration is: 2.8130732473296924\n",
            "The running loss at 89 iteration is: 2.569576878162149\n",
            "The running loss at 90 iteration is: 2.761705012339658\n",
            "The running loss at 91 iteration is: 2.914830252357952\n",
            "The running loss at 92 iteration is: 2.7148901874133013\n",
            "The running loss at 93 iteration is: 2.541629243290891\n",
            "The running loss at 94 iteration is: 2.6644002263797497\n",
            "The running loss at 95 iteration is: 2.7661308426853153\n",
            "The running loss at 96 iteration is: 2.62592785145546\n",
            "The running loss at 97 iteration is: 2.513100050715083\n",
            "The running loss at 98 iteration is: 2.599738001449698\n",
            "The running loss at 99 iteration is: 2.657217636047408\n",
            "The running loss at 100 iteration is: 2.554836488636282\n",
            "The running loss at 101 iteration is: 2.485437840307706\n",
            "The running loss at 102 iteration is: 2.5476308069031295\n",
            "The running loss at 103 iteration is: 2.5742481341525534\n",
            "The running loss at 104 iteration is: 2.496088415723753\n",
            "The running loss at 105 iteration is: 2.4587788480927717\n",
            "The running loss at 106 iteration is: 2.503525076280748\n",
            "The running loss at 107 iteration is: 2.5075671265813173\n",
            "The running loss at 108 iteration is: 2.4492419419594307\n",
            "The running loss at 109 iteration is: 2.4328268740846424\n",
            "The running loss at 110 iteration is: 2.463038172164627\n",
            "The running loss at 111 iteration is: 2.4524566822572367\n",
            "The running loss at 112 iteration is: 2.410602821060253\n",
            "The running loss at 113 iteration is: 2.407460473659471\n",
            "The running loss at 114 iteration is: 2.4240923699761607\n",
            "The running loss at 115 iteration is: 2.406131340897339\n",
            "The running loss at 116 iteration is: 2.377951501372689\n",
            "The running loss at 117 iteration is: 2.38068931385729\n",
            "The running loss at 118 iteration is: 2.3862120147877155\n",
            "The running loss at 119 iteration is: 2.3660808118361523\n",
            "The running loss at 120 iteration is: 2.34948943075072\n",
            "The running loss at 121 iteration is: 2.353072354224757\n",
            "The running loss at 122 iteration is: 2.3496671677894723\n",
            "The running loss at 123 iteration is: 2.3311983954433027\n",
            "The running loss at 124 iteration is: 2.3228003658280767\n",
            "The running loss at 125 iteration is: 2.323792544104554\n",
            "The running loss at 126 iteration is: 2.314928014424223\n",
            "The running loss at 127 iteration is: 2.3006883070947817\n",
            "The running loss at 128 iteration is: 2.2962515623504767\n",
            "The running loss at 129 iteration is: 2.293671498518359\n",
            "The running loss at 130 iteration is: 2.282865680091248\n",
            "The running loss at 131 iteration is: 2.272605388908706\n",
            "The running loss at 132 iteration is: 2.269706144025543\n",
            "The running loss at 133 iteration is: 2.263512807896478\n",
            "The running loss at 134 iteration is: 2.252753225788119\n",
            "The running loss at 135 iteration is: 2.245952793457375\n",
            "The running loss at 136 iteration is: 2.242018845448287\n",
            "The running loss at 137 iteration is: 2.233722397388394\n",
            "The running loss at 138 iteration is: 2.2249028282633443\n",
            "The running loss at 139 iteration is: 2.2195493097127224\n",
            "The running loss at 140 iteration is: 2.21358019295285\n",
            "The running loss at 141 iteration is: 2.20513131684834\n",
            "The running loss at 142 iteration is: 2.198090477065886\n",
            "The running loss at 143 iteration is: 2.1927404121711396\n",
            "The running loss at 144 iteration is: 2.1857713885076544\n",
            "The running loss at 145 iteration is: 2.1778564533827014\n",
            "The running loss at 146 iteration is: 2.171918729671241\n",
            "The running loss at 147 iteration is: 2.1658860842051832\n",
            "The running loss at 148 iteration is: 2.1582099499952045\n",
            "The running loss at 149 iteration is: 2.151548945380136\n",
            "The running loss at 150 iteration is: 2.1457228727667803\n",
            "The running loss at 151 iteration is: 2.1387323045660858\n",
            "The running loss at 152 iteration is: 2.131899025927862\n",
            "The running loss at 153 iteration is: 2.1256383521898656\n",
            "The running loss at 154 iteration is: 2.1194889861122794\n",
            "The running loss at 155 iteration is: 2.11237653490844\n",
            "The running loss at 156 iteration is: 2.1060190044791636\n",
            "The running loss at 157 iteration is: 2.099919849069719\n",
            "The running loss at 158 iteration is: 2.093237730145464\n",
            "The running loss at 159 iteration is: 2.086608057921006\n",
            "The running loss at 160 iteration is: 2.0806444678140386\n",
            "The running loss at 161 iteration is: 2.0741551625946206\n",
            "The running loss at 162 iteration is: 2.0677558367318216\n",
            "The running loss at 163 iteration is: 2.061566078574919\n",
            "The running loss at 164 iteration is: 2.0552016278573935\n",
            "The running loss at 165 iteration is: 2.0489528171368803\n",
            "The running loss at 166 iteration is: 2.0426529223134224\n",
            "The running loss at 167 iteration is: 2.0366378971526684\n",
            "The running loss at 168 iteration is: 2.030249644073145\n",
            "The running loss at 169 iteration is: 2.023847682649868\n",
            "The running loss at 170 iteration is: 2.0178903267823665\n",
            "The running loss at 171 iteration is: 2.011883068560078\n",
            "The running loss at 172 iteration is: 2.0056233345654575\n",
            "The running loss at 173 iteration is: 1.999384438626949\n",
            "The running loss at 174 iteration is: 1.9934774756491818\n",
            "The running loss at 175 iteration is: 1.9875006329635538\n",
            "The running loss at 176 iteration is: 1.9813524956254873\n",
            "The running loss at 177 iteration is: 1.9751884736663012\n",
            "The running loss at 178 iteration is: 1.9692849444027571\n",
            "The running loss at 179 iteration is: 1.96348386488911\n",
            "The running loss at 180 iteration is: 1.9574028531974106\n",
            "The running loss at 181 iteration is: 1.9514618649626818\n",
            "The running loss at 182 iteration is: 1.9455809058903881\n",
            "The running loss at 183 iteration is: 1.9395840011170726\n",
            "The running loss at 184 iteration is: 1.933873396885081\n",
            "The running loss at 185 iteration is: 1.9278946625258586\n",
            "The running loss at 186 iteration is: 1.922016797258603\n",
            "The running loss at 187 iteration is: 1.9160749155657062\n",
            "The running loss at 188 iteration is: 1.9103768049900232\n",
            "The running loss at 189 iteration is: 1.9045766555434904\n",
            "The running loss at 190 iteration is: 1.898917842084894\n",
            "The running loss at 191 iteration is: 1.8933793571278494\n",
            "The running loss at 192 iteration is: 1.8875807959362179\n",
            "The running loss at 193 iteration is: 1.881861156077345\n",
            "The running loss at 194 iteration is: 1.875997762473754\n",
            "The running loss at 195 iteration is: 1.8705366162411825\n",
            "The running loss at 196 iteration is: 1.86478548636433\n",
            "The running loss at 197 iteration is: 1.8591374610136755\n",
            "The running loss at 198 iteration is: 1.853461622124165\n",
            "The running loss at 199 iteration is: 1.8480808129747108\n",
            "The running loss at 200 iteration is: 1.84243470647986\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.00013415150345517858\n"
          ]
        }
      ],
      "source": [
        "run_train(lr=0.1, num_e= 200)\n",
        "run_train(lr=0.05, num_e= 200)\n",
        "# run_train(lr=0.0001, num_e= 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6n6igUNwbr7s",
        "outputId": "f2489a9c-225b-457e-9d31-16b81c3c592a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The running loss at 1 iteration is: 1.8368460363064958\n",
            "The running loss at 2 iteration is: 20.23802618941624\n",
            "The running loss at 3 iteration is: 1217.5754484672696\n",
            "The running loss at 4 iteration is: 13.287050354463595\n",
            "The running loss at 5 iteration is: 398.89765929688036\n",
            "The running loss at 6 iteration is: 793.7165462678898\n",
            "The running loss at 7 iteration is: 660.2532184690377\n",
            "The running loss at 8 iteration is: 256.5757540537162\n",
            "The running loss at 9 iteration is: 3.6702857535529465\n",
            "The running loss at 10 iteration is: 173.5966265797955\n",
            "The running loss at 11 iteration is: 429.7219595764385\n",
            "The running loss at 12 iteration is: 315.1456251615978\n",
            "The running loss at 13 iteration is: 66.90520203170115\n",
            "The running loss at 14 iteration is: 8.736263460565043\n",
            "The running loss at 15 iteration is: 122.1319853551717\n",
            "The running loss at 16 iteration is: 228.20321178565425\n",
            "The running loss at 17 iteration is: 220.97218495663464\n",
            "The running loss at 18 iteration is: 118.9904338633175\n",
            "The running loss at 19 iteration is: 18.940855544308445\n",
            "The running loss at 20 iteration is: 13.078215625989035\n",
            "The running loss at 21 iteration is: 91.44263852566694\n",
            "The running loss at 22 iteration is: 137.56445899850385\n",
            "The running loss at 23 iteration is: 89.22266737226501\n",
            "The running loss at 24 iteration is: 18.088879061912053\n",
            "The running loss at 25 iteration is: 4.978829966022291\n",
            "The running loss at 26 iteration is: 43.78806769103405\n",
            "The running loss at 27 iteration is: 77.59108532987835\n",
            "The running loss at 28 iteration is: 69.63140313761711\n",
            "The running loss at 29 iteration is: 31.14580384367138\n",
            "The running loss at 30 iteration is: 2.88507662276112\n",
            "The running loss at 31 iteration is: 12.052457070269417\n",
            "The running loss at 32 iteration is: 40.069812681139304\n",
            "The running loss at 33 iteration is: 45.80981020773209\n",
            "The running loss at 34 iteration is: 22.78077715391106\n",
            "The running loss at 35 iteration is: 2.854804311650374\n",
            "The running loss at 36 iteration is: 7.0939187061368205\n",
            "The running loss at 37 iteration is: 23.301104592702035\n",
            "The running loss at 38 iteration is: 28.792860893517936\n",
            "The running loss at 39 iteration is: 17.64395986264197\n",
            "The running loss at 40 iteration is: 3.947016236036936\n",
            "The running loss at 41 iteration is: 3.3325194137902905\n",
            "The running loss at 42 iteration is: 13.523833778545976\n",
            "The running loss at 43 iteration is: 18.221388222918975\n",
            "The running loss at 44 iteration is: 10.867878762202173\n",
            "The running loss at 45 iteration is: 2.485588521782849\n",
            "The running loss at 46 iteration is: 3.351810168777402\n",
            "The running loss at 47 iteration is: 9.664336492968197\n",
            "The running loss at 48 iteration is: 11.602182993452287\n",
            "The running loss at 49 iteration is: 6.797058283061187\n",
            "The running loss at 50 iteration is: 1.9780796050376277\n",
            "The running loss at 51 iteration is: 3.0433483405584805\n",
            "The running loss at 52 iteration is: 7.0975242846986655\n",
            "The running loss at 53 iteration is: 7.425179631678505\n",
            "The running loss at 54 iteration is: 3.7488696801695536\n",
            "The running loss at 55 iteration is: 1.6261754698475042\n",
            "The running loss at 56 iteration is: 3.374275129780822\n",
            "The running loss at 57 iteration is: 5.483448827169104\n",
            "The running loss at 58 iteration is: 4.648022751037863\n",
            "The running loss at 59 iteration is: 2.2758159236606645\n",
            "The running loss at 60 iteration is: 1.7696752751002434\n",
            "The running loss at 61 iteration is: 3.3395241409174927\n",
            "The running loss at 62 iteration is: 4.081232955515962\n",
            "The running loss at 63 iteration is: 2.8184452727830713\n",
            "The running loss at 64 iteration is: 1.6440073172030782\n",
            "The running loss at 65 iteration is: 2.14499252714027\n",
            "The running loss at 66 iteration is: 3.1093701961279208\n",
            "The running loss at 67 iteration is: 2.8723267513649255\n",
            "The running loss at 68 iteration is: 1.880424392081343\n",
            "The running loss at 69 iteration is: 1.6696764695535982\n",
            "The running loss at 70 iteration is: 2.333550840781803\n",
            "The running loss at 71 iteration is: 2.579717541733273\n",
            "The running loss at 72 iteration is: 2.0024248346912716\n",
            "The running loss at 73 iteration is: 1.591226939589746\n",
            "The running loss at 74 iteration is: 1.897055348569394\n",
            "The running loss at 75 iteration is: 2.2349158942616714\n",
            "The running loss at 76 iteration is: 1.9992935472420843\n",
            "The running loss at 77 iteration is: 1.6145941710173537\n",
            "The running loss at 78 iteration is: 1.6811392417614366\n",
            "The running loss at 79 iteration is: 1.9641474343391188\n",
            "The running loss at 80 iteration is: 1.911896752880568\n",
            "The running loss at 81 iteration is: 1.6313440869884408\n",
            "The running loss at 82 iteration is: 1.5961346015450397\n",
            "The running loss at 83 iteration is: 1.7859969545183116\n",
            "The running loss at 84 iteration is: 1.8102896650083695\n",
            "The running loss at 85 iteration is: 1.6288859942711291\n",
            "The running loss at 86 iteration is: 1.559214997745577\n",
            "The running loss at 87 iteration is: 1.6758893935841972\n",
            "The running loss at 88 iteration is: 1.7211713889340694\n",
            "The running loss at 89 iteration is: 1.6068167547974808\n",
            "The running loss at 90 iteration is: 1.541143905059509\n",
            "The running loss at 91 iteration is: 1.6108468778571383\n",
            "The running loss at 92 iteration is: 1.6511572916798\n",
            "The running loss at 93 iteration is: 1.5798243909312313\n",
            "The running loss at 94 iteration is: 1.5273796760558054\n",
            "The running loss at 95 iteration is: 1.568833754608272\n",
            "The running loss at 96 iteration is: 1.5992044551613895\n",
            "The running loss at 97 iteration is: 1.5523082964229926\n",
            "The running loss at 98 iteration is: 1.5142630788926994\n",
            "The running loss at 99 iteration is: 1.5403609048670592\n",
            "The running loss at 100 iteration is: 1.559666751402127\n",
            "The running loss at 101 iteration is: 1.5270247956728633\n",
            "The running loss at 102 iteration is: 1.5010622626001389\n",
            "The running loss at 103 iteration is: 1.5183727120370007\n",
            "The running loss at 104 iteration is: 1.5289267952460384\n",
            "The running loss at 105 iteration is: 1.5047642808045072\n",
            "The running loss at 106 iteration is: 1.4881843399569856\n",
            "The running loss at 107 iteration is: 1.4997654429846898\n",
            "The running loss at 108 iteration is: 1.5038039940781474\n",
            "The running loss at 109 iteration is: 1.4851683341372202\n",
            "The running loss at 110 iteration is: 1.4750032559597481\n",
            "The running loss at 111 iteration is: 1.4827286353986469\n",
            "The running loss at 112 iteration is: 1.4821922399606082\n",
            "The running loss at 113 iteration is: 1.4680112245348724\n",
            "The running loss at 114 iteration is: 1.4624123577434804\n",
            "The running loss at 115 iteration is: 1.467047861949655\n",
            "The running loss at 116 iteration is: 1.463249215769161\n",
            "The running loss at 117 iteration is: 1.4522641174755897\n",
            "The running loss at 118 iteration is: 1.449457277778611\n",
            "The running loss at 119 iteration is: 1.4513519065699414\n",
            "The running loss at 120 iteration is: 1.4460561336573603\n",
            "The running loss at 121 iteration is: 1.438021603246148\n",
            "The running loss at 122 iteration is: 1.4366425234559907\n",
            "The running loss at 123 iteration is: 1.4360069826286115\n",
            "The running loss at 124 iteration is: 1.4302599383182588\n",
            "The running loss at 125 iteration is: 1.4245998195097223\n",
            "The running loss at 126 iteration is: 1.4233693820141717\n",
            "The running loss at 127 iteration is: 1.4210071896988477\n",
            "The running loss at 128 iteration is: 1.4153658842699777\n",
            "The running loss at 129 iteration is: 1.4112929824808764\n",
            "The running loss at 130 iteration is: 1.4098329257143156\n",
            "The running loss at 131 iteration is: 1.4063699297025787\n",
            "The running loss at 132 iteration is: 1.4014234349660328\n",
            "The running loss at 133 iteration is: 1.3983118268431658\n",
            "The running loss at 134 iteration is: 1.3959941515219496\n",
            "The running loss at 135 iteration is: 1.3922373790988825\n",
            "The running loss at 136 iteration is: 1.3878847342612628\n",
            "The running loss at 137 iteration is: 1.3852641708466251\n",
            "The running loss at 138 iteration is: 1.3823811019663097\n",
            "The running loss at 139 iteration is: 1.3782372118483945\n",
            "The running loss at 140 iteration is: 1.3747435208244845\n",
            "The running loss at 141 iteration is: 1.3721157277677902\n",
            "The running loss at 142 iteration is: 1.3685596295638656\n",
            "The running loss at 143 iteration is: 1.364776523603424\n",
            "The running loss at 144 iteration is: 1.3617666423105186\n",
            "The running loss at 145 iteration is: 1.358832679286294\n",
            "The running loss at 146 iteration is: 1.3551814735012968\n",
            "The running loss at 147 iteration is: 1.351797937469449\n",
            "The running loss at 148 iteration is: 1.3489837079278302\n",
            "The running loss at 149 iteration is: 1.3455498217357436\n",
            "The running loss at 150 iteration is: 1.3419545528824877\n",
            "The running loss at 151 iteration is: 1.3388399934840178\n",
            "The running loss at 152 iteration is: 1.3357964862006704\n",
            "The running loss at 153 iteration is: 1.332522925629861\n",
            "The running loss at 154 iteration is: 1.3291056260683187\n",
            "The running loss at 155 iteration is: 1.3260675993023607\n",
            "The running loss at 156 iteration is: 1.3229357443163186\n",
            "The running loss at 157 iteration is: 1.3194297665399657\n",
            "The running loss at 158 iteration is: 1.3163609882257987\n",
            "The running loss at 159 iteration is: 1.3132527361420199\n",
            "The running loss at 160 iteration is: 1.3098323926151667\n",
            "The running loss at 161 iteration is: 1.3065579602604378\n",
            "The running loss at 162 iteration is: 1.30354382732\n",
            "The running loss at 163 iteration is: 1.3004236309246675\n",
            "The running loss at 164 iteration is: 1.2972292131175138\n",
            "The running loss at 165 iteration is: 1.2940270149433448\n",
            "The running loss at 166 iteration is: 1.2907121147978855\n",
            "The running loss at 167 iteration is: 1.2877698687311083\n",
            "The running loss at 168 iteration is: 1.2843392092512982\n",
            "The running loss at 169 iteration is: 1.2813525317314594\n",
            "The running loss at 170 iteration is: 1.2782741730682112\n",
            "The running loss at 171 iteration is: 1.2751114823229213\n",
            "The running loss at 172 iteration is: 1.271872301691915\n",
            "The running loss at 173 iteration is: 1.268831139110968\n",
            "The running loss at 174 iteration is: 1.2658119573256952\n",
            "The running loss at 175 iteration is: 1.2625556581451534\n",
            "The running loss at 176 iteration is: 1.2596039168979751\n",
            "The running loss at 177 iteration is: 1.2562959313508992\n",
            "The running loss at 178 iteration is: 1.2533034455895815\n",
            "The running loss at 179 iteration is: 1.2501777525276825\n",
            "The running loss at 180 iteration is: 1.2471241256801264\n",
            "The running loss at 181 iteration is: 1.2441741446586374\n",
            "The running loss at 182 iteration is: 1.2409503509168964\n",
            "The running loss at 183 iteration is: 1.2377908985402504\n",
            "The running loss at 184 iteration is: 1.2347945478108924\n",
            "The running loss at 185 iteration is: 1.2316955255672624\n",
            "The running loss at 186 iteration is: 1.2289256774710877\n",
            "The running loss at 187 iteration is: 1.225774890146018\n",
            "The running loss at 188 iteration is: 1.222835813933257\n",
            "The running loss at 189 iteration is: 1.2196804477043572\n",
            "The running loss at 190 iteration is: 1.2168195721294177\n",
            "The running loss at 191 iteration is: 1.21371729816241\n",
            "The running loss at 192 iteration is: 1.210744049483525\n",
            "The running loss at 193 iteration is: 1.2078771478862795\n",
            "The running loss at 194 iteration is: 1.2046079266704832\n",
            "The running loss at 195 iteration is: 1.2017217771315718\n",
            "The running loss at 196 iteration is: 1.19886468659277\n",
            "The running loss at 197 iteration is: 1.1957553568045445\n",
            "The running loss at 198 iteration is: 1.1926911576339883\n",
            "The running loss at 199 iteration is: 1.18985584069081\n",
            "The running loss at 200 iteration is: 1.186981564900857\n",
            "The running loss at 201 iteration is: 1.1841292975247966\n",
            "The running loss at 202 iteration is: 1.181289409748642\n",
            "The running loss at 203 iteration is: 1.1780958166535864\n",
            "The running loss at 204 iteration is: 1.1752837486010652\n",
            "The running loss at 205 iteration is: 1.1724700920524647\n",
            "The running loss at 206 iteration is: 1.1696380829470707\n",
            "The running loss at 207 iteration is: 1.1666789327613067\n",
            "The running loss at 208 iteration is: 1.1638510828969435\n",
            "The running loss at 209 iteration is: 1.1609244535807342\n",
            "The running loss at 210 iteration is: 1.1581279080134668\n",
            "The running loss at 211 iteration is: 1.1552013627399087\n",
            "The running loss at 212 iteration is: 1.1524050005229576\n",
            "The running loss at 213 iteration is: 1.149484919944555\n",
            "The running loss at 214 iteration is: 1.1466582845901703\n",
            "The running loss at 215 iteration is: 1.1437619999998594\n",
            "The running loss at 216 iteration is: 1.1408643382510804\n",
            "The running loss at 217 iteration is: 1.1380905075742747\n",
            "The running loss at 218 iteration is: 1.1354159298667568\n",
            "The running loss at 219 iteration is: 1.1325210365263971\n",
            "The running loss at 220 iteration is: 1.1296394091482116\n",
            "The running loss at 221 iteration is: 1.1269834814959618\n",
            "The running loss at 222 iteration is: 1.1240835755797383\n",
            "The running loss at 223 iteration is: 1.1214520213044423\n",
            "The running loss at 224 iteration is: 1.1187017336304437\n",
            "The running loss at 225 iteration is: 1.1160596615031582\n",
            "The running loss at 226 iteration is: 1.1130585324956197\n",
            "The running loss at 227 iteration is: 1.1104346553253057\n",
            "The running loss at 228 iteration is: 1.107566493714191\n",
            "The running loss at 229 iteration is: 1.1049422088946266\n",
            "The running loss at 230 iteration is: 1.102332204133722\n",
            "The running loss at 231 iteration is: 1.0994745142622233\n",
            "The running loss at 232 iteration is: 1.0967462379735151\n",
            "The running loss at 233 iteration is: 1.0941403866238006\n",
            "The running loss at 234 iteration is: 1.09152934833857\n",
            "The running loss at 235 iteration is: 1.0888060393213224\n",
            "The running loss at 236 iteration is: 1.0859907748496909\n",
            "The running loss at 237 iteration is: 1.0832627720044319\n",
            "The running loss at 238 iteration is: 1.0806646305127667\n",
            "The running loss at 239 iteration is: 1.0779407604450675\n",
            "The running loss at 240 iteration is: 1.0756073488215867\n",
            "The running loss at 241 iteration is: 1.072904781587576\n",
            "The running loss at 242 iteration is: 1.0702153451214318\n",
            "The running loss at 243 iteration is: 1.0676335592783301\n",
            "The running loss at 244 iteration is: 1.064927496594457\n",
            "The running loss at 245 iteration is: 1.062258327735802\n",
            "The running loss at 246 iteration is: 1.0598048523068648\n",
            "The running loss at 247 iteration is: 1.0571425194366626\n",
            "The running loss at 248 iteration is: 1.0545563453670945\n",
            "The running loss at 249 iteration is: 1.052132847258661\n",
            "The running loss at 250 iteration is: 1.0494584877469206\n",
            "The running loss at 251 iteration is: 1.0470309754434393\n",
            "The running loss at 252 iteration is: 1.0442370598166903\n",
            "The running loss at 253 iteration is: 1.0418165662871963\n",
            "The running loss at 254 iteration is: 1.0391664941207306\n",
            "The running loss at 255 iteration is: 1.0367508347093322\n",
            "The running loss at 256 iteration is: 1.034314813807557\n",
            "The running loss at 257 iteration is: 1.0315600503963673\n",
            "The running loss at 258 iteration is: 1.0291345684527862\n",
            "The running loss at 259 iteration is: 1.0267334670904809\n",
            "The running loss at 260 iteration is: 1.0239891269790162\n",
            "The running loss at 261 iteration is: 1.0215892280397687\n",
            "The running loss at 262 iteration is: 1.0190675985166615\n",
            "The running loss at 263 iteration is: 1.0166850986125284\n",
            "The running loss at 264 iteration is: 1.0141733320174702\n",
            "The running loss at 265 iteration is: 1.011769899838364\n",
            "The running loss at 266 iteration is: 1.0092850335946097\n",
            "The running loss at 267 iteration is: 1.0067695668417227\n",
            "The running loss at 268 iteration is: 1.004402179768457\n",
            "The running loss at 269 iteration is: 1.0021085303983794\n",
            "The running loss at 270 iteration is: 0.9996277796423818\n",
            "The running loss at 271 iteration is: 0.9971393506553395\n",
            "The running loss at 272 iteration is: 0.9946826206605833\n",
            "The running loss at 273 iteration is: 0.9924184879184349\n",
            "The running loss at 274 iteration is: 0.9899236963508002\n",
            "The running loss at 275 iteration is: 0.9874567920465547\n",
            "The running loss at 276 iteration is: 0.9852093750005071\n",
            "The running loss at 277 iteration is: 0.9827500012850456\n",
            "The running loss at 278 iteration is: 0.9805123056020312\n",
            "The running loss at 279 iteration is: 0.9780524912452326\n",
            "The running loss at 280 iteration is: 0.9757997477393959\n",
            "The running loss at 281 iteration is: 0.9734632712571377\n",
            "The running loss at 282 iteration is: 0.9710109182521104\n",
            "The running loss at 283 iteration is: 0.9688005811167076\n",
            "The running loss at 284 iteration is: 0.9664462204741842\n",
            "The running loss at 285 iteration is: 0.9642263093626134\n",
            "The running loss at 286 iteration is: 0.9616922684827941\n",
            "The running loss at 287 iteration is: 0.9594631366732485\n",
            "The running loss at 288 iteration is: 0.9571461210595686\n",
            "The running loss at 289 iteration is: 0.9549301190504896\n",
            "The running loss at 290 iteration is: 0.9526264001396516\n",
            "The running loss at 291 iteration is: 0.9503151514770731\n",
            "The running loss at 292 iteration is: 0.9479871097609864\n",
            "The running loss at 293 iteration is: 0.9457837069354179\n",
            "The running loss at 294 iteration is: 0.9437140497235594\n",
            "The running loss at 295 iteration is: 0.941415722777929\n",
            "The running loss at 296 iteration is: 0.939106528319412\n",
            "The running loss at 297 iteration is: 0.9368118800727477\n",
            "The running loss at 298 iteration is: 0.934719049710525\n",
            "The running loss at 299 iteration is: 0.932425950596122\n",
            "The running loss at 300 iteration is: 0.9301653716548112\n",
            "The running loss at 301 iteration is: 0.9280866963480453\n",
            "The running loss at 302 iteration is: 0.925804094671629\n",
            "The running loss at 303 iteration is: 0.9236342491889556\n",
            "The running loss at 304 iteration is: 0.9213525507549979\n",
            "The running loss at 305 iteration is: 0.9192815440757637\n",
            "The running loss at 306 iteration is: 0.9170171412858992\n",
            "The running loss at 307 iteration is: 0.9148478509071039\n",
            "The running loss at 308 iteration is: 0.9128072747800235\n",
            "The running loss at 309 iteration is: 0.9106626059583466\n",
            "The running loss at 310 iteration is: 0.9084059712278395\n",
            "The running loss at 311 iteration is: 0.9062400527538792\n",
            "The running loss at 312 iteration is: 0.9042167349924234\n",
            "The running loss at 313 iteration is: 0.9020643191984398\n",
            "The running loss at 314 iteration is: 0.899902102128055\n",
            "The running loss at 315 iteration is: 0.897882392775013\n",
            "The running loss at 316 iteration is: 0.8957489442102009\n",
            "The running loss at 317 iteration is: 0.8936079280977812\n",
            "The running loss at 318 iteration is: 0.8914792869087492\n",
            "The running loss at 319 iteration is: 0.889479484643429\n",
            "The running loss at 320 iteration is: 0.8873227702677444\n",
            "The running loss at 321 iteration is: 0.8854131941302472\n",
            "The running loss at 322 iteration is: 0.8832962181510884\n",
            "The running loss at 323 iteration is: 0.8811924920450741\n",
            "The running loss at 324 iteration is: 0.8790669358987483\n",
            "The running loss at 325 iteration is: 0.8770585330536472\n",
            "The running loss at 326 iteration is: 0.8749489835032179\n",
            "The running loss at 327 iteration is: 0.8730450751750168\n",
            "The running loss at 328 iteration is: 0.8709441023812782\n",
            "The running loss at 329 iteration is: 0.8690522339230249\n",
            "The running loss at 330 iteration is: 0.8668568933255197\n",
            "The running loss at 331 iteration is: 0.8649576535969072\n",
            "The running loss at 332 iteration is: 0.8627501317761396\n",
            "The running loss at 333 iteration is: 0.8608714073809801\n",
            "The running loss at 334 iteration is: 0.8589914199633929\n",
            "The running loss at 335 iteration is: 0.8569982382915752\n",
            "The running loss at 336 iteration is: 0.8548193757692399\n",
            "The running loss at 337 iteration is: 0.8529443476268636\n",
            "The running loss at 338 iteration is: 0.850959424384046\n",
            "The running loss at 339 iteration is: 0.8490930759665866\n",
            "The running loss at 340 iteration is: 0.8471332512832019\n",
            "The running loss at 341 iteration is: 0.8451430782544724\n",
            "The running loss at 342 iteration is: 0.8432066388309125\n",
            "The running loss at 343 iteration is: 0.8412210447232558\n",
            "The running loss at 344 iteration is: 0.8392712959340782\n",
            "The running loss at 345 iteration is: 0.8374064546903521\n",
            "The running loss at 346 iteration is: 0.8354610935265319\n",
            "The running loss at 347 iteration is: 0.8337003599790783\n",
            "The running loss at 348 iteration is: 0.8316436976509317\n",
            "The running loss at 349 iteration is: 0.8296845778709634\n",
            "The running loss at 350 iteration is: 0.8277495555899138\n",
            "The running loss at 351 iteration is: 0.826018937645336\n",
            "The running loss at 352 iteration is: 0.8240641881934837\n",
            "The running loss at 353 iteration is: 0.8223146356020509\n",
            "The running loss at 354 iteration is: 0.8202860228066663\n",
            "The running loss at 355 iteration is: 0.8185641136443412\n",
            "The running loss at 356 iteration is: 0.8166207366690486\n",
            "The running loss at 357 iteration is: 0.8147788095417059\n",
            "The running loss at 358 iteration is: 0.8128590468418184\n",
            "The running loss at 359 iteration is: 0.8110432365335566\n",
            "The running loss at 360 iteration is: 0.809328814599895\n",
            "The running loss at 361 iteration is: 0.8072946446389867\n",
            "The running loss at 362 iteration is: 0.8055860546629223\n",
            "The running loss at 363 iteration is: 0.8037589714719321\n",
            "The running loss at 364 iteration is: 0.8019436432197657\n",
            "The running loss at 365 iteration is: 0.800229634598674\n",
            "The running loss at 366 iteration is: 0.7984174924451678\n",
            "The running loss at 367 iteration is: 0.7964180314022646\n",
            "The running loss at 368 iteration is: 0.7946109568405616\n",
            "The running loss at 369 iteration is: 0.7928054046179693\n",
            "The running loss at 370 iteration is: 0.7910026860547543\n",
            "The running loss at 371 iteration is: 0.7893999829901125\n",
            "The running loss at 372 iteration is: 0.7876110812546179\n",
            "The running loss at 373 iteration is: 0.7858137387147439\n",
            "The running loss at 374 iteration is: 0.7840088035482036\n",
            "The running loss at 375 iteration is: 0.7822257735931156\n",
            "The running loss at 376 iteration is: 0.7806427193699417\n",
            "The running loss at 377 iteration is: 0.7788431027291711\n",
            "The running loss at 378 iteration is: 0.7770661517982841\n",
            "The running loss at 379 iteration is: 0.7753802986963038\n",
            "The running loss at 380 iteration is: 0.7735876476529597\n",
            "The running loss at 381 iteration is: 0.7718173823236782\n",
            "The running loss at 382 iteration is: 0.7701564689054234\n",
            "The running loss at 383 iteration is: 0.7683776550098063\n",
            "The running loss at 384 iteration is: 0.7667978297084626\n",
            "The running loss at 385 iteration is: 0.7651236001829075\n",
            "The running loss at 386 iteration is: 0.7633652413489863\n",
            "The running loss at 387 iteration is: 0.7616933467673084\n",
            "The running loss at 388 iteration is: 0.760031824829506\n",
            "The running loss at 389 iteration is: 0.75825595031261\n",
            "The running loss at 390 iteration is: 0.7566015775477282\n",
            "The running loss at 391 iteration is: 0.7549698096121301\n",
            "The running loss at 392 iteration is: 0.7533731816003172\n",
            "The running loss at 393 iteration is: 0.7517113906501807\n",
            "The running loss at 394 iteration is: 0.7498814213914006\n",
            "The running loss at 395 iteration is: 0.7482167108472123\n",
            "The running loss at 396 iteration is: 0.7465718233618356\n",
            "The running loss at 397 iteration is: 0.7449273809547513\n",
            "The running loss at 398 iteration is: 0.7432755659816792\n",
            "The running loss at 399 iteration is: 0.7418295515871312\n",
            "The running loss at 400 iteration is: 0.7401839296168833\n",
            "The running loss at 401 iteration is: 0.738548631536625\n",
            "The running loss at 402 iteration is: 0.7369031961765071\n",
            "The running loss at 403 iteration is: 0.7352604201073891\n",
            "The running loss at 404 iteration is: 0.7336327921302821\n",
            "The running loss at 405 iteration is: 0.732206767122775\n",
            "The running loss at 406 iteration is: 0.7304723758719605\n",
            "The running loss at 407 iteration is: 0.7288360940700976\n",
            "The running loss at 408 iteration is: 0.7274048794876458\n",
            "The running loss at 409 iteration is: 0.7256909740061234\n",
            "The running loss at 410 iteration is: 0.7242609409797496\n",
            "The running loss at 411 iteration is: 0.7226312994699008\n",
            "The running loss at 412 iteration is: 0.721099738480157\n",
            "The running loss at 413 iteration is: 0.7194786237630297\n",
            "The running loss at 414 iteration is: 0.7179728080765686\n",
            "The running loss at 415 iteration is: 0.7163410288867783\n",
            "The running loss at 416 iteration is: 0.7148340241627721\n",
            "The running loss at 417 iteration is: 0.7133308687362784\n",
            "The running loss at 418 iteration is: 0.7117073431743445\n",
            "The running loss at 419 iteration is: 0.7101995105169798\n",
            "The running loss at 420 iteration is: 0.7086719019327937\n",
            "The running loss at 421 iteration is: 0.7071849271762776\n",
            "The running loss at 422 iteration is: 0.7057627664356457\n",
            "The running loss at 423 iteration is: 0.7040630191644992\n",
            "The running loss at 424 iteration is: 0.702553888410283\n",
            "The running loss at 425 iteration is: 0.7010557179884481\n",
            "The running loss at 426 iteration is: 0.6995576491661201\n",
            "The running loss at 427 iteration is: 0.6980681876211249\n",
            "The running loss at 428 iteration is: 0.696563106774565\n",
            "The running loss at 429 iteration is: 0.6950614377227348\n",
            "The running loss at 430 iteration is: 0.6935821698021164\n",
            "The running loss at 431 iteration is: 0.6922673492468409\n",
            "The running loss at 432 iteration is: 0.6906847944206191\n",
            "The running loss at 433 iteration is: 0.6891965399170337\n",
            "The running loss at 434 iteration is: 0.6877030729381135\n",
            "The running loss at 435 iteration is: 0.6862222510809207\n",
            "The running loss at 436 iteration is: 0.6849442871696622\n",
            "The running loss at 437 iteration is: 0.6833484450203107\n",
            "The running loss at 438 iteration is: 0.681878942285742\n",
            "The running loss at 439 iteration is: 0.6805047041414575\n",
            "The running loss at 440 iteration is: 0.6790306794920414\n",
            "The running loss at 441 iteration is: 0.6777189435759728\n",
            "The running loss at 442 iteration is: 0.6761550350442843\n",
            "The running loss at 443 iteration is: 0.6748577136315269\n",
            "The running loss at 444 iteration is: 0.6732989088260366\n",
            "The running loss at 445 iteration is: 0.6719320025160732\n",
            "The running loss at 446 iteration is: 0.6704638200451225\n",
            "The running loss at 447 iteration is: 0.669078994819251\n",
            "The running loss at 448 iteration is: 0.6677087323809696\n",
            "The running loss at 449 iteration is: 0.6662603920465087\n",
            "The running loss at 450 iteration is: 0.6648884727648092\n",
            "The running loss at 451 iteration is: 0.66350840085213\n",
            "The running loss at 452 iteration is: 0.6621517671667597\n",
            "The running loss at 453 iteration is: 0.6606092976451704\n",
            "The running loss at 454 iteration is: 0.6593365031985613\n",
            "The running loss at 455 iteration is: 0.657981604110906\n",
            "The running loss at 456 iteration is: 0.6566004332395329\n",
            "The running loss at 457 iteration is: 0.655252447918467\n",
            "The running loss at 458 iteration is: 0.6539063480300169\n",
            "The running loss at 459 iteration is: 0.6525490628069028\n",
            "The running loss at 460 iteration is: 0.651191123964465\n",
            "The running loss at 461 iteration is: 0.6497479513156168\n",
            "The running loss at 462 iteration is: 0.6483884983491591\n",
            "The running loss at 463 iteration is: 0.6470495421377525\n",
            "The running loss at 464 iteration is: 0.64569889320266\n",
            "The running loss at 465 iteration is: 0.6445263596303014\n",
            "The running loss at 466 iteration is: 0.6430912086760067\n",
            "The running loss at 467 iteration is: 0.6417585561265754\n",
            "The running loss at 468 iteration is: 0.640405358572534\n",
            "The running loss at 469 iteration is: 0.6389727276069511\n",
            "The running loss at 470 iteration is: 0.6378276581750097\n",
            "The running loss at 471 iteration is: 0.6364784331325112\n",
            "The running loss at 472 iteration is: 0.6352394151403749\n",
            "The running loss at 473 iteration is: 0.6338936453067621\n",
            "The running loss at 474 iteration is: 0.6324886139116039\n",
            "The running loss at 475 iteration is: 0.6313109151954442\n",
            "The running loss at 476 iteration is: 0.6299005800818958\n",
            "The running loss at 477 iteration is: 0.6287460890069733\n",
            "The running loss at 478 iteration is: 0.6273286211854762\n",
            "The running loss at 479 iteration is: 0.6261058823306807\n",
            "The running loss at 480 iteration is: 0.6249717385154915\n",
            "The running loss at 481 iteration is: 0.6235289400538888\n",
            "The running loss at 482 iteration is: 0.6223037927960349\n",
            "The running loss at 483 iteration is: 0.6208969519944006\n",
            "The running loss at 484 iteration is: 0.6196599365376163\n",
            "The running loss at 485 iteration is: 0.618502556112088\n",
            "The running loss at 486 iteration is: 0.6172900623273486\n",
            "The running loss at 487 iteration is: 0.6160900918116077\n",
            "The running loss at 488 iteration is: 0.6146753767629402\n",
            "The running loss at 489 iteration is: 0.6134454523646803\n",
            "The running loss at 490 iteration is: 0.6122197674567447\n",
            "The running loss at 491 iteration is: 0.6110151004247447\n",
            "The running loss at 492 iteration is: 0.6097804395840708\n",
            "The running loss at 493 iteration is: 0.6085559986699535\n",
            "The running loss at 494 iteration is: 0.6073526859004896\n",
            "The running loss at 495 iteration is: 0.6061328096938966\n",
            "The running loss at 496 iteration is: 0.6048365575553697\n",
            "The running loss at 497 iteration is: 0.6036275599240574\n",
            "The running loss at 498 iteration is: 0.6024093125813044\n",
            "The running loss at 499 iteration is: 0.6011943274463412\n",
            "The running loss at 500 iteration is: 0.6000662230022169\n",
            "The running loss at 501 iteration is: 0.598863458093676\n",
            "The running loss at 502 iteration is: 0.5976402616106373\n",
            "The running loss at 503 iteration is: 0.5963808847320612\n",
            "The running loss at 504 iteration is: 0.595152087282362\n",
            "The running loss at 505 iteration is: 0.5941247037870404\n",
            "The running loss at 506 iteration is: 0.5928428320173529\n",
            "The running loss at 507 iteration is: 0.5916352851592279\n",
            "The running loss at 508 iteration is: 0.5905234774282327\n",
            "The running loss at 509 iteration is: 0.5893299748390065\n",
            "The running loss at 510 iteration is: 0.5882040932204614\n",
            "The running loss at 511 iteration is: 0.5869374196882783\n",
            "The running loss at 512 iteration is: 0.5859137508090226\n",
            "The running loss at 513 iteration is: 0.5846203110030616\n",
            "The running loss at 514 iteration is: 0.5835183743260001\n",
            "The running loss at 515 iteration is: 0.5823422610973121\n",
            "The running loss at 516 iteration is: 0.5812374923935786\n",
            "The running loss at 517 iteration is: 0.5799394095627469\n",
            "The running loss at 518 iteration is: 0.5788608934555752\n",
            "The running loss at 519 iteration is: 0.5778476973255232\n",
            "The running loss at 520 iteration is: 0.5765714575301452\n",
            "The running loss at 521 iteration is: 0.5754637346928799\n",
            "The running loss at 522 iteration is: 0.5743609980141402\n",
            "The running loss at 523 iteration is: 0.5731215734188924\n",
            "The running loss at 524 iteration is: 0.5720070826366491\n",
            "The running loss at 525 iteration is: 0.570940925314848\n",
            "The running loss at 526 iteration is: 0.5698218552701495\n",
            "The running loss at 527 iteration is: 0.5687442765106193\n",
            "The running loss at 528 iteration is: 0.5676467203896693\n",
            "The running loss at 529 iteration is: 0.5665661349096193\n",
            "The running loss at 530 iteration is: 0.5654686257931452\n",
            "The running loss at 531 iteration is: 0.5643749847221088\n",
            "The running loss at 532 iteration is: 0.5631277412846848\n",
            "The running loss at 533 iteration is: 0.561960879596434\n",
            "The running loss at 534 iteration is: 0.5610497929403365\n",
            "The running loss at 535 iteration is: 0.5599585876406077\n",
            "The running loss at 536 iteration is: 0.5588809652852599\n",
            "The running loss at 537 iteration is: 0.5577212592669392\n",
            "The running loss at 538 iteration is: 0.5566399517731077\n",
            "The running loss at 539 iteration is: 0.5555610055269179\n",
            "The running loss at 540 iteration is: 0.5543952537954826\n",
            "The running loss at 541 iteration is: 0.5533330769979905\n",
            "The running loss at 542 iteration is: 0.5524016297059534\n",
            "The running loss at 543 iteration is: 0.5512638540606436\n",
            "The running loss at 544 iteration is: 0.550191762452972\n",
            "The running loss at 545 iteration is: 0.549204010254173\n",
            "The running loss at 546 iteration is: 0.5481343285352045\n",
            "The running loss at 547 iteration is: 0.5469735529199928\n",
            "The running loss at 548 iteration is: 0.5459868361199868\n",
            "The running loss at 549 iteration is: 0.5449305951469953\n",
            "The running loss at 550 iteration is: 0.5437652502294151\n",
            "The running loss at 551 iteration is: 0.5428902946200356\n",
            "The running loss at 552 iteration is: 0.5417255647325254\n",
            "The running loss at 553 iteration is: 0.5407537213390892\n",
            "The running loss at 554 iteration is: 0.5396213009531134\n",
            "The running loss at 555 iteration is: 0.538703352186337\n",
            "The running loss at 556 iteration is: 0.5377332299198985\n",
            "The running loss at 557 iteration is: 0.536593754795467\n",
            "The running loss at 558 iteration is: 0.5356266653222124\n",
            "The running loss at 559 iteration is: 0.5344893334772937\n",
            "The running loss at 560 iteration is: 0.5335039287912555\n",
            "The running loss at 561 iteration is: 0.5326106220622088\n",
            "The running loss at 562 iteration is: 0.5314876253565975\n",
            "The running loss at 563 iteration is: 0.5305141042645372\n",
            "The running loss at 564 iteration is: 0.5295500601530574\n",
            "The running loss at 565 iteration is: 0.5285797604072068\n",
            "The running loss at 566 iteration is: 0.5276116831419881\n",
            "The running loss at 567 iteration is: 0.5264832852178217\n",
            "The running loss at 568 iteration is: 0.5255231113189421\n",
            "The running loss at 569 iteration is: 0.5244666110899088\n",
            "The running loss at 570 iteration is: 0.5235113888280085\n",
            "The running loss at 571 iteration is: 0.5225471053525254\n",
            "The running loss at 572 iteration is: 0.5215720216325705\n",
            "The running loss at 573 iteration is: 0.5206266995216933\n",
            "The running loss at 574 iteration is: 0.5196642446258026\n",
            "The running loss at 575 iteration is: 0.5186154603358129\n",
            "The running loss at 576 iteration is: 0.5176642260724026\n",
            "The running loss at 577 iteration is: 0.5167214581166578\n",
            "The running loss at 578 iteration is: 0.5156735563168846\n",
            "The running loss at 579 iteration is: 0.5147169149309556\n",
            "The running loss at 580 iteration is: 0.513761537416729\n",
            "The running loss at 581 iteration is: 0.512725422196346\n",
            "The running loss at 582 iteration is: 0.5117782509785855\n",
            "The running loss at 583 iteration is: 0.5109025494126008\n",
            "The running loss at 584 iteration is: 0.5099506407378336\n",
            "The running loss at 585 iteration is: 0.509016234184892\n",
            "The running loss at 586 iteration is: 0.5079751129632987\n",
            "The running loss at 587 iteration is: 0.5071807706678715\n",
            "The running loss at 588 iteration is: 0.5061731405966974\n",
            "The running loss at 589 iteration is: 0.5051453562091411\n",
            "The running loss at 590 iteration is: 0.5041951766350894\n",
            "The running loss at 591 iteration is: 0.5033260018860504\n",
            "The running loss at 592 iteration is: 0.5024002165669816\n",
            "The running loss at 593 iteration is: 0.5015309597774106\n",
            "The running loss at 594 iteration is: 0.5004980755656828\n",
            "The running loss at 595 iteration is: 0.4994865899436326\n",
            "The running loss at 596 iteration is: 0.4987091186148005\n",
            "The running loss at 597 iteration is: 0.49770733941214923\n",
            "The running loss at 598 iteration is: 0.49683177782255933\n",
            "The running loss at 599 iteration is: 0.4958244335709743\n",
            "The running loss at 600 iteration is: 0.4950414791812787\n",
            "The running loss at 601 iteration is: 0.4940247757612237\n",
            "The running loss at 602 iteration is: 0.4931807147898263\n",
            "The running loss at 603 iteration is: 0.4923115381413929\n",
            "The running loss at 604 iteration is: 0.49130921991827436\n",
            "The running loss at 605 iteration is: 0.490460771279258\n",
            "The running loss at 606 iteration is: 0.4896005613197054\n",
            "The running loss at 607 iteration is: 0.4885994215280576\n",
            "The running loss at 608 iteration is: 0.4877451319816331\n",
            "The running loss at 609 iteration is: 0.4868901795815429\n",
            "The running loss at 610 iteration is: 0.4860611937922312\n",
            "The running loss at 611 iteration is: 0.48504439731409227\n",
            "The running loss at 612 iteration is: 0.48420219932456593\n",
            "The running loss at 613 iteration is: 0.48336041730465756\n",
            "The running loss at 614 iteration is: 0.4825069112714127\n",
            "The running loss at 615 iteration is: 0.48165550567634835\n",
            "The running loss at 616 iteration is: 0.4806732966421307\n",
            "The running loss at 617 iteration is: 0.4797571002658991\n",
            "The running loss at 618 iteration is: 0.47890744067309093\n",
            "The running loss at 619 iteration is: 0.4780599059447811\n",
            "The running loss at 620 iteration is: 0.47722320435631305\n",
            "The running loss at 621 iteration is: 0.4763017320978044\n",
            "The running loss at 622 iteration is: 0.47547291387841945\n",
            "The running loss at 623 iteration is: 0.4746279973923765\n",
            "The running loss at 624 iteration is: 0.47372781952422405\n",
            "The running loss at 625 iteration is: 0.47288451287242966\n",
            "The running loss at 626 iteration is: 0.47205953701010833\n",
            "The running loss at 627 iteration is: 0.47114204331473647\n",
            "The running loss at 628 iteration is: 0.47031145245414424\n",
            "The running loss at 629 iteration is: 0.4695464961813553\n",
            "The running loss at 630 iteration is: 0.4687153262342869\n",
            "The running loss at 631 iteration is: 0.46781027463432184\n",
            "The running loss at 632 iteration is: 0.46697310719395124\n",
            "The running loss at 633 iteration is: 0.46607978810688117\n",
            "The running loss at 634 iteration is: 0.4653915877882444\n",
            "The running loss at 635 iteration is: 0.46448118242293335\n",
            "The running loss at 636 iteration is: 0.4636650869746015\n",
            "The running loss at 637 iteration is: 0.46276573868746196\n",
            "The running loss at 638 iteration is: 0.4620140842875372\n",
            "The running loss at 639 iteration is: 0.46119015109997336\n",
            "The running loss at 640 iteration is: 0.4603026099748295\n",
            "The running loss at 641 iteration is: 0.45954528475985545\n",
            "The running loss at 642 iteration is: 0.45872322372917246\n",
            "The running loss at 643 iteration is: 0.4578195640048263\n",
            "The running loss at 644 iteration is: 0.45709214170421053\n",
            "The running loss at 645 iteration is: 0.456188588321436\n",
            "The running loss at 646 iteration is: 0.45551739057990276\n",
            "The running loss at 647 iteration is: 0.45462657685337704\n",
            "The running loss at 648 iteration is: 0.4538744941280111\n",
            "The running loss at 649 iteration is: 0.45299243551795776\n",
            "The running loss at 650 iteration is: 0.45224932706815313\n",
            "The running loss at 651 iteration is: 0.4513615367883923\n",
            "The running loss at 652 iteration is: 0.4506768354045539\n",
            "The running loss at 653 iteration is: 0.44980794673114116\n",
            "The running loss at 654 iteration is: 0.44906627220496287\n",
            "The running loss at 655 iteration is: 0.4483272252394763\n",
            "The running loss at 656 iteration is: 0.447435384950028\n",
            "The running loss at 657 iteration is: 0.4466978379018112\n",
            "The running loss at 658 iteration is: 0.44595904285426813\n",
            "The running loss at 659 iteration is: 0.4450854318067797\n",
            "The running loss at 660 iteration is: 0.4443505034270241\n",
            "The running loss at 661 iteration is: 0.4436241597392466\n",
            "The running loss at 662 iteration is: 0.4428153526968827\n",
            "The running loss at 663 iteration is: 0.44192741274914116\n",
            "The running loss at 664 iteration is: 0.44119550034085925\n",
            "The running loss at 665 iteration is: 0.4404717400013008\n",
            "The running loss at 666 iteration is: 0.4397291997845646\n",
            "The running loss at 667 iteration is: 0.43900675732798355\n",
            "The running loss at 668 iteration is: 0.43805121178779294\n",
            "The running loss at 669 iteration is: 0.43732130353462495\n",
            "The running loss at 670 iteration is: 0.43660845779127444\n",
            "The running loss at 671 iteration is: 0.4358787264938956\n",
            "The running loss at 672 iteration is: 0.4350704481141699\n",
            "The running loss at 673 iteration is: 0.4343437481556852\n",
            "The running loss at 674 iteration is: 0.433606980876157\n",
            "The running loss at 675 iteration is: 0.43281728742003506\n",
            "The running loss at 676 iteration is: 0.4321002290678426\n",
            "The running loss at 677 iteration is: 0.4313749879855282\n",
            "The running loss at 678 iteration is: 0.4305876957948284\n",
            "The running loss at 679 iteration is: 0.4298541819687687\n",
            "The running loss at 680 iteration is: 0.42906884807886053\n",
            "The running loss at 681 iteration is: 0.42835499266672844\n",
            "The running loss at 682 iteration is: 0.42756065196979237\n",
            "The running loss at 683 iteration is: 0.4268305809221678\n",
            "The running loss at 684 iteration is: 0.4261199430807861\n",
            "The running loss at 685 iteration is: 0.425336509789766\n",
            "The running loss at 686 iteration is: 0.4246954753500073\n",
            "The running loss at 687 iteration is: 0.42396796802498127\n",
            "The running loss at 688 iteration is: 0.4231800981171821\n",
            "The running loss at 689 iteration is: 0.422461901304156\n",
            "The running loss at 690 iteration is: 0.42183148200702264\n",
            "The running loss at 691 iteration is: 0.42103531261116856\n",
            "The running loss at 692 iteration is: 0.42033825383496054\n",
            "The running loss at 693 iteration is: 0.41954282419885414\n",
            "The running loss at 694 iteration is: 0.4188976672010402\n",
            "The running loss at 695 iteration is: 0.4182017279048886\n",
            "The running loss at 696 iteration is: 0.4174088481373137\n",
            "The running loss at 697 iteration is: 0.41677449991508947\n",
            "The running loss at 698 iteration is: 0.41606218745128254\n",
            "The running loss at 699 iteration is: 0.4152889605162641\n",
            "The running loss at 700 iteration is: 0.41465632451092965\n",
            "The running loss at 701 iteration is: 0.4138665962109422\n",
            "The running loss at 702 iteration is: 0.413218317308733\n",
            "The running loss at 703 iteration is: 0.4125362863244652\n",
            "The running loss at 704 iteration is: 0.4117557076262401\n",
            "The running loss at 705 iteration is: 0.41112562655167756\n",
            "The running loss at 706 iteration is: 0.4103492548537009\n",
            "The running loss at 707 iteration is: 0.4097120679355244\n",
            "The running loss at 708 iteration is: 0.4089440011821895\n",
            "The running loss at 709 iteration is: 0.40830735043110417\n",
            "The running loss at 710 iteration is: 0.4076723378508803\n",
            "The running loss at 711 iteration is: 0.4068986210812942\n",
            "The running loss at 712 iteration is: 0.4062715686590038\n",
            "The running loss at 713 iteration is: 0.40551583191689705\n",
            "The running loss at 714 iteration is: 0.4048910881620839\n",
            "The running loss at 715 iteration is: 0.4042578139885919\n",
            "The running loss at 716 iteration is: 0.4034950467410435\n",
            "The running loss at 717 iteration is: 0.4028546451651631\n",
            "The running loss at 718 iteration is: 0.4022408900750502\n",
            "The running loss at 719 iteration is: 0.4014710786643411\n",
            "The running loss at 720 iteration is: 0.4008488843507705\n",
            "The running loss at 721 iteration is: 0.40014187792799066\n",
            "The running loss at 722 iteration is: 0.3994016326144707\n",
            "The running loss at 723 iteration is: 0.39877095897683706\n",
            "The running loss at 724 iteration is: 0.39814253018494056\n",
            "The running loss at 725 iteration is: 0.39752443934863435\n",
            "The running loss at 726 iteration is: 0.39683657712218245\n",
            "The running loss at 727 iteration is: 0.39608022469415216\n",
            "The running loss at 728 iteration is: 0.39545396595735444\n",
            "The running loss at 729 iteration is: 0.3948467606368003\n",
            "The running loss at 730 iteration is: 0.3941428463930459\n",
            "The running loss at 731 iteration is: 0.39352658009455405\n",
            "The running loss at 732 iteration is: 0.39290238226458946\n",
            "The running loss at 733 iteration is: 0.39221094120331673\n",
            "The running loss at 734 iteration is: 0.3915955397942636\n",
            "The running loss at 735 iteration is: 0.39098065193211357\n",
            "The running loss at 736 iteration is: 0.390307575351172\n",
            "The running loss at 737 iteration is: 0.3896857903918469\n",
            "The running loss at 738 iteration is: 0.38898795227745553\n",
            "The running loss at 739 iteration is: 0.38838424337907373\n",
            "The running loss at 740 iteration is: 0.3877810084600765\n",
            "The running loss at 741 iteration is: 0.3870925634832212\n",
            "The running loss at 742 iteration is: 0.3864821037623319\n",
            "The running loss at 743 iteration is: 0.3858132445208248\n",
            "The running loss at 744 iteration is: 0.3851940190217423\n",
            "The running loss at 745 iteration is: 0.38450869508835334\n",
            "The running loss at 746 iteration is: 0.3838420917122725\n",
            "The running loss at 747 iteration is: 0.3832244572750473\n",
            "The running loss at 748 iteration is: 0.38254042102044117\n",
            "The running loss at 749 iteration is: 0.3819342158445755\n",
            "The running loss at 750 iteration is: 0.3813942015660584\n",
            "The running loss at 751 iteration is: 0.3807281619857613\n",
            "The running loss at 752 iteration is: 0.3801136843622807\n",
            "The running loss at 753 iteration is: 0.37944294230250986\n",
            "The running loss at 754 iteration is: 0.3788372731528778\n",
            "The running loss at 755 iteration is: 0.378298681785632\n",
            "The running loss at 756 iteration is: 0.3776197301538465\n",
            "The running loss at 757 iteration is: 0.37696847543479495\n",
            "The running loss at 758 iteration is: 0.3763550151253841\n",
            "The running loss at 759 iteration is: 0.37581019261868\n",
            "The running loss at 760 iteration is: 0.375143731293306\n",
            "The running loss at 761 iteration is: 0.37448343746516577\n",
            "The running loss at 762 iteration is: 0.37400554021774646\n",
            "The running loss at 763 iteration is: 0.37334749685760377\n",
            "The running loss at 764 iteration is: 0.3726737080209631\n",
            "The running loss at 765 iteration is: 0.37213041122133755\n",
            "The running loss at 766 iteration is: 0.37146517048549793\n",
            "The running loss at 767 iteration is: 0.3709332155585918\n",
            "The running loss at 768 iteration is: 0.37035205845404295\n",
            "The running loss at 769 iteration is: 0.36967898935519755\n",
            "The running loss at 770 iteration is: 0.36914781622434545\n",
            "The running loss at 771 iteration is: 0.368486626190594\n",
            "The running loss at 772 iteration is: 0.3679629718245778\n",
            "The running loss at 773 iteration is: 0.36729277590638726\n",
            "The running loss at 774 iteration is: 0.3667558856187259\n",
            "The running loss at 775 iteration is: 0.3661042165122892\n",
            "The running loss at 776 iteration is: 0.36557376171349965\n",
            "The running loss at 777 iteration is: 0.36491448520700004\n",
            "The running loss at 778 iteration is: 0.3643792051120943\n",
            "The running loss at 779 iteration is: 0.3637289888587783\n",
            "The running loss at 780 iteration is: 0.3632008454485536\n",
            "The running loss at 781 iteration is: 0.36266583842768885\n",
            "The running loss at 782 iteration is: 0.3620195606601643\n",
            "The running loss at 783 iteration is: 0.3614998129171246\n",
            "The running loss at 784 iteration is: 0.3609004118804212\n",
            "The running loss at 785 iteration is: 0.3602551629962617\n",
            "The running loss at 786 iteration is: 0.35973019385711535\n",
            "The running loss at 787 iteration is: 0.35921280530863725\n",
            "The running loss at 788 iteration is: 0.3585508443659107\n",
            "The running loss at 789 iteration is: 0.3580297260458296\n",
            "The running loss at 790 iteration is: 0.3574395983300428\n",
            "The running loss at 791 iteration is: 0.35678747637109515\n",
            "The running loss at 792 iteration is: 0.3562744034650748\n",
            "The running loss at 793 iteration is: 0.3557525125092048\n",
            "The running loss at 794 iteration is: 0.35523829727412115\n",
            "The running loss at 795 iteration is: 0.354524003989561\n",
            "The running loss at 796 iteration is: 0.35399579903584044\n",
            "The running loss at 797 iteration is: 0.3534751385815324\n",
            "The running loss at 798 iteration is: 0.3528986626395995\n",
            "The running loss at 799 iteration is: 0.35237951984024796\n",
            "The running loss at 800 iteration is: 0.35186080372329137\n",
            "The running loss at 801 iteration is: 0.3511491741801379\n",
            "The running loss at 802 iteration is: 0.3506394216759962\n",
            "The running loss at 803 iteration is: 0.350113154048241\n",
            "The running loss at 804 iteration is: 0.34954886127415824\n",
            "The running loss at 805 iteration is: 0.34903114439086874\n",
            "The running loss at 806 iteration is: 0.3484589346748465\n",
            "The running loss at 807 iteration is: 0.3479428942341178\n",
            "The running loss at 808 iteration is: 0.3474185928330748\n",
            "The running loss at 809 iteration is: 0.34683918354672943\n",
            "The running loss at 810 iteration is: 0.3463328398851153\n",
            "The running loss at 811 iteration is: 0.3457539852290024\n",
            "The running loss at 812 iteration is: 0.34524799728572075\n",
            "The running loss at 813 iteration is: 0.34466251846138357\n",
            "The running loss at 814 iteration is: 0.34416582240492954\n",
            "The running loss at 815 iteration is: 0.3435803797947921\n",
            "The running loss at 816 iteration is: 0.3430762508363411\n",
            "The running loss at 817 iteration is: 0.34250916145023536\n",
            "The running loss at 818 iteration is: 0.3419334193430972\n",
            "The running loss at 819 iteration is: 0.34143048401800175\n",
            "The running loss at 820 iteration is: 0.34085613177479\n",
            "The running loss at 821 iteration is: 0.34032875897468645\n",
            "The running loss at 822 iteration is: 0.3397725861104986\n",
            "The running loss at 823 iteration is: 0.33933310676773915\n",
            "The running loss at 824 iteration is: 0.33883149259546735\n",
            "The running loss at 825 iteration is: 0.3382598824566475\n",
            "The running loss at 826 iteration is: 0.3376883298185638\n",
            "The running loss at 827 iteration is: 0.33717974010246776\n",
            "The running loss at 828 iteration is: 0.336608850739904\n",
            "The running loss at 829 iteration is: 0.3361808761261858\n",
            "The running loss at 830 iteration is: 0.3356018546769742\n",
            "The running loss at 831 iteration is: 0.33509521881111576\n",
            "The running loss at 832 iteration is: 0.3345434988525824\n",
            "The running loss at 833 iteration is: 0.3340988434053537\n",
            "The running loss at 834 iteration is: 0.33353890791784274\n",
            "The running loss at 835 iteration is: 0.3330258823649274\n",
            "The running loss at 836 iteration is: 0.332475354978435\n",
            "The running loss at 837 iteration is: 0.33203220359581664\n",
            "The running loss at 838 iteration is: 0.3314742863746713\n",
            "The running loss at 839 iteration is: 0.3309005162088585\n",
            "The running loss at 840 iteration is: 0.33047496343956284\n",
            "The running loss at 841 iteration is: 0.3299097050587019\n",
            "The running loss at 842 iteration is: 0.32942408334814627\n",
            "The running loss at 843 iteration is: 0.32898353854963674\n",
            "The running loss at 844 iteration is: 0.3284192749085812\n",
            "The running loss at 845 iteration is: 0.3279794437853016\n",
            "The running loss at 846 iteration is: 0.32742551413738574\n",
            "The running loss at 847 iteration is: 0.32686230974065933\n",
            "The running loss at 848 iteration is: 0.32643155425289605\n",
            "The running loss at 849 iteration is: 0.3258706399830212\n",
            "The running loss at 850 iteration is: 0.3254483863074564\n",
            "The running loss at 851 iteration is: 0.32487936389390376\n",
            "The running loss at 852 iteration is: 0.32445062283221937\n",
            "The running loss at 853 iteration is: 0.3239070867461737\n",
            "The running loss at 854 iteration is: 0.32346164579547043\n",
            "The running loss at 855 iteration is: 0.32291895631727074\n",
            "The running loss at 856 iteration is: 0.3224833101696842\n",
            "The running loss at 857 iteration is: 0.321924041695619\n",
            "The running loss at 858 iteration is: 0.3215046379582698\n",
            "The running loss at 859 iteration is: 0.3209563467598574\n",
            "The running loss at 860 iteration is: 0.3205285536001605\n",
            "The running loss at 861 iteration is: 0.3200247388637593\n",
            "The running loss at 862 iteration is: 0.31947830478267103\n",
            "The running loss at 863 iteration is: 0.31905098162028767\n",
            "The running loss at 864 iteration is: 0.3184958695356353\n",
            "The running loss at 865 iteration is: 0.318079542044488\n",
            "The running loss at 866 iteration is: 0.31763760306135413\n",
            "The running loss at 867 iteration is: 0.317038779986723\n",
            "The running loss at 868 iteration is: 0.31661468660318054\n",
            "The running loss at 869 iteration is: 0.316182765618863\n",
            "The running loss at 870 iteration is: 0.31562957907117867\n",
            "The running loss at 871 iteration is: 0.3152145880989717\n",
            "The running loss at 872 iteration is: 0.31471473361203617\n",
            "The running loss at 873 iteration is: 0.31417960682268725\n",
            "The running loss at 874 iteration is: 0.31374898767427667\n",
            "The running loss at 875 iteration is: 0.31332689641188577\n",
            "The running loss at 876 iteration is: 0.3128530836368529\n",
            "The running loss at 877 iteration is: 0.3123029721589212\n",
            "The running loss at 878 iteration is: 0.31188189753293843\n",
            "The running loss at 879 iteration is: 0.31140125910140526\n",
            "The running loss at 880 iteration is: 0.3109721085745055\n",
            "The running loss at 881 iteration is: 0.31056006910580525\n",
            "The running loss at 882 iteration is: 0.3099523333129159\n",
            "The running loss at 883 iteration is: 0.3095244788072358\n",
            "The running loss at 884 iteration is: 0.3091216163098824\n",
            "The running loss at 885 iteration is: 0.30862629123388585\n",
            "The running loss at 886 iteration is: 0.3082160282327391\n",
            "The running loss at 887 iteration is: 0.3077294688169137\n",
            "The running loss at 888 iteration is: 0.30720021292432964\n",
            "The running loss at 889 iteration is: 0.3066986307068899\n",
            "The running loss at 890 iteration is: 0.30631453586184576\n",
            "The running loss at 891 iteration is: 0.3058209627683836\n",
            "The running loss at 892 iteration is: 0.30541206615733635\n",
            "The running loss at 893 iteration is: 0.3049715133222798\n",
            "The running loss at 894 iteration is: 0.3045116301663059\n",
            "The running loss at 895 iteration is: 0.3040956360603447\n",
            "The running loss at 896 iteration is: 0.30362082540885016\n",
            "The running loss at 897 iteration is: 0.30313803201763917\n",
            "The running loss at 898 iteration is: 0.30272290206314606\n",
            "The running loss at 899 iteration is: 0.3022489500160159\n",
            "The running loss at 900 iteration is: 0.30183482713708043\n",
            "The running loss at 901 iteration is: 0.30136120424063784\n",
            "The running loss at 902 iteration is: 0.30094753695475496\n",
            "The running loss at 903 iteration is: 0.300483344334834\n",
            "The running loss at 904 iteration is: 0.29999467123011786\n",
            "The running loss at 905 iteration is: 0.29958190663618867\n",
            "The running loss at 906 iteration is: 0.2991105921871012\n",
            "The running loss at 907 iteration is: 0.2987140200670554\n",
            "The running loss at 908 iteration is: 0.2982352138368342\n",
            "The running loss at 909 iteration is: 0.29776536958292893\n",
            "The running loss at 910 iteration is: 0.2973454448036327\n",
            "The running loss at 911 iteration is: 0.2970006714735681\n",
            "The running loss at 912 iteration is: 0.29652322496620837\n",
            "The running loss at 913 iteration is: 0.29612906594783195\n",
            "The running loss at 914 iteration is: 0.29564418927100566\n",
            "The running loss at 915 iteration is: 0.29517576540889606\n",
            "The running loss at 916 iteration is: 0.29472420601931726\n",
            "The running loss at 917 iteration is: 0.29430673385359224\n",
            "The running loss at 918 iteration is: 0.2939716584853096\n",
            "The running loss at 919 iteration is: 0.2934884162175273\n",
            "The running loss at 920 iteration is: 0.2930295566078569\n",
            "The running loss at 921 iteration is: 0.2925632713282499\n",
            "The running loss at 922 iteration is: 0.2921555032674751\n",
            "The running loss at 923 iteration is: 0.29180559363869596\n",
            "The running loss at 924 iteration is: 0.29134828400876567\n",
            "The running loss at 925 iteration is: 0.29086734473509485\n",
            "The running loss at 926 iteration is: 0.29041085293406105\n",
            "The running loss at 927 iteration is: 0.29010412170883304\n",
            "The running loss at 928 iteration is: 0.2896645310575215\n",
            "The running loss at 929 iteration is: 0.2892001857914751\n",
            "The running loss at 930 iteration is: 0.28884428846954613\n",
            "The running loss at 931 iteration is: 0.2883901219822129\n",
            "The running loss at 932 iteration is: 0.2879186549749251\n",
            "The running loss at 933 iteration is: 0.2875872535809346\n",
            "The running loss at 934 iteration is: 0.2871175725470068\n",
            "The running loss at 935 iteration is: 0.28665583329431316\n",
            "The running loss at 936 iteration is: 0.28630924051943557\n",
            "The running loss at 937 iteration is: 0.2858563498816632\n",
            "The running loss at 938 iteration is: 0.28539567209177696\n",
            "The running loss at 939 iteration is: 0.2851151906940103\n",
            "The running loss at 940 iteration is: 0.2846471807811723\n",
            "The running loss at 941 iteration is: 0.2843019236654577\n",
            "The running loss at 942 iteration is: 0.28385044195197207\n",
            "The running loss at 943 iteration is: 0.28339162944182184\n",
            "The running loss at 944 iteration is: 0.28298190453272115\n",
            "The running loss at 945 iteration is: 0.28253971292741376\n",
            "The running loss at 946 iteration is: 0.28218761348944316\n",
            "The running loss at 947 iteration is: 0.2817459655507751\n",
            "The running loss at 948 iteration is: 0.2814019820186962\n",
            "The running loss at 949 iteration is: 0.2809373823029386\n",
            "The running loss at 950 iteration is: 0.2806025764818401\n",
            "The running loss at 951 iteration is: 0.2801375946332256\n",
            "The running loss at 952 iteration is: 0.27981870077091586\n",
            "The running loss at 953 iteration is: 0.27935509760597804\n",
            "The running loss at 954 iteration is: 0.27902091034256093\n",
            "The running loss at 955 iteration is: 0.2785657477750983\n",
            "The running loss at 956 iteration is: 0.27815946463770574\n",
            "The running loss at 957 iteration is: 0.277721154250331\n",
            "The running loss at 958 iteration is: 0.27737185251808155\n",
            "The running loss at 959 iteration is: 0.2769179855548739\n",
            "The running loss at 960 iteration is: 0.27659353565162703\n",
            "The running loss at 961 iteration is: 0.27624513337375356\n",
            "The running loss at 962 iteration is: 0.2757357604335497\n",
            "The running loss at 963 iteration is: 0.27539581359379584\n",
            "The running loss at 964 iteration is: 0.27495939623325033\n",
            "The running loss at 965 iteration is: 0.2746196133494828\n",
            "The running loss at 966 iteration is: 0.2742968186799916\n",
            "The running loss at 967 iteration is: 0.2737814852604484\n",
            "The running loss at 968 iteration is: 0.27344215443291364\n",
            "The running loss at 969 iteration is: 0.27311157368799777\n",
            "The running loss at 970 iteration is: 0.27259805796501324\n",
            "The running loss at 971 iteration is: 0.2722592297050942\n",
            "The running loss at 972 iteration is: 0.2719371772346729\n",
            "The running loss at 973 iteration is: 0.2714327250877532\n",
            "The running loss at 974 iteration is: 0.27108658461435287\n",
            "The running loss at 975 iteration is: 0.2707655429482991\n",
            "The running loss at 976 iteration is: 0.2703656912109087\n",
            "The running loss at 977 iteration is: 0.26993250531467244\n",
            "The running loss at 978 iteration is: 0.26959544384447426\n",
            "The running loss at 979 iteration is: 0.26920420951313867\n",
            "The running loss at 980 iteration is: 0.26887551926140807\n",
            "The running loss at 981 iteration is: 0.26843643611735124\n",
            "The running loss at 982 iteration is: 0.2680534940991182\n",
            "The running loss at 983 iteration is: 0.26771779963899034\n",
            "The running loss at 984 iteration is: 0.26739060057286207\n",
            "The running loss at 985 iteration is: 0.2669930238716095\n",
            "The running loss at 986 iteration is: 0.26654740646743974\n",
            "The running loss at 987 iteration is: 0.2661651972134578\n",
            "The running loss at 988 iteration is: 0.2658319551376722\n",
            "The running loss at 989 iteration is: 0.2654580929995341\n",
            "The running loss at 990 iteration is: 0.2651234178812847\n",
            "The running loss at 991 iteration is: 0.264743106367361\n",
            "The running loss at 992 iteration is: 0.26429971987477724\n",
            "The running loss at 993 iteration is: 0.26391112713622283\n",
            "The running loss at 994 iteration is: 0.26357864781508006\n",
            "The running loss at 995 iteration is: 0.2632149655079891\n",
            "The running loss at 996 iteration is: 0.26287422390425985\n",
            "The running loss at 997 iteration is: 0.2624867069652488\n",
            "The running loss at 998 iteration is: 0.2621631951889413\n",
            "The running loss at 999 iteration is: 0.2617768635766192\n",
            "The running loss at 1000 iteration is: 0.261468120281136\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.2824640533375735e-06\n"
          ]
        }
      ],
      "source": [
        "run_train(lr=0.03, num_e= 1000, isOn=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-10foETutG48",
        "outputId": "b2c57d58-307d-4221-8297-675688b23c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The running loss at 1 iteration is: 0.2610662196204867\n",
            "The running loss at 2 iteration is: 59.94121684564872\n",
            "The running loss at 3 iteration is: 1155.4659034817894\n",
            "The running loss at 4 iteration is: 8.632044306954873\n",
            "The running loss at 5 iteration is: 392.1565766238396\n",
            "The running loss at 6 iteration is: 736.9623336451132\n",
            "The running loss at 7 iteration is: 563.5753037582745\n",
            "The running loss at 8 iteration is: 169.74956766307682\n",
            "The running loss at 9 iteration is: 4.573914261749537\n",
            "The running loss at 10 iteration is: 253.38389171100948\n",
            "The running loss at 11 iteration is: 409.0936025781135\n",
            "The running loss at 12 iteration is: 198.36010832692997\n",
            "The running loss at 13 iteration is: 10.148580097103372\n",
            "The running loss at 14 iteration is: 49.50918051967669\n",
            "The running loss at 15 iteration is: 179.15191722181618\n",
            "The running loss at 16 iteration is: 228.14720176190454\n",
            "The running loss at 17 iteration is: 157.5774222984036\n",
            "The running loss at 18 iteration is: 44.412772755307586\n",
            "The running loss at 19 iteration is: 1.307042262551666\n",
            "The running loss at 20 iteration is: 64.06605211886549\n",
            "The running loss at 21 iteration is: 130.89771403144798\n",
            "The running loss at 22 iteration is: 101.41401317386585\n",
            "The running loss at 23 iteration is: 25.86867174404481\n",
            "The running loss at 24 iteration is: 1.431126454759713\n",
            "The running loss at 25 iteration is: 36.999231053451645\n",
            "The running loss at 26 iteration is: 74.05190781700371\n",
            "The running loss at 27 iteration is: 68.9312095267414\n",
            "The running loss at 28 iteration is: 30.364304048377157\n",
            "The running loss at 29 iteration is: 1.525276111451729\n",
            "The running loss at 30 iteration is: 11.864486900037202\n",
            "The running loss at 31 iteration is: 40.393143637689604\n",
            "The running loss at 32 iteration is: 43.38405635981397\n",
            "The running loss at 33 iteration is: 18.176460594724997\n",
            "The running loss at 34 iteration is: 0.6374851210880293\n",
            "The running loss at 35 iteration is: 8.713599239495652\n",
            "The running loss at 36 iteration is: 24.9347827332206\n",
            "The running loss at 37 iteration is: 26.530934682972674\n",
            "The running loss at 38 iteration is: 12.483600800203117\n",
            "The running loss at 39 iteration is: 0.8567743577359717\n",
            "The running loss at 40 iteration is: 4.855052189577045\n",
            "The running loss at 41 iteration is: 15.594923178435792\n",
            "The running loss at 42 iteration is: 15.795181139715707\n",
            "The running loss at 43 iteration is: 5.802902545604889\n",
            "The running loss at 44 iteration is: 0.3511547281818989\n",
            "The running loss at 45 iteration is: 4.961796307028785\n",
            "The running loss at 46 iteration is: 10.631090382204604\n",
            "The running loss at 47 iteration is: 8.888811325833556\n",
            "The running loss at 48 iteration is: 2.6300254047794334\n",
            "The running loss at 49 iteration is: 0.5206665787302609\n",
            "The running loss at 50 iteration is: 4.344626540493897\n",
            "The running loss at 51 iteration is: 7.034956711233593\n",
            "The running loss at 52 iteration is: 4.269513879656494\n",
            "The running loss at 53 iteration is: 0.6741774834101724\n",
            "The running loss at 54 iteration is: 1.2702027578483344\n",
            "The running loss at 55 iteration is: 4.002636800328921\n",
            "The running loss at 56 iteration is: 4.1787573893166385\n",
            "The running loss at 57 iteration is: 1.6808543221970147\n",
            "The running loss at 58 iteration is: 0.37022601965692514\n",
            "The running loss at 59 iteration is: 1.7831696436360327\n",
            "The running loss at 60 iteration is: 3.047616155470456\n",
            "The running loss at 61 iteration is: 1.982884415015711\n",
            "The running loss at 62 iteration is: 0.4803551468208935\n",
            "The running loss at 63 iteration is: 0.7903202980675572\n",
            "The running loss at 64 iteration is: 1.9250528267006615\n",
            "The running loss at 65 iteration is: 1.8188132132724675\n",
            "The running loss at 66 iteration is: 0.7215670895512069\n",
            "The running loss at 67 iteration is: 0.4214639831346564\n",
            "The running loss at 68 iteration is: 1.1565801157559825\n",
            "The running loss at 69 iteration is: 1.4424362219964173\n",
            "The running loss at 70 iteration is: 0.7900840988779501\n",
            "The running loss at 71 iteration is: 0.36345754435027133\n",
            "The running loss at 72 iteration is: 0.7468367504581326\n",
            "The running loss at 73 iteration is: 1.0871872659421502\n",
            "The running loss at 74 iteration is: 0.7652398119633864\n",
            "The running loss at 75 iteration is: 0.3761556681336891\n",
            "The running loss at 76 iteration is: 0.5356548225364313\n",
            "The running loss at 77 iteration is: 0.8278652920622523\n",
            "The running loss at 78 iteration is: 0.680624592823127\n",
            "The running loss at 79 iteration is: 0.38780994414189246\n",
            "The running loss at 80 iteration is: 0.4466326235762931\n",
            "The running loss at 81 iteration is: 0.6570797796232385\n",
            "The running loss at 82 iteration is: 0.5958338923326097\n",
            "The running loss at 83 iteration is: 0.3910071401318103\n",
            "The running loss at 84 iteration is: 0.4043512696725578\n",
            "The running loss at 85 iteration is: 0.5509636292008825\n",
            "The running loss at 86 iteration is: 0.5230676319480974\n",
            "The running loss at 87 iteration is: 0.38351395510978936\n",
            "The running loss at 88 iteration is: 0.38762259482175787\n",
            "The running loss at 89 iteration is: 0.4860822816468467\n",
            "The running loss at 90 iteration is: 0.4687579435531382\n",
            "The running loss at 91 iteration is: 0.3751802125471925\n",
            "The running loss at 92 iteration is: 0.3790903469224891\n",
            "The running loss at 93 iteration is: 0.445031296061037\n",
            "The running loss at 94 iteration is: 0.4292760217629127\n",
            "The running loss at 95 iteration is: 0.3671717864056457\n",
            "The running loss at 96 iteration is: 0.37552125722119867\n",
            "The running loss at 97 iteration is: 0.4184698034017041\n",
            "The running loss at 98 iteration is: 0.4017220868473484\n",
            "The running loss at 99 iteration is: 0.361733358349024\n",
            "The running loss at 100 iteration is: 0.37325799027797196\n",
            "The running loss at 101 iteration is: 0.39965208229008176\n",
            "The running loss at 102 iteration is: 0.3827270796619255\n",
            "The running loss at 103 iteration is: 0.35866091652048876\n",
            "The running loss at 104 iteration is: 0.37156521859775665\n",
            "The running loss at 105 iteration is: 0.38581680943017443\n",
            "The running loss at 106 iteration is: 0.3699691020071714\n",
            "The running loss at 107 iteration is: 0.357572626169859\n",
            "The running loss at 108 iteration is: 0.3695274643811398\n",
            "The running loss at 109 iteration is: 0.37514499442578364\n",
            "The running loss at 110 iteration is: 0.36196414001042515\n",
            "The running loss at 111 iteration is: 0.35750705232777535\n",
            "The running loss at 112 iteration is: 0.3670831635320826\n",
            "The running loss at 113 iteration is: 0.36694486598038\n",
            "The running loss at 114 iteration is: 0.3573359687126882\n",
            "The running loss at 115 iteration is: 0.35788740503479066\n",
            "The running loss at 116 iteration is: 0.36399463967337925\n",
            "The running loss at 117 iteration is: 0.360854038856229\n",
            "The running loss at 118 iteration is: 0.3551297378227519\n",
            "The running loss at 119 iteration is: 0.3577955974707099\n",
            "The running loss at 120 iteration is: 0.36058391499668974\n",
            "The running loss at 121 iteration is: 0.3565373574848333\n",
            "The running loss at 122 iteration is: 0.3542666424159437\n",
            "The running loss at 123 iteration is: 0.3571393186021761\n",
            "The running loss at 124 iteration is: 0.3572111806305338\n",
            "The running loss at 125 iteration is: 0.35383604694593357\n",
            "The running loss at 126 iteration is: 0.3538053795782033\n",
            "The running loss at 127 iteration is: 0.3557417042284526\n",
            "The running loss at 128 iteration is: 0.354308006533377\n",
            "The running loss at 129 iteration is: 0.35251672441394255\n",
            "The running loss at 130 iteration is: 0.35338642178779506\n",
            "The running loss at 131 iteration is: 0.35383965193762174\n",
            "The running loss at 132 iteration is: 0.352234725592655\n",
            "The running loss at 133 iteration is: 0.35164962671818645\n",
            "The running loss at 134 iteration is: 0.35254801462867835\n",
            "The running loss at 135 iteration is: 0.351892401600897\n",
            "The running loss at 136 iteration is: 0.3507188081052287\n",
            "The running loss at 137 iteration is: 0.3509288628179203\n",
            "The running loss at 138 iteration is: 0.3512032069204429\n",
            "The running loss at 139 iteration is: 0.35029356086195146\n",
            "The running loss at 140 iteration is: 0.3497942583546001\n",
            "The running loss at 141 iteration is: 0.35016153526528404\n",
            "The running loss at 142 iteration is: 0.3497386086715295\n",
            "The running loss at 143 iteration is: 0.34904574608213057\n",
            "The running loss at 144 iteration is: 0.3489873705166859\n",
            "The running loss at 145 iteration is: 0.3490216494148104\n",
            "The running loss at 146 iteration is: 0.3484319297308629\n",
            "The running loss at 147 iteration is: 0.34804548796267226\n",
            "The running loss at 148 iteration is: 0.34805360604530067\n",
            "The running loss at 149 iteration is: 0.34779918778713775\n",
            "The running loss at 150 iteration is: 0.3473012112884528\n",
            "The running loss at 151 iteration is: 0.3471803020498258\n",
            "The running loss at 152 iteration is: 0.34696621365351493\n",
            "The running loss at 153 iteration is: 0.3465236204560179\n",
            "The running loss at 154 iteration is: 0.3463146916289232\n",
            "The running loss at 155 iteration is: 0.3461862648067745\n",
            "The running loss at 156 iteration is: 0.3458120619462457\n",
            "The running loss at 157 iteration is: 0.34544835990725464\n",
            "The running loss at 158 iteration is: 0.34530175520499723\n",
            "The running loss at 159 iteration is: 0.3450113638320196\n",
            "The running loss at 160 iteration is: 0.3447315377643418\n",
            "The running loss at 161 iteration is: 0.3444763648712708\n",
            "The running loss at 162 iteration is: 0.34424621186289606\n",
            "The running loss at 163 iteration is: 0.34400394869428963\n",
            "The running loss at 164 iteration is: 0.343620110583576\n",
            "The running loss at 165 iteration is: 0.3434864715118394\n",
            "The running loss at 166 iteration is: 0.3431824835861976\n",
            "The running loss at 167 iteration is: 0.3428788018587537\n",
            "The running loss at 168 iteration is: 0.3426937774194719\n",
            "The running loss at 169 iteration is: 0.3424255579807616\n",
            "The running loss at 170 iteration is: 0.34208781427941276\n",
            "The running loss at 171 iteration is: 0.34184221287589867\n",
            "The running loss at 172 iteration is: 0.34156278836074194\n",
            "The running loss at 173 iteration is: 0.34126882944445636\n",
            "The running loss at 174 iteration is: 0.34105437075411005\n",
            "The running loss at 175 iteration is: 0.3407771173613524\n",
            "The running loss at 176 iteration is: 0.3404842385397009\n",
            "The running loss at 177 iteration is: 0.34027972462341566\n",
            "The running loss at 178 iteration is: 0.34000635599481144\n",
            "The running loss at 179 iteration is: 0.3397227282820465\n",
            "The running loss at 180 iteration is: 0.33940334145572454\n",
            "The running loss at 181 iteration is: 0.3391481982325164\n",
            "The running loss at 182 iteration is: 0.3388599127504305\n",
            "The running loss at 183 iteration is: 0.33855932890157403\n",
            "The running loss at 184 iteration is: 0.3383955530572479\n",
            "The running loss at 185 iteration is: 0.3380998154978162\n",
            "The running loss at 186 iteration is: 0.33786331998493724\n",
            "The running loss at 187 iteration is: 0.33759154849157313\n",
            "The running loss at 188 iteration is: 0.33731626502654644\n",
            "The running loss at 189 iteration is: 0.3370132035385425\n",
            "The running loss at 190 iteration is: 0.3367099981162352\n",
            "The running loss at 191 iteration is: 0.33642307906793956\n",
            "The running loss at 192 iteration is: 0.33619119122064267\n",
            "The running loss at 193 iteration is: 0.33590007331250693\n",
            "The running loss at 194 iteration is: 0.3356034451887718\n",
            "The running loss at 195 iteration is: 0.3353249293077285\n",
            "The running loss at 196 iteration is: 0.335091904510323\n",
            "The running loss at 197 iteration is: 0.33479409942751565\n",
            "The running loss at 198 iteration is: 0.33451569566472267\n",
            "The running loss at 199 iteration is: 0.33421670924865643\n",
            "The running loss at 200 iteration is: 0.33393907286032215\n",
            "The running loss at 201 iteration is: 0.33373112163717855\n",
            "The running loss at 202 iteration is: 0.33343145619797987\n",
            "The running loss at 203 iteration is: 0.33313394295768695\n",
            "The running loss at 204 iteration is: 0.3328568603329074\n",
            "The running loss at 205 iteration is: 0.33262752080023306\n",
            "The running loss at 206 iteration is: 0.3323309543663668\n",
            "The running loss at 207 iteration is: 0.3320356807972359\n",
            "The running loss at 208 iteration is: 0.3318168178176179\n",
            "The running loss at 209 iteration is: 0.3315197439348191\n",
            "The running loss at 210 iteration is: 0.331234041984588\n",
            "The running loss at 211 iteration is: 0.3309382814732452\n",
            "The running loss at 212 iteration is: 0.33071048584915685\n",
            "The running loss at 213 iteration is: 0.33041603959800525\n",
            "The running loss at 214 iteration is: 0.3301502522820787\n",
            "The running loss at 215 iteration is: 0.3299026673260827\n",
            "The running loss at 216 iteration is: 0.32962743290374724\n",
            "The running loss at 217 iteration is: 0.3293331990900242\n",
            "The running loss at 218 iteration is: 0.32898062404257344\n",
            "The running loss at 219 iteration is: 0.3287066448572886\n",
            "The running loss at 220 iteration is: 0.32840326913233214\n",
            "The running loss at 221 iteration is: 0.32818615343696883\n",
            "The running loss at 222 iteration is: 0.32790212729674495\n",
            "The running loss at 223 iteration is: 0.32758990998602894\n",
            "The running loss at 224 iteration is: 0.3273733199566871\n",
            "The running loss at 225 iteration is: 0.3270906237267784\n",
            "The running loss at 226 iteration is: 0.32674045119269474\n",
            "The running loss at 227 iteration is: 0.32644739192430383\n",
            "The running loss at 228 iteration is: 0.3261644937769817\n",
            "The running loss at 229 iteration is: 0.32593007702909516\n",
            "The running loss at 230 iteration is: 0.3256477398523481\n",
            "The running loss at 231 iteration is: 0.32541370972488665\n",
            "The running loss at 232 iteration is: 0.32512202751708824\n",
            "The running loss at 233 iteration is: 0.324725053112529\n",
            "The running loss at 234 iteration is: 0.3245008918696436\n",
            "The running loss at 235 iteration is: 0.3242099472982001\n",
            "The running loss at 236 iteration is: 0.3239668914074413\n",
            "The running loss at 237 iteration is: 0.32369581791349755\n",
            "The running loss at 238 iteration is: 0.32346281820966877\n",
            "The running loss at 239 iteration is: 0.32306704686001125\n",
            "The running loss at 240 iteration is: 0.3227676114641004\n",
            "The running loss at 241 iteration is: 0.3225349102431202\n",
            "The running loss at 242 iteration is: 0.3222548677015923\n",
            "The running loss at 243 iteration is: 0.3220422747113785\n",
            "The running loss at 244 iteration is: 0.3216188023940445\n",
            "The running loss at 245 iteration is: 0.32139654033541315\n",
            "The running loss at 246 iteration is: 0.32110761689751377\n",
            "The running loss at 247 iteration is: 0.32088566165715476\n",
            "The running loss at 248 iteration is: 0.3204730873082217\n",
            "The running loss at 249 iteration is: 0.320232554201854\n",
            "The running loss at 250 iteration is: 0.31995391468484\n",
            "The running loss at 251 iteration is: 0.31973264765225445\n",
            "The running loss at 252 iteration is: 0.3193402023275154\n",
            "The running loss at 253 iteration is: 0.3191001076510341\n",
            "The running loss at 254 iteration is: 0.31882214554193433\n",
            "The running loss at 255 iteration is: 0.31857260648399716\n",
            "The running loss at 256 iteration is: 0.3181720203787299\n",
            "The running loss at 257 iteration is: 0.31796137006812075\n",
            "The running loss at 258 iteration is: 0.3176647493283615\n",
            "The running loss at 259 iteration is: 0.3174447379667182\n",
            "The running loss at 260 iteration is: 0.3170448959114523\n",
            "The running loss at 261 iteration is: 0.3168058240127392\n",
            "The running loss at 262 iteration is: 0.31652968825762084\n",
            "The running loss at 263 iteration is: 0.31617765997046837\n",
            "The running loss at 264 iteration is: 0.31589201720724586\n",
            "The running loss at 265 iteration is: 0.3156633760068025\n",
            "The running loss at 266 iteration is: 0.3153781127495545\n",
            "The running loss at 267 iteration is: 0.3150271613582271\n",
            "The running loss at 268 iteration is: 0.31479884321592055\n",
            "The running loss at 269 iteration is: 0.3145143280539206\n",
            "The running loss at 270 iteration is: 0.3141830464492145\n",
            "The running loss at 271 iteration is: 0.3138986403830414\n",
            "The running loss at 272 iteration is: 0.3136520488281925\n",
            "The running loss at 273 iteration is: 0.31326493026527774\n",
            "The running loss at 274 iteration is: 0.3130378836997412\n",
            "The running loss at 275 iteration is: 0.31281050196553617\n",
            "The running loss at 276 iteration is: 0.31250811843052767\n",
            "The running loss at 277 iteration is: 0.31216895581811754\n",
            "The running loss at 278 iteration is: 0.3118857018654675\n",
            "The running loss at 279 iteration is: 0.31165932677020436\n",
            "The running loss at 280 iteration is: 0.31126446566216454\n",
            "The running loss at 281 iteration is: 0.3110380281760469\n",
            "The running loss at 282 iteration is: 0.31081206345577844\n",
            "The running loss at 283 iteration is: 0.3104178900304215\n",
            "The running loss at 284 iteration is: 0.31019204151727486\n",
            "The running loss at 285 iteration is: 0.30977937345906886\n",
            "The running loss at 286 iteration is: 0.30956347788170097\n",
            "The running loss at 287 iteration is: 0.3093381427816534\n",
            "The running loss at 288 iteration is: 0.30892615062435724\n",
            "The running loss at 289 iteration is: 0.3087106076743343\n",
            "The running loss at 290 iteration is: 0.30847628382995707\n",
            "The running loss at 291 iteration is: 0.3080935721526473\n",
            "The running loss at 292 iteration is: 0.30785012291651825\n",
            "The running loss at 293 iteration is: 0.3075602734386461\n",
            "The running loss at 294 iteration is: 0.30724382839347397\n",
            "The running loss at 295 iteration is: 0.3070102024363988\n",
            "The running loss at 296 iteration is: 0.30660028493771296\n",
            "The running loss at 297 iteration is: 0.30636697241500627\n",
            "The running loss at 298 iteration is: 0.30615290444534177\n",
            "The running loss at 299 iteration is: 0.30575328274668306\n",
            "The running loss at 300 iteration is: 0.305529771670078\n",
            "The running loss at 301 iteration is: 0.305241434811808\n",
            "The running loss at 302 iteration is: 0.30489816669081343\n",
            "The running loss at 303 iteration is: 0.3046657883963942\n",
            "The running loss at 304 iteration is: 0.30427682388056343\n",
            "The running loss at 305 iteration is: 0.30405430396205513\n",
            "The running loss at 306 iteration is: 0.3038223420809091\n",
            "The running loss at 307 iteration is: 0.30342454831365706\n",
            "The running loss at 308 iteration is: 0.3032023683618374\n",
            "The running loss at 309 iteration is: 0.3028797402868754\n",
            "The running loss at 310 iteration is: 0.3025834634874721\n",
            "The running loss at 311 iteration is: 0.3023617485410617\n",
            "The running loss at 312 iteration is: 0.30202049278250725\n",
            "The running loss at 313 iteration is: 0.3017439575967425\n",
            "The running loss at 314 iteration is: 0.3014030505763012\n",
            "The running loss at 315 iteration is: 0.30117256656507757\n",
            "The running loss at 316 iteration is: 0.30088705392686904\n",
            "The running loss at 317 iteration is: 0.3005562960076137\n",
            "The running loss at 318 iteration is: 0.30031694149350835\n",
            "The running loss at 319 iteration is: 0.29994079010058583\n",
            "The running loss at 320 iteration is: 0.2997110922259808\n",
            "The running loss at 321 iteration is: 0.2993717655294609\n",
            "The running loss at 322 iteration is: 0.29908741532678224\n",
            "The running loss at 323 iteration is: 0.2988676792670385\n",
            "The running loss at 324 iteration is: 0.29851930470021054\n",
            "The running loss at 325 iteration is: 0.29829038147833775\n",
            "The running loss at 326 iteration is: 0.2979066471578434\n",
            "The running loss at 327 iteration is: 0.29768753007402227\n",
            "The running loss at 328 iteration is: 0.29744968374271397\n",
            "The running loss at 329 iteration is: 0.29704754539139816\n",
            "The running loss at 330 iteration is: 0.29681949548104947\n",
            "The running loss at 331 iteration is: 0.2965012627498304\n",
            "The running loss at 332 iteration is: 0.2962183958015097\n",
            "The running loss at 333 iteration is: 0.2958816528777434\n",
            "The running loss at 334 iteration is: 0.2956634624962681\n",
            "The running loss at 335 iteration is: 0.2952724984068309\n",
            "The running loss at 336 iteration is: 0.29505463564724604\n",
            "The running loss at 337 iteration is: 0.29481808952527594\n",
            "The running loss at 338 iteration is: 0.29447284347538794\n",
            "The running loss at 339 iteration is: 0.2942008483337251\n",
            "The running loss at 340 iteration is: 0.29385610783634153\n",
            "The running loss at 341 iteration is: 0.2936296258346264\n",
            "The running loss at 342 iteration is: 0.29325909354153484\n",
            "The running loss at 343 iteration is: 0.29302357476336255\n",
            "The running loss at 344 iteration is: 0.2926889511214627\n",
            "The running loss at 345 iteration is: 0.29246323253115586\n",
            "The running loss at 346 iteration is: 0.29218305759747587\n",
            "The running loss at 347 iteration is: 0.29184910376382506\n",
            "The running loss at 348 iteration is: 0.29163305062838263\n",
            "The running loss at 349 iteration is: 0.2912359599807912\n",
            "The running loss at 350 iteration is: 0.2910108444748928\n",
            "The running loss at 351 iteration is: 0.2906777577828757\n",
            "The running loss at 352 iteration is: 0.2904435527435497\n",
            "The running loss at 353 iteration is: 0.29007555831673904\n",
            "The running loss at 354 iteration is: 0.2898417060448149\n",
            "The running loss at 355 iteration is: 0.2896173554571127\n",
            "The running loss at 356 iteration is: 0.2892405420467636\n",
            "The running loss at 357 iteration is: 0.2890071368160103\n",
            "The running loss at 358 iteration is: 0.28866620249517794\n",
            "The running loss at 359 iteration is: 0.288442306560807\n",
            "The running loss at 360 iteration is: 0.2880665898191663\n",
            "The running loss at 361 iteration is: 0.28783370665298325\n",
            "The running loss at 362 iteration is: 0.2875028696288358\n",
            "The running loss at 363 iteration is: 0.28727984221788844\n",
            "The running loss at 364 iteration is: 0.28690490125517354\n",
            "The running loss at 365 iteration is: 0.28666334386205927\n",
            "The running loss at 366 iteration is: 0.28634292806911255\n",
            "The running loss at 367 iteration is: 0.2860757512771765\n",
            "The running loss at 368 iteration is: 0.2858440386873018\n",
            "The running loss at 369 iteration is: 0.28552408240937477\n",
            "The running loss at 370 iteration is: 0.28528336200284493\n",
            "The running loss at 371 iteration is: 0.2849100837242236\n",
            "The running loss at 372 iteration is: 0.28467890944701074\n",
            "The running loss at 373 iteration is: 0.2843504131926739\n",
            "The running loss at 374 iteration is: 0.2841289203624018\n",
            "The running loss at 375 iteration is: 0.2837473606119678\n",
            "The running loss at 376 iteration is: 0.2835261013949527\n",
            "The running loss at 377 iteration is: 0.28318905596489924\n",
            "The running loss at 378 iteration is: 0.28292399396072426\n",
            "The running loss at 379 iteration is: 0.2825782862813318\n",
            "The running loss at 380 iteration is: 0.28235763083389703\n",
            "The running loss at 381 iteration is: 0.2820400545230062\n",
            "The running loss at 382 iteration is: 0.28174777981103305\n",
            "The running loss at 383 iteration is: 0.2815274591435636\n",
            "The running loss at 384 iteration is: 0.2812104246492712\n",
            "The running loss at 385 iteration is: 0.28099066090121805\n",
            "The running loss at 386 iteration is: 0.2806023764217974\n",
            "The running loss at 387 iteration is: 0.28037336937565893\n",
            "The running loss at 388 iteration is: 0.2800479824825595\n",
            "The running loss at 389 iteration is: 0.27977554728153164\n",
            "The running loss at 390 iteration is: 0.27945059995278726\n",
            "The running loss at 391 iteration is: 0.2792224656239814\n",
            "The running loss at 392 iteration is: 0.2789070212756716\n",
            "The running loss at 393 iteration is: 0.2786166941484768\n",
            "The running loss at 394 iteration is: 0.2782924600336095\n",
            "The running loss at 395 iteration is: 0.2780739283449641\n",
            "The running loss at 396 iteration is: 0.2778556393809234\n",
            "The running loss at 397 iteration is: 0.2774699605532715\n",
            "The running loss at 398 iteration is: 0.27725188061249006\n",
            "The running loss at 399 iteration is: 0.27692872866692486\n",
            "The running loss at 400 iteration is: 0.27665830313163686\n",
            "The running loss at 401 iteration is: 0.2763264302494848\n",
            "The running loss at 402 iteration is: 0.2761088284409646\n",
            "The running loss at 403 iteration is: 0.27578658308584625\n",
            "The running loss at 404 iteration is: 0.27550738589669915\n",
            "The running loss at 405 iteration is: 0.27516712454148234\n",
            "The running loss at 406 iteration is: 0.27495936025149936\n",
            "The running loss at 407 iteration is: 0.27463779689721296\n",
            "The running loss at 408 iteration is: 0.27435946785486404\n",
            "The running loss at 409 iteration is: 0.27403849515781764\n",
            "The running loss at 410 iteration is: 0.2738220814595704\n",
            "The running loss at 411 iteration is: 0.2735443597835697\n",
            "The running loss at 412 iteration is: 0.273223892572998\n",
            "The running loss at 413 iteration is: 0.2729986573916898\n",
            "The running loss at 414 iteration is: 0.27267861250091396\n",
            "The running loss at 415 iteration is: 0.27239221748097586\n",
            "The running loss at 416 iteration is: 0.272072571725803\n",
            "The running loss at 417 iteration is: 0.27184783591588163\n",
            "The running loss at 418 iteration is: 0.2715286285576052\n",
            "The running loss at 419 iteration is: 0.27126148077059814\n",
            "The running loss at 420 iteration is: 0.2709244797813793\n",
            "The running loss at 421 iteration is: 0.2707186150764989\n",
            "The running loss at 422 iteration is: 0.2704425969840291\n",
            "The running loss at 423 iteration is: 0.27011527217805753\n",
            "The running loss at 424 iteration is: 0.2698916192600152\n",
            "The running loss at 425 iteration is: 0.2695829057600081\n",
            "The running loss at 426 iteration is: 0.26930767586260623\n",
            "The running loss at 427 iteration is: 0.2689902766856586\n",
            "The running loss at 428 iteration is: 0.268767349576724\n",
            "The running loss at 429 iteration is: 0.2683894345294224\n",
            "The running loss at 430 iteration is: 0.2681757274853016\n",
            "The running loss at 431 iteration is: 0.26784988659228315\n",
            "The running loss at 432 iteration is: 0.26763664302519713\n",
            "The running loss at 433 iteration is: 0.2673536931225687\n",
            "The running loss at 434 iteration is: 0.26703742334403535\n",
            "The running loss at 435 iteration is: 0.26682459917161094\n",
            "The running loss at 436 iteration is: 0.266457268920659\n",
            "The running loss at 437 iteration is: 0.2662356674545929\n",
            "The running loss at 438 iteration is: 0.2659112489483674\n",
            "The running loss at 439 iteration is: 0.26569889354582416\n",
            "The running loss at 440 iteration is: 0.2653235427707818\n",
            "The running loss at 441 iteration is: 0.2651113799626759\n",
            "The running loss at 442 iteration is: 0.2648996399375789\n",
            "The running loss at 443 iteration is: 0.2645158494214869\n",
            "The running loss at 444 iteration is: 0.2642946893769632\n",
            "The running loss at 445 iteration is: 0.2639904280206027\n",
            "The running loss at 446 iteration is: 0.2637091901804633\n",
            "The running loss at 447 iteration is: 0.26338709449735226\n",
            "The running loss at 448 iteration is: 0.26316657024347107\n",
            "The running loss at 449 iteration is: 0.2629647901257869\n",
            "The running loss at 450 iteration is: 0.2626007232188348\n",
            "The running loss at 451 iteration is: 0.262372123416053\n",
            "The running loss at 452 iteration is: 0.26204163295310173\n",
            "The running loss at 453 iteration is: 0.2617887969099338\n",
            "The running loss at 454 iteration is: 0.2614862163586985\n",
            "The running loss at 455 iteration is: 0.2612484973875667\n",
            "The running loss at 456 iteration is: 0.2609879551403894\n",
            "The running loss at 457 iteration is: 0.2606851841060322\n",
            "The running loss at 458 iteration is: 0.26044792938238953\n",
            "The running loss at 459 iteration is: 0.2600769380259837\n",
            "The running loss at 460 iteration is: 0.25985836328664264\n",
            "The running loss at 461 iteration is: 0.2595479254248363\n",
            "The running loss at 462 iteration is: 0.2592877863662645\n",
            "The running loss at 463 iteration is: 0.25906970131149337\n",
            "The running loss at 464 iteration is: 0.258741338762906\n",
            "The running loss at 465 iteration is: 0.25853265905102235\n",
            "The running loss at 466 iteration is: 0.2581633671262038\n",
            "The running loss at 467 iteration is: 0.2579550185798247\n",
            "The running loss at 468 iteration is: 0.25773755211017874\n",
            "The running loss at 469 iteration is: 0.2573684930112802\n",
            "The running loss at 470 iteration is: 0.25715145693856684\n",
            "The running loss at 471 iteration is: 0.2568246024740995\n",
            "The running loss at 472 iteration is: 0.25657540182931377\n",
            "The running loss at 473 iteration is: 0.25634961582087573\n",
            "The running loss at 474 iteration is: 0.2560504658175624\n",
            "The running loss at 475 iteration is: 0.2557837448379348\n",
            "The running loss at 476 iteration is: 0.25546691494080626\n",
            "The running loss at 477 iteration is: 0.2552509781522299\n",
            "The running loss at 478 iteration is: 0.2548839905767723\n",
            "The running loss at 479 iteration is: 0.25467722300864415\n",
            "The running loss at 480 iteration is: 0.2544523731022086\n",
            "The running loss at 481 iteration is: 0.254086678329061\n",
            "The running loss at 482 iteration is: 0.2538709442838407\n",
            "The running loss at 483 iteration is: 0.2536144303915785\n",
            "The running loss at 484 iteration is: 0.25329916090020227\n",
            "The running loss at 485 iteration is: 0.25309345032522695\n",
            "The running loss at 486 iteration is: 0.25271051418132284\n",
            "The running loss at 487 iteration is: 0.2525044320088563\n",
            "The running loss at 488 iteration is: 0.2523079047374878\n",
            "The running loss at 489 iteration is: 0.2519347524969736\n",
            "The running loss at 490 iteration is: 0.25172068247983914\n",
            "The running loss at 491 iteration is: 0.2514154882012525\n",
            "The running loss at 492 iteration is: 0.25114227957371543\n",
            "The running loss at 493 iteration is: 0.25092839452342813\n",
            "The running loss at 494 iteration is: 0.2506151445908905\n",
            "The running loss at 495 iteration is: 0.25035175484700967\n",
            "The running loss at 496 iteration is: 0.2501470422371775\n",
            "The running loss at 497 iteration is: 0.249775789027413\n",
            "The running loss at 498 iteration is: 0.24958034615964753\n",
            "The running loss at 499 iteration is: 0.2492681296898157\n",
            "The running loss at 500 iteration is: 0.24901461863803878\n",
            "The running loss at 501 iteration is: 0.24878375018306836\n",
            "The running loss at 502 iteration is: 0.24848076366136612\n",
            "The running loss at 503 iteration is: 0.24821857301816955\n",
            "The running loss at 504 iteration is: 0.24801514606565478\n",
            "The running loss at 505 iteration is: 0.24764592762652957\n",
            "The running loss at 506 iteration is: 0.24742454446018772\n",
            "The running loss at 507 iteration is: 0.24723041492082626\n",
            "The running loss at 508 iteration is: 0.24685226965657053\n",
            "The running loss at 509 iteration is: 0.24665852519947554\n",
            "The running loss at 510 iteration is: 0.2463797711245551\n",
            "The running loss at 511 iteration is: 0.24607825071100253\n",
            "The running loss at 512 iteration is: 0.24587599696012463\n",
            "The running loss at 513 iteration is: 0.24551743914940605\n",
            "The running loss at 514 iteration is: 0.24532423204986717\n",
            "The running loss at 515 iteration is: 0.2450461263303986\n",
            "The running loss at 516 iteration is: 0.24473687206526912\n",
            "The running loss at 517 iteration is: 0.24452614076249096\n",
            "The running loss at 518 iteration is: 0.24426662539471605\n",
            "The running loss at 519 iteration is: 0.24395821845669138\n",
            "The running loss at 520 iteration is: 0.24368974620052\n",
            "The running loss at 521 iteration is: 0.2434884788852621\n",
            "The running loss at 522 iteration is: 0.24317165920373418\n",
            "The running loss at 523 iteration is: 0.24291304330085264\n",
            "The running loss at 524 iteration is: 0.2427118842698771\n",
            "The running loss at 525 iteration is: 0.24236469420395124\n",
            "The running loss at 526 iteration is: 0.2421463220643547\n",
            "The running loss at 527 iteration is: 0.2419367562797526\n",
            "The running loss at 528 iteration is: 0.2416787677331748\n",
            "The running loss at 529 iteration is: 0.24137222132266808\n",
            "The running loss at 530 iteration is: 0.2411144895823831\n",
            "The running loss at 531 iteration is: 0.24090561596374763\n",
            "The running loss at 532 iteration is: 0.24055086918653193\n",
            "The running loss at 533 iteration is: 0.2403420860689204\n",
            "The running loss at 534 iteration is: 0.24014276255552922\n",
            "The running loss at 535 iteration is: 0.23978909377214253\n",
            "The running loss at 536 iteration is: 0.23958080334683382\n",
            "The running loss at 537 iteration is: 0.23931490167446012\n",
            "The running loss at 538 iteration is: 0.23901908906033803\n",
            "The running loss at 539 iteration is: 0.23874523541823564\n",
            "The running loss at 540 iteration is: 0.23855522012729613\n",
            "The running loss at 541 iteration is: 0.2382990862637642\n",
            "The running loss at 542 iteration is: 0.2379950406884935\n",
            "The running loss at 543 iteration is: 0.237787912406963\n",
            "The running loss at 544 iteration is: 0.2375324358080438\n",
            "The running loss at 545 iteration is: 0.2372200091728935\n",
            "The running loss at 546 iteration is: 0.23696465300923417\n",
            "The running loss at 547 iteration is: 0.23676684599191358\n",
            "The running loss at 548 iteration is: 0.2364944260133973\n",
            "The running loss at 549 iteration is: 0.2361915269122293\n",
            "The running loss at 550 iteration is: 0.23593681818348583\n",
            "The running loss at 551 iteration is: 0.23573935914972877\n",
            "The running loss at 552 iteration is: 0.23538943788028985\n",
            "The running loss at 553 iteration is: 0.2351924193118803\n",
            "The running loss at 554 iteration is: 0.2349295266310827\n",
            "The running loss at 555 iteration is: 0.2347324157157337\n",
            "The running loss at 556 iteration is: 0.2343742859306566\n",
            "The running loss at 557 iteration is: 0.23416890835299922\n",
            "The running loss at 558 iteration is: 0.2339155158908687\n",
            "The running loss at 559 iteration is: 0.23370140657080568\n",
            "The running loss at 560 iteration is: 0.23340956343205033\n",
            "The running loss at 561 iteration is: 0.23314797924511876\n",
            "The running loss at 562 iteration is: 0.2328954236073857\n",
            "The running loss at 563 iteration is: 0.23269053981454707\n",
            "The running loss at 564 iteration is: 0.23239069623296923\n",
            "The running loss at 565 iteration is: 0.2321384381938339\n",
            "The running loss at 566 iteration is: 0.23188670998727326\n",
            "The running loss at 567 iteration is: 0.23169118502095384\n",
            "The running loss at 568 iteration is: 0.23134443126649837\n",
            "The running loss at 569 iteration is: 0.23114051706329822\n",
            "The running loss at 570 iteration is: 0.23088040783725114\n",
            "The running loss at 571 iteration is: 0.23067666930720596\n",
            "The running loss at 572 iteration is: 0.23033047481322866\n",
            "The running loss at 573 iteration is: 0.23011843320190345\n",
            "The running loss at 574 iteration is: 0.22987661128030254\n",
            "The running loss at 575 iteration is: 0.22967345159825112\n",
            "The running loss at 576 iteration is: 0.22941401108481407\n",
            "The running loss at 577 iteration is: 0.22911646414974493\n",
            "The running loss at 578 iteration is: 0.22887523047524896\n",
            "The running loss at 579 iteration is: 0.2286641392409497\n",
            "The running loss at 580 iteration is: 0.22840537140023062\n",
            "The running loss at 581 iteration is: 0.22811714957303925\n",
            "The running loss at 582 iteration is: 0.22786764729001796\n",
            "The running loss at 583 iteration is: 0.22765732151257873\n",
            "The running loss at 584 iteration is: 0.227407929579786\n",
            "The running loss at 585 iteration is: 0.22720555762222802\n",
            "The running loss at 586 iteration is: 0.22686262797017367\n",
            "The running loss at 587 iteration is: 0.22662297461548012\n",
            "The running loss at 588 iteration is: 0.22642182400049718\n",
            "The running loss at 589 iteration is: 0.22615555945425733\n",
            "The running loss at 590 iteration is: 0.22596278542505294\n",
            "The running loss at 591 iteration is: 0.22562967941192802\n",
            "The running loss at 592 iteration is: 0.22541225089057587\n",
            "The running loss at 593 iteration is: 0.22515482996383437\n",
            "The running loss at 594 iteration is: 0.22496268509830175\n",
            "The running loss at 595 iteration is: 0.2247238045772805\n",
            "The running loss at 596 iteration is: 0.22445203419784274\n",
            "The running loss at 597 iteration is: 0.22418293075031476\n",
            "The running loss at 598 iteration is: 0.2239269783636949\n",
            "The running loss at 599 iteration is: 0.22372685311383578\n",
            "The running loss at 600 iteration is: 0.22348955727282782\n",
            "The running loss at 601 iteration is: 0.22322555578067246\n",
            "The running loss at 602 iteration is: 0.22293191070134405\n",
            "The running loss at 603 iteration is: 0.2226944439081244\n",
            "The running loss at 604 iteration is: 0.22247819728338608\n",
            "The running loss at 605 iteration is: 0.22222354774336903\n",
            "The running loss at 606 iteration is: 0.22204100516165826\n",
            "The running loss at 607 iteration is: 0.22179529375552479\n",
            "The running loss at 608 iteration is: 0.22155082582351474\n",
            "The running loss at 609 iteration is: 0.22124177881197243\n",
            "The running loss at 610 iteration is: 0.22100426068368498\n",
            "The running loss at 611 iteration is: 0.22076812640373672\n",
            "The running loss at 612 iteration is: 0.2205532807237356\n",
            "The running loss at 613 iteration is: 0.2203253857198397\n",
            "The running loss at 614 iteration is: 0.22011772705556143\n",
            "The running loss at 615 iteration is: 0.2198732543287605\n",
            "The running loss at 616 iteration is: 0.2195282506921019\n",
            "The running loss at 617 iteration is: 0.21932208551235485\n",
            "The running loss at 618 iteration is: 0.21907729224629693\n",
            "The running loss at 619 iteration is: 0.2188886376798327\n",
            "The running loss at 620 iteration is: 0.21864532511488172\n",
            "The running loss at 621 iteration is: 0.21839397674702848\n",
            "The running loss at 622 iteration is: 0.21820426109539737\n",
            "The running loss at 623 iteration is: 0.21795210041204077\n",
            "The running loss at 624 iteration is: 0.2176174095057659\n",
            "The running loss at 625 iteration is: 0.21742975307912973\n",
            "The running loss at 626 iteration is: 0.217177728547539\n",
            "The running loss at 627 iteration is: 0.21692624643651973\n",
            "The running loss at 628 iteration is: 0.21673843028633208\n",
            "The running loss at 629 iteration is: 0.21648820396794996\n",
            "The running loss at 630 iteration is: 0.21623707794781227\n",
            "The running loss at 631 iteration is: 0.21605798674984483\n",
            "The running loss at 632 iteration is: 0.21579872819862878\n",
            "The running loss at 633 iteration is: 0.21555756006763435\n",
            "The running loss at 634 iteration is: 0.21526987401434894\n",
            "The running loss at 635 iteration is: 0.21501960950364266\n",
            "The running loss at 636 iteration is: 0.21477848285571746\n",
            "The running loss at 637 iteration is: 0.21459209162289086\n",
            "The running loss at 638 iteration is: 0.2143342160843004\n",
            "The running loss at 639 iteration is: 0.21410116334856436\n",
            "The running loss at 640 iteration is: 0.2138977039299654\n",
            "The running loss at 641 iteration is: 0.21365741957504772\n",
            "The running loss at 642 iteration is: 0.21342583152204428\n",
            "The running loss at 643 iteration is: 0.21323091507937272\n",
            "The running loss at 644 iteration is: 0.21298221273961265\n",
            "The running loss at 645 iteration is: 0.212750938586892\n",
            "The running loss at 646 iteration is: 0.21254812537274667\n",
            "The running loss at 647 iteration is: 0.21230831778644121\n",
            "The running loss at 648 iteration is: 0.21196949958805542\n",
            "The running loss at 649 iteration is: 0.21173869825980246\n",
            "The running loss at 650 iteration is: 0.2115366703016886\n",
            "The running loss at 651 iteration is: 0.21130608532815695\n",
            "The running loss at 652 iteration is: 0.21105840458821623\n",
            "The running loss at 653 iteration is: 0.21085653692679976\n",
            "The running loss at 654 iteration is: 0.21061773774415105\n",
            "The running loss at 655 iteration is: 0.2103879937998643\n",
            "The running loss at 656 iteration is: 0.2101327992880637\n",
            "The running loss at 657 iteration is: 0.20993106806170644\n",
            "The running loss at 658 iteration is: 0.20971010921602135\n",
            "The running loss at 659 iteration is: 0.20947266917806479\n",
            "The running loss at 660 iteration is: 0.20922686115767306\n",
            "The running loss at 661 iteration is: 0.20902498072700398\n",
            "The running loss at 662 iteration is: 0.20880475796736478\n",
            "The running loss at 663 iteration is: 0.20855939538602533\n",
            "The running loss at 664 iteration is: 0.20831425647357715\n",
            "The running loss at 665 iteration is: 0.20812113424174075\n",
            "The running loss at 666 iteration is: 0.20789291980908053\n",
            "The running loss at 667 iteration is: 0.2076400425524581\n",
            "The running loss at 668 iteration is: 0.20740340677870878\n",
            "The running loss at 669 iteration is: 0.2072195022973228\n",
            "The running loss at 670 iteration is: 0.2069662257220655\n",
            "The running loss at 671 iteration is: 0.20673071754030398\n",
            "The running loss at 672 iteration is: 0.20649438692064637\n",
            "The running loss at 673 iteration is: 0.2063115455516272\n",
            "The running loss at 674 iteration is: 0.20607625885093667\n",
            "The running loss at 675 iteration is: 0.2058327043768944\n",
            "The running loss at 676 iteration is: 0.20559648931862076\n",
            "The running loss at 677 iteration is: 0.2054059772720367\n",
            "The running loss at 678 iteration is: 0.20516275992564312\n",
            "The running loss at 679 iteration is: 0.20492788345829\n",
            "The running loss at 680 iteration is: 0.2046927253520058\n",
            "The running loss at 681 iteration is: 0.20444952157335455\n",
            "The running loss at 682 iteration is: 0.2042683416596017\n",
            "The running loss at 683 iteration is: 0.204025739278253\n",
            "The running loss at 684 iteration is: 0.20379959163861933\n",
            "The running loss at 685 iteration is: 0.2035484476224129\n",
            "The running loss at 686 iteration is: 0.203323125386791\n",
            "The running loss at 687 iteration is: 0.20312576214055802\n",
            "The running loss at 688 iteration is: 0.2028917203158809\n",
            "The running loss at 689 iteration is: 0.20265805680746865\n",
            "The running loss at 690 iteration is: 0.20242498028439454\n",
            "The running loss at 691 iteration is: 0.20228052273999317\n",
            "The running loss at 692 iteration is: 0.2020996302861341\n",
            "The running loss at 693 iteration is: 0.20185814833159527\n",
            "The running loss at 694 iteration is: 0.201617121465445\n",
            "The running loss at 695 iteration is: 0.20138444731551214\n",
            "The running loss at 696 iteration is: 0.20116022020212768\n",
            "The running loss at 697 iteration is: 0.2009111896659877\n",
            "The running loss at 698 iteration is: 0.20072311263077222\n",
            "The running loss at 699 iteration is: 0.20049069774056102\n",
            "The running loss at 700 iteration is: 0.2002504298531077\n",
            "The running loss at 701 iteration is: 0.20001889581301324\n",
            "The running loss at 702 iteration is: 0.19979559546575548\n",
            "The running loss at 703 iteration is: 0.19955571156687044\n",
            "The running loss at 704 iteration is: 0.19932431999852954\n",
            "The running loss at 705 iteration is: 0.199216186303865\n",
            "The running loss at 706 iteration is: 0.19898492709633897\n",
            "The running loss at 707 iteration is: 0.19875408122761704\n",
            "The running loss at 708 iteration is: 0.19851511624816479\n",
            "The running loss at 709 iteration is: 0.1982843017156505\n",
            "The running loss at 710 iteration is: 0.19806219910539383\n",
            "The running loss at 711 iteration is: 0.19782376369670474\n",
            "The running loss at 712 iteration is: 0.19759335830736396\n",
            "The running loss at 713 iteration is: 0.1974067874768928\n",
            "The running loss at 714 iteration is: 0.1972558762489015\n",
            "The running loss at 715 iteration is: 0.1970259353896185\n",
            "The running loss at 716 iteration is: 0.1968130563638493\n",
            "The running loss at 717 iteration is: 0.19657562041881815\n",
            "The running loss at 718 iteration is: 0.19634608142600712\n",
            "The running loss at 719 iteration is: 0.1961080753145944\n",
            "The running loss at 720 iteration is: 0.19587063042679584\n",
            "The running loss at 721 iteration is: 0.1956334404176694\n",
            "The running loss at 722 iteration is: 0.19554299621103108\n",
            "The running loss at 723 iteration is: 0.19529820469022513\n",
            "The running loss at 724 iteration is: 0.19508578628098408\n",
            "The running loss at 725 iteration is: 0.1948574913292367\n",
            "The running loss at 726 iteration is: 0.1946295824074196\n",
            "The running loss at 727 iteration is: 0.19438469213587095\n",
            "The running loss at 728 iteration is: 0.1941647320537856\n",
            "The running loss at 729 iteration is: 0.19401517890032607\n",
            "The running loss at 730 iteration is: 0.19379608827041717\n",
            "The running loss at 731 iteration is: 0.19355147382743093\n",
            "The running loss at 732 iteration is: 0.19334064726993036\n",
            "The running loss at 733 iteration is: 0.19309739423219316\n",
            "The running loss at 734 iteration is: 0.1928782669770342\n",
            "The running loss at 735 iteration is: 0.1927800480792109\n",
            "The running loss at 736 iteration is: 0.19252860038853528\n",
            "The running loss at 737 iteration is: 0.1923098352838669\n",
            "The running loss at 738 iteration is: 0.1920832476892096\n",
            "The running loss at 739 iteration is: 0.19185715667103936\n",
            "The running loss at 740 iteration is: 0.1916307964799476\n",
            "The running loss at 741 iteration is: 0.19149857206404106\n",
            "The running loss at 742 iteration is: 0.19126430616042148\n",
            "The running loss at 743 iteration is: 0.1910386181751495\n",
            "The running loss at 744 iteration is: 0.19080420783013302\n",
            "The running loss at 745 iteration is: 0.19057843299064595\n",
            "The running loss at 746 iteration is: 0.19043864821581002\n",
            "The running loss at 747 iteration is: 0.19020550667125702\n",
            "The running loss at 748 iteration is: 0.18998806962229395\n",
            "The running loss at 749 iteration is: 0.18975460049059967\n",
            "The running loss at 750 iteration is: 0.1895299473383066\n",
            "The running loss at 751 iteration is: 0.18939056767755\n",
            "The running loss at 752 iteration is: 0.18916581922830183\n",
            "The running loss at 753 iteration is: 0.18893286383960334\n",
            "The running loss at 754 iteration is: 0.18870848184518718\n",
            "The running loss at 755 iteration is: 0.18857769315654138\n",
            "The running loss at 756 iteration is: 0.18835389408534622\n",
            "The running loss at 757 iteration is: 0.18811310121351585\n",
            "The running loss at 758 iteration is: 0.18790549085282834\n",
            "The running loss at 759 iteration is: 0.1876743317369232\n",
            "The running loss at 760 iteration is: 0.18752749444564448\n",
            "The running loss at 761 iteration is: 0.18728643684693247\n",
            "The running loss at 762 iteration is: 0.18707978756897306\n",
            "The running loss at 763 iteration is: 0.18685810676938924\n",
            "The running loss at 764 iteration is: 0.1867113446254155\n",
            "The running loss at 765 iteration is: 0.1864955640897983\n",
            "The running loss at 766 iteration is: 0.1862727753633385\n",
            "The running loss at 767 iteration is: 0.18604360827853186\n",
            "The running loss at 768 iteration is: 0.18589566851482392\n",
            "The running loss at 769 iteration is: 0.18568131483993963\n",
            "The running loss at 770 iteration is: 0.18545161757811307\n",
            "The running loss at 771 iteration is: 0.1853225238783597\n",
            "The running loss at 772 iteration is: 0.18509106700882966\n",
            "The running loss at 773 iteration is: 0.18486112902106028\n",
            "The running loss at 774 iteration is: 0.1846398398230154\n",
            "The running loss at 775 iteration is: 0.1845024967890321\n",
            "The running loss at 776 iteration is: 0.18423846289333376\n",
            "The running loss at 777 iteration is: 0.1840089974913494\n",
            "The running loss at 778 iteration is: 0.18379629014928053\n",
            "The running loss at 779 iteration is: 0.1836594301409805\n",
            "The running loss at 780 iteration is: 0.18343010275281946\n",
            "The running loss at 781 iteration is: 0.18320096237441727\n",
            "The running loss at 782 iteration is: 0.18307244845602932\n",
            "The running loss at 783 iteration is: 0.1828605370554372\n",
            "The running loss at 784 iteration is: 0.18263222089547448\n",
            "The running loss at 785 iteration is: 0.18247839408407018\n",
            "The running loss at 786 iteration is: 0.18225828487151144\n",
            "The running loss at 787 iteration is: 0.18204695183265426\n",
            "The running loss at 788 iteration is: 0.18182784452510986\n",
            "The running loss at 789 iteration is: 0.1816488618952663\n",
            "The running loss at 790 iteration is: 0.18142933964791552\n",
            "The running loss at 791 iteration is: 0.18120247200262407\n",
            "The running loss at 792 iteration is: 0.18106610727655856\n",
            "The running loss at 793 iteration is: 0.1808383312915533\n",
            "The running loss at 794 iteration is: 0.18062771646185172\n",
            "The running loss at 795 iteration is: 0.1804924796082568\n",
            "The running loss at 796 iteration is: 0.1802738631000818\n",
            "The running loss at 797 iteration is: 0.1800544027631157\n",
            "The running loss at 798 iteration is: 0.1798695030444952\n",
            "The running loss at 799 iteration is: 0.1796436904339443\n",
            "The running loss at 800 iteration is: 0.179432866733503\n",
            "The running loss at 801 iteration is: 0.1792975286351617\n",
            "The running loss at 802 iteration is: 0.17907230744023736\n",
            "The running loss at 803 iteration is: 0.1788376775597949\n",
            "The running loss at 804 iteration is: 0.17871849602087606\n",
            "The running loss at 805 iteration is: 0.17850160230436338\n",
            "The running loss at 806 iteration is: 0.1783179370267287\n",
            "The running loss at 807 iteration is: 0.17811567454442323\n",
            "The running loss at 808 iteration is: 0.1778824701620673\n",
            "The running loss at 809 iteration is: 0.17774042871210036\n",
            "The running loss at 810 iteration is: 0.17753076652188132\n",
            "The running loss at 811 iteration is: 0.17731400158531724\n",
            "The running loss at 812 iteration is: 0.17713132458048508\n",
            "The running loss at 813 iteration is: 0.17690622177973117\n",
            "The running loss at 814 iteration is: 0.1767876290470927\n",
            "The running loss at 815 iteration is: 0.17656412666272764\n",
            "The running loss at 816 iteration is: 0.17634821472513723\n",
            "The running loss at 817 iteration is: 0.1762132925530817\n",
            "The running loss at 818 iteration is: 0.1759646075105188\n",
            "The running loss at 819 iteration is: 0.1758236332131088\n",
            "The running loss at 820 iteration is: 0.17559939648825007\n",
            "The running loss at 821 iteration is: 0.17539142301095678\n",
            "The running loss at 822 iteration is: 0.1752502901220849\n",
            "The running loss at 823 iteration is: 0.17499502786135657\n",
            "The running loss at 824 iteration is: 0.17486828351130496\n",
            "The running loss at 825 iteration is: 0.17464541878747233\n",
            "The running loss at 826 iteration is: 0.17443168543699347\n",
            "The running loss at 827 iteration is: 0.17429758401900128\n",
            "The running loss at 828 iteration is: 0.1740828950821149\n",
            "The running loss at 829 iteration is: 0.17390958014026056\n",
            "The running loss at 830 iteration is: 0.17368758117223043\n",
            "The running loss at 831 iteration is: 0.17348062197074038\n",
            "The running loss at 832 iteration is: 0.17333997779878957\n",
            "The running loss at 833 iteration is: 0.17307815575388769\n",
            "The running loss at 834 iteration is: 0.1729526523952734\n",
            "The running loss at 835 iteration is: 0.1727229978252863\n",
            "The running loss at 836 iteration is: 0.1725987357161433\n",
            "The running loss at 837 iteration is: 0.1723450891888861\n",
            "The running loss at 838 iteration is: 0.1722123552521457\n",
            "The running loss at 839 iteration is: 0.1720074783067481\n",
            "The running loss at 840 iteration is: 0.17177118106213177\n",
            "The running loss at 841 iteration is: 0.17165414941865964\n",
            "The running loss at 842 iteration is: 0.17139309446439435\n",
            "The running loss at 843 iteration is: 0.17126180462108614\n",
            "The running loss at 844 iteration is: 0.17104891243214643\n",
            "The running loss at 845 iteration is: 0.17092487205924456\n",
            "The running loss at 846 iteration is: 0.17065709655162947\n",
            "The running loss at 847 iteration is: 0.1705329863978491\n",
            "The running loss at 848 iteration is: 0.17031271642152493\n",
            "The running loss at 849 iteration is: 0.17013342782872992\n",
            "The running loss at 850 iteration is: 0.1699374143821515\n",
            "The running loss at 851 iteration is: 0.169798417546744\n",
            "The running loss at 852 iteration is: 0.1695869244447067\n",
            "The running loss at 853 iteration is: 0.16942333791829234\n",
            "The running loss at 854 iteration is: 0.16920438450972478\n",
            "The running loss at 855 iteration is: 0.16907347068278786\n",
            "The running loss at 856 iteration is: 0.16885433791872168\n",
            "The running loss at 857 iteration is: 0.16869106483280552\n",
            "The running loss at 858 iteration is: 0.1684810704611207\n",
            "The running loss at 859 iteration is: 0.1683506481830854\n",
            "The running loss at 860 iteration is: 0.16809952688106392\n",
            "The running loss at 861 iteration is: 0.16796910265681664\n",
            "The running loss at 862 iteration is: 0.16775153786192007\n",
            "The running loss at 863 iteration is: 0.16758853481247257\n",
            "The running loss at 864 iteration is: 0.1673706906186939\n",
            "The running loss at 865 iteration is: 0.16723290687828313\n",
            "The running loss at 866 iteration is: 0.16698288419559723\n",
            "The running loss at 867 iteration is: 0.16685285622732018\n",
            "The running loss at 868 iteration is: 0.1666598143536041\n",
            "The running loss at 869 iteration is: 0.16648272497249517\n",
            "The running loss at 870 iteration is: 0.16625681755551763\n",
            "The running loss at 871 iteration is: 0.1661346752241842\n",
            "The running loss at 872 iteration is: 0.16589511107013327\n",
            "The running loss at 873 iteration is: 0.1657580235116267\n",
            "The running loss at 874 iteration is: 0.16564316254361997\n",
            "The running loss at 875 iteration is: 0.165379685926773\n",
            "The running loss at 876 iteration is: 0.1652506611987136\n",
            "The running loss at 877 iteration is: 0.16503330872117464\n",
            "The running loss at 878 iteration is: 0.16487249578484406\n",
            "The running loss at 879 iteration is: 0.16466595240343782\n",
            "The running loss at 880 iteration is: 0.16452820524143544\n",
            "The running loss at 881 iteration is: 0.1642963110295287\n",
            "The running loss at 882 iteration is: 0.16416054158946838\n",
            "The running loss at 883 iteration is: 0.16403194099804227\n",
            "The running loss at 884 iteration is: 0.16377562083767802\n",
            "The running loss at 885 iteration is: 0.16365509544429105\n",
            "The running loss at 886 iteration is: 0.16339544861905622\n",
            "The running loss at 887 iteration is: 0.1632800685380342\n",
            "The running loss at 888 iteration is: 0.16307311283954032\n",
            "The running loss at 889 iteration is: 0.16289994052066706\n",
            "The running loss at 890 iteration is: 0.16276956642904294\n",
            "The running loss at 891 iteration is: 0.16257072010293883\n",
            "The running loss at 892 iteration is: 0.16239703291100266\n",
            "The running loss at 893 iteration is: 0.16219006506823677\n",
            "The running loss at 894 iteration is: 0.16202997127065155\n",
            "The running loss at 895 iteration is: 0.16189532226143033\n",
            "The running loss at 896 iteration is: 0.1616891163074102\n",
            "The running loss at 897 iteration is: 0.16152141454244912\n",
            "The running loss at 898 iteration is: 0.16131626923647804\n",
            "The running loss at 899 iteration is: 0.16114997456778984\n",
            "The running loss at 900 iteration is: 0.1610217459751135\n",
            "The running loss at 901 iteration is: 0.1608166084146072\n",
            "The running loss at 902 iteration is: 0.16065117601670656\n",
            "The running loss at 903 iteration is: 0.1605229095505189\n",
            "The running loss at 904 iteration is: 0.16027906693446597\n",
            "The running loss at 905 iteration is: 0.16015274276631908\n",
            "The running loss at 906 iteration is: 0.1599085798628365\n",
            "The running loss at 907 iteration is: 0.15978131135463094\n",
            "The running loss at 908 iteration is: 0.159654854577368\n",
            "The running loss at 909 iteration is: 0.15941156556586444\n",
            "The running loss at 910 iteration is: 0.15928460156052196\n",
            "The running loss at 911 iteration is: 0.15911926058880332\n",
            "The running loss at 912 iteration is: 0.15891563610493178\n",
            "The running loss at 913 iteration is: 0.15875029620093672\n",
            "The running loss at 914 iteration is: 0.15861556557987824\n",
            "The running loss at 915 iteration is: 0.15838134218762057\n",
            "The running loss at 916 iteration is: 0.15825553060640826\n",
            "The running loss at 917 iteration is: 0.15813650605049429\n",
            "The running loss at 918 iteration is: 0.15788701427102425\n",
            "The running loss at 919 iteration is: 0.157753511274776\n",
            "The running loss at 920 iteration is: 0.15759608728410868\n",
            "The running loss at 921 iteration is: 0.1573856709373533\n",
            "The running loss at 922 iteration is: 0.1572138884498021\n",
            "The running loss at 923 iteration is: 0.15709497789493115\n",
            "The running loss at 924 iteration is: 0.15686218707850624\n",
            "The running loss at 925 iteration is: 0.1567453061722287\n",
            "The running loss at 926 iteration is: 0.15657288400937724\n",
            "The running loss at 927 iteration is: 0.1563701011780518\n",
            "The running loss at 928 iteration is: 0.15619952869781084\n",
            "The running loss at 929 iteration is: 0.15608164743258782\n",
            "The running loss at 930 iteration is: 0.1558410869276918\n",
            "The running loss at 931 iteration is: 0.1557239029452845\n",
            "The running loss at 932 iteration is: 0.1555610732231566\n",
            "The running loss at 933 iteration is: 0.1554278769935711\n",
            "The running loss at 934 iteration is: 0.1551877251984112\n",
            "The running loss at 935 iteration is: 0.155063597687873\n",
            "The running loss at 936 iteration is: 0.15490070572191625\n",
            "The running loss at 937 iteration is: 0.15470662433734197\n",
            "The running loss at 938 iteration is: 0.15453702599416233\n",
            "The running loss at 939 iteration is: 0.15441992831666373\n",
            "The running loss at 940 iteration is: 0.15424895955258974\n",
            "The running loss at 941 iteration is: 0.1540490898990032\n",
            "The running loss at 942 iteration is: 0.15388643817110578\n",
            "The running loss at 943 iteration is: 0.15376975612712368\n",
            "The running loss at 944 iteration is: 0.15353989895955647\n",
            "The running loss at 945 iteration is: 0.15337019892529027\n",
            "The running loss at 946 iteration is: 0.15324524285411933\n",
            "The running loss at 947 iteration is: 0.15306848469586287\n",
            "The running loss at 948 iteration is: 0.15288382910210335\n",
            "The running loss at 949 iteration is: 0.1527145999292\n",
            "The running loss at 950 iteration is: 0.15259832413511537\n",
            "The running loss at 951 iteration is: 0.15242932145472163\n",
            "The running loss at 952 iteration is: 0.15222995134568773\n",
            "The running loss at 953 iteration is: 0.15207616927639517\n",
            "The running loss at 954 iteration is: 0.15190784026181817\n",
            "The running loss at 955 iteration is: 0.1517835143290421\n",
            "The running loss at 956 iteration is: 0.15154743299876453\n",
            "The running loss at 957 iteration is: 0.1514397564117242\n",
            "The running loss at 958 iteration is: 0.1512714532120931\n",
            "The running loss at 959 iteration is: 0.15114760819428938\n",
            "The running loss at 960 iteration is: 0.15100274817150078\n",
            "The running loss at 961 iteration is: 0.15076045518177328\n",
            "The running loss at 962 iteration is: 0.15063599253697726\n",
            "The running loss at 963 iteration is: 0.15047563111240372\n",
            "The running loss at 964 iteration is: 0.15034670646421303\n",
            "The running loss at 965 iteration is: 0.15012517099054892\n",
            "The running loss at 966 iteration is: 0.1499503757859819\n",
            "The running loss at 967 iteration is: 0.14983545701326206\n",
            "The running loss at 968 iteration is: 0.14967524205331703\n",
            "The running loss at 969 iteration is: 0.14955289175586153\n",
            "The running loss at 970 iteration is: 0.1493264140993994\n",
            "The running loss at 971 iteration is: 0.14916658634097604\n",
            "The running loss at 972 iteration is: 0.1490366319903413\n",
            "The running loss at 973 iteration is: 0.14888433262255849\n",
            "The running loss at 974 iteration is: 0.14876406053504365\n",
            "The running loss at 975 iteration is: 0.14860340437441755\n",
            "The running loss at 976 iteration is: 0.14836179534706967\n",
            "The running loss at 977 iteration is: 0.14824809868511882\n",
            "The running loss at 978 iteration is: 0.14808187472431272\n",
            "The running loss at 979 iteration is: 0.14792971468239546\n",
            "The running loss at 980 iteration is: 0.14782403655031318\n",
            "The running loss at 981 iteration is: 0.14758469984803316\n",
            "The running loss at 982 iteration is: 0.1474614039397717\n",
            "The running loss at 983 iteration is: 0.14729522480177573\n",
            "The running loss at 984 iteration is: 0.1471383638666998\n",
            "The running loss at 985 iteration is: 0.14701546976823981\n",
            "The running loss at 986 iteration is: 0.14687265748294795\n",
            "The running loss at 987 iteration is: 0.14670845239961008\n",
            "The running loss at 988 iteration is: 0.14651183369327936\n",
            "The running loss at 989 iteration is: 0.1463613208964235\n",
            "The running loss at 990 iteration is: 0.14620507159015825\n",
            "The running loss at 991 iteration is: 0.14609069229694052\n",
            "The running loss at 992 iteration is: 0.14592491725978812\n",
            "The running loss at 993 iteration is: 0.14576842121327407\n",
            "The running loss at 994 iteration is: 0.1455811439343199\n",
            "The running loss at 995 iteration is: 0.14541589952091827\n",
            "The running loss at 996 iteration is: 0.14526611675859105\n",
            "The running loss at 997 iteration is: 0.14514652492846442\n",
            "The running loss at 998 iteration is: 0.14498120149833457\n",
            "The running loss at 999 iteration is: 0.14483894861071966\n",
            "The running loss at 1000 iteration is: 0.14466885690933845\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.312356855829214e-06\n"
          ]
        }
      ],
      "source": [
        "run_train(lr=0.03, num_e= 1000, isOn=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S9qFM6elERbf",
        "outputId": "91506305-fd5a-46fd-82a8-6484c810a2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "The running loss at 5001 iteration is: 0.14405010716623778\n",
            "The running loss at 5002 iteration is: 0.14350172735056105\n",
            "The running loss at 5003 iteration is: 0.14295176024687867\n",
            "The running loss at 5004 iteration is: 0.14242444142731497\n",
            "The running loss at 5005 iteration is: 0.1419756179543985\n",
            "The running loss at 5006 iteration is: 0.14156057017644533\n",
            "The running loss at 5007 iteration is: 0.14099168593547037\n",
            "The running loss at 5008 iteration is: 0.14052688790028842\n",
            "The running loss at 5009 iteration is: 0.14001026385542595\n",
            "The running loss at 5010 iteration is: 0.1395489851413156\n",
            "The running loss at 5011 iteration is: 0.13904773572598653\n",
            "The running loss at 5012 iteration is: 0.13860258920530613\n",
            "The running loss at 5013 iteration is: 0.13814504205602995\n",
            "The running loss at 5014 iteration is: 0.13763353680399565\n",
            "The running loss at 5015 iteration is: 0.13718974243126178\n",
            "The running loss at 5016 iteration is: 0.1367013302282759\n",
            "The running loss at 5017 iteration is: 0.13621847479771626\n",
            "The running loss at 5018 iteration is: 0.13579333106010155\n",
            "The running loss at 5019 iteration is: 0.13534674608635885\n",
            "The running loss at 5020 iteration is: 0.13487017648433686\n",
            "The running loss at 5021 iteration is: 0.1344258612958753\n",
            "The running loss at 5022 iteration is: 0.13394900437936003\n",
            "The running loss at 5023 iteration is: 0.13352073185604532\n",
            "The running loss at 5024 iteration is: 0.13309613785504168\n",
            "The running loss at 5025 iteration is: 0.13260465143604083\n",
            "The running loss at 5026 iteration is: 0.1321790496279141\n",
            "The running loss at 5027 iteration is: 0.13173302516248434\n",
            "The running loss at 5028 iteration is: 0.13130537211872495\n",
            "The running loss at 5029 iteration is: 0.13089908823673838\n",
            "The running loss at 5030 iteration is: 0.1304593363987082\n",
            "The running loss at 5031 iteration is: 0.13000616124728662\n",
            "The running loss at 5032 iteration is: 0.12955904594954018\n",
            "The running loss at 5033 iteration is: 0.12914556659478027\n",
            "The running loss at 5034 iteration is: 0.1287000584419248\n",
            "The running loss at 5035 iteration is: 0.1282691572003426\n",
            "The running loss at 5036 iteration is: 0.1278707721946376\n",
            "The running loss at 5037 iteration is: 0.12742725207763536\n",
            "The running loss at 5038 iteration is: 0.12703429722405316\n",
            "The running loss at 5039 iteration is: 0.1266502583593598\n",
            "The running loss at 5040 iteration is: 0.1262360528570784\n",
            "The running loss at 5041 iteration is: 0.1258210103227263\n",
            "The running loss at 5042 iteration is: 0.1254177509620706\n",
            "The running loss at 5043 iteration is: 0.1250330775337375\n",
            "The running loss at 5044 iteration is: 0.12460697717567144\n",
            "The running loss at 5045 iteration is: 0.12418003846228587\n",
            "The running loss at 5046 iteration is: 0.12377028830206784\n",
            "The running loss at 5047 iteration is: 0.12338773296005566\n",
            "The running loss at 5048 iteration is: 0.12297656705994105\n",
            "The running loss at 5049 iteration is: 0.1225928540941423\n",
            "The running loss at 5050 iteration is: 0.12218725959596559\n",
            "The running loss at 5051 iteration is: 0.12177711110410772\n",
            "The running loss at 5052 iteration is: 0.12140879446508279\n",
            "The running loss at 5053 iteration is: 0.12101687484023438\n",
            "The running loss at 5054 iteration is: 0.1206698326760894\n",
            "The running loss at 5055 iteration is: 0.12028170292989832\n",
            "The running loss at 5056 iteration is: 0.119872091355415\n",
            "The running loss at 5057 iteration is: 0.11950764007888931\n",
            "The running loss at 5058 iteration is: 0.11910892643648162\n",
            "The running loss at 5059 iteration is: 0.11872325884983269\n",
            "The running loss at 5060 iteration is: 0.1183793939429581\n",
            "The running loss at 5061 iteration is: 0.11802044739431218\n",
            "The running loss at 5062 iteration is: 0.11760708273022544\n",
            "The running loss at 5063 iteration is: 0.1172566545464677\n",
            "The running loss at 5064 iteration is: 0.11691887830617106\n",
            "The running loss at 5065 iteration is: 0.1165611043319501\n",
            "The running loss at 5066 iteration is: 0.11618534552741071\n",
            "The running loss at 5067 iteration is: 0.11587271203605358\n",
            "The running loss at 5068 iteration is: 0.11553779851462119\n",
            "The running loss at 5069 iteration is: 0.11528310560342796\n",
            "The running loss at 5070 iteration is: 0.11509880987634537\n",
            "The running loss at 5071 iteration is: 0.1150722023980283\n",
            "The running loss at 5072 iteration is: 0.11540067692740609\n",
            "The running loss at 5073 iteration is: 0.11634530701262867\n",
            "The running loss at 5074 iteration is: 0.11863351968993033\n",
            "The running loss at 5075 iteration is: 0.12372727150649593\n",
            "The running loss at 5076 iteration is: 0.13507707975889283\n",
            "The running loss at 5077 iteration is: 0.15949699537944126\n",
            "The running loss at 5078 iteration is: 0.2139570100931017\n",
            "The running loss at 5079 iteration is: 0.3321091890602636\n",
            "The running loss at 5080 iteration is: 0.6067554306618659\n",
            "The running loss at 5081 iteration is: 1.2029985136406718\n",
            "The running loss at 5082 iteration is: 2.6662214568710954\n",
            "The running loss at 5083 iteration is: 5.662568395897865\n",
            "The running loss at 5084 iteration is: 13.37389478430979\n",
            "The running loss at 5085 iteration is: 25.0871325691906\n",
            "The running loss at 5086 iteration is: 52.745145726009724\n",
            "The running loss at 5087 iteration is: 53.19029023734145\n",
            "The running loss at 5088 iteration is: 47.96030267112517\n",
            "The running loss at 5089 iteration is: 5.82762282025589\n",
            "The running loss at 5090 iteration is: 8.780266668814873\n",
            "The running loss at 5091 iteration is: 36.451281865383066\n",
            "The running loss at 5092 iteration is: 16.688492319648827\n",
            "The running loss at 5093 iteration is: 0.2829642402479533\n",
            "The running loss at 5094 iteration is: 11.903668386488105\n",
            "The running loss at 5095 iteration is: 21.458030880967275\n",
            "The running loss at 5096 iteration is: 16.634802467495387\n",
            "The running loss at 5097 iteration is: 1.8696467284803668\n",
            "The running loss at 5098 iteration is: 3.7116791515466936\n",
            "The running loss at 5099 iteration is: 15.597925281761034\n",
            "The running loss at 5100 iteration is: 14.073053262678858\n",
            "The running loss at 5101 iteration is: 5.5393273432807355\n",
            "The running loss at 5102 iteration is: 0.3326424646778111\n",
            "The running loss at 5103 iteration is: 5.608954485815047\n",
            "The running loss at 5104 iteration is: 10.380893123628862\n",
            "The running loss at 5105 iteration is: 4.15814997383185\n",
            "The running loss at 5106 iteration is: 0.30659842310026925\n",
            "The running loss at 5107 iteration is: 3.968731012244029\n",
            "The running loss at 5108 iteration is: 5.21699243156984\n",
            "The running loss at 5109 iteration is: 1.700004551952928\n",
            "The running loss at 5110 iteration is: 0.5558669491930179\n",
            "The running loss at 5111 iteration is: 3.277172763740892\n",
            "The running loss at 5112 iteration is: 3.0745046567210372\n",
            "The running loss at 5113 iteration is: 0.380244762924122\n",
            "The running loss at 5114 iteration is: 1.4283866050752592\n",
            "The running loss at 5115 iteration is: 2.880213890520555\n",
            "The running loss at 5116 iteration is: 0.9412418910711497\n",
            "The running loss at 5117 iteration is: 0.46029119397476087\n",
            "The running loss at 5118 iteration is: 1.9295718192366884\n",
            "The running loss at 5119 iteration is: 1.2768545645525842\n",
            "The running loss at 5120 iteration is: 0.2241336400406332\n",
            "The running loss at 5121 iteration is: 1.001038783227004\n",
            "The running loss at 5122 iteration is: 1.2565817893698903\n",
            "The running loss at 5123 iteration is: 0.37580540723480593\n",
            "The running loss at 5124 iteration is: 0.4255104293000831\n",
            "The running loss at 5125 iteration is: 0.9942670047487991\n",
            "The running loss at 5126 iteration is: 0.5972531138732997\n",
            "The running loss at 5127 iteration is: 0.21660907373473812\n",
            "The running loss at 5128 iteration is: 0.6510153160825229\n",
            "The running loss at 5129 iteration is: 0.694586178770001\n",
            "The running loss at 5130 iteration is: 0.24232473365369098\n",
            "The running loss at 5131 iteration is: 0.3661953896868989\n",
            "The running loss at 5132 iteration is: 0.6230017304937313\n",
            "The running loss at 5133 iteration is: 0.3444917499958014\n",
            "The running loss at 5134 iteration is: 0.21814982015405696\n",
            "The running loss at 5135 iteration is: 0.45145805686464735\n",
            "The running loss at 5136 iteration is: 0.4066544060488656\n",
            "The running loss at 5137 iteration is: 0.20368304245989025\n",
            "The running loss at 5138 iteration is: 0.2883907373244849\n",
            "The running loss at 5139 iteration is: 0.3879047513358031\n",
            "The running loss at 5140 iteration is: 0.2558533188081031\n",
            "The running loss at 5141 iteration is: 0.20158948032512533\n",
            "The running loss at 5142 iteration is: 0.30993248659557565\n",
            "The running loss at 5143 iteration is: 0.2960210354868667\n",
            "The running loss at 5144 iteration is: 0.1949144364490162\n",
            "The running loss at 5145 iteration is: 0.2290610074428868\n",
            "The running loss at 5146 iteration is: 0.2854072578484666\n",
            "The running loss at 5147 iteration is: 0.22392221991945088\n",
            "The running loss at 5148 iteration is: 0.18798538928114394\n",
            "The running loss at 5149 iteration is: 0.23827815431229923\n",
            "The running loss at 5150 iteration is: 0.24043859860350458\n",
            "The running loss at 5151 iteration is: 0.1903685815849611\n",
            "The running loss at 5152 iteration is: 0.19510471542968763\n",
            "The running loss at 5153 iteration is: 0.22689290106817867\n",
            "The running loss at 5154 iteration is: 0.20752324288236412\n",
            "The running loss at 5155 iteration is: 0.18059194108801715\n",
            "The running loss at 5156 iteration is: 0.1981945405424017\n",
            "The running loss at 5157 iteration is: 0.21031766395227078\n",
            "The running loss at 5158 iteration is: 0.1877860079844911\n",
            "The running loss at 5159 iteration is: 0.17905683834914385\n",
            "The running loss at 5160 iteration is: 0.19501009676210607\n",
            "The running loss at 5161 iteration is: 0.1944260656992442\n",
            "The running loss at 5162 iteration is: 0.1780077973066326\n",
            "The running loss at 5163 iteration is: 0.17843559898272465\n",
            "The running loss at 5164 iteration is: 0.1886303955156721\n",
            "The running loss at 5165 iteration is: 0.18357577737969924\n",
            "The running loss at 5166 iteration is: 0.17329917030839867\n",
            "The running loss at 5167 iteration is: 0.17652146256341733\n",
            "The running loss at 5168 iteration is: 0.18188641154454585\n",
            "The running loss at 5169 iteration is: 0.1761295030412744\n",
            "The running loss at 5170 iteration is: 0.17029619932956674\n",
            "The running loss at 5171 iteration is: 0.1734965420953219\n",
            "The running loss at 5172 iteration is: 0.17586123043014124\n",
            "The running loss at 5173 iteration is: 0.17109998849287786\n",
            "The running loss at 5174 iteration is: 0.16773520021065466\n",
            "The running loss at 5175 iteration is: 0.17009789985300972\n",
            "The running loss at 5176 iteration is: 0.17094302974841863\n",
            "The running loss at 5177 iteration is: 0.16734303774541362\n",
            "The running loss at 5178 iteration is: 0.16530296553588655\n",
            "The running loss at 5179 iteration is: 0.16668243310386485\n",
            "The running loss at 5180 iteration is: 0.1669046564577659\n",
            "The running loss at 5181 iteration is: 0.16439618007789028\n",
            "The running loss at 5182 iteration is: 0.16284624847168455\n",
            "The running loss at 5183 iteration is: 0.16360111892694373\n",
            "The running loss at 5184 iteration is: 0.1636026681837021\n",
            "The running loss at 5185 iteration is: 0.16178742297161156\n",
            "The running loss at 5186 iteration is: 0.16041942327651787\n",
            "The running loss at 5187 iteration is: 0.16062478490507132\n",
            "The running loss at 5188 iteration is: 0.16063840219779646\n",
            "The running loss at 5189 iteration is: 0.1593132280457528\n",
            "The running loss at 5190 iteration is: 0.15811127572257785\n",
            "The running loss at 5191 iteration is: 0.1579009990596254\n",
            "The running loss at 5192 iteration is: 0.15778618721615734\n",
            "The running loss at 5193 iteration is: 0.15688361446737695\n",
            "The running loss at 5194 iteration is: 0.1558760445068024\n",
            "The running loss at 5195 iteration is: 0.15542163893767483\n",
            "The running loss at 5196 iteration is: 0.15521942557786012\n",
            "The running loss at 5197 iteration is: 0.15459929197874098\n",
            "The running loss at 5198 iteration is: 0.1536826383149597\n",
            "The running loss at 5199 iteration is: 0.15304268766388526\n",
            "The running loss at 5200 iteration is: 0.15272841685628072\n",
            "The running loss at 5201 iteration is: 0.15225776993101628\n",
            "The running loss at 5202 iteration is: 0.15154031861999365\n",
            "The running loss at 5203 iteration is: 0.15087948256453368\n",
            "The running loss at 5204 iteration is: 0.15044240130421996\n",
            "The running loss at 5205 iteration is: 0.1500651195274312\n",
            "The running loss at 5206 iteration is: 0.14947298664868516\n",
            "The running loss at 5207 iteration is: 0.1488056247072682\n",
            "The running loss at 5208 iteration is: 0.1482318954723477\n",
            "The running loss at 5209 iteration is: 0.1477915764990166\n",
            "The running loss at 5210 iteration is: 0.14733171171381487\n",
            "The running loss at 5211 iteration is: 0.1467872446858617\n",
            "The running loss at 5212 iteration is: 0.14616639579514143\n",
            "The running loss at 5213 iteration is: 0.14563801257733097\n",
            "The running loss at 5214 iteration is: 0.1451827769511034\n",
            "The running loss at 5215 iteration is: 0.14472455661044814\n",
            "The running loss at 5216 iteration is: 0.14415738770694694\n",
            "The running loss at 5217 iteration is: 0.14364182075138757\n",
            "The running loss at 5218 iteration is: 0.14315447179271634\n",
            "The running loss at 5219 iteration is: 0.14265742098726228\n",
            "The running loss at 5220 iteration is: 0.1421585972678333\n",
            "The running loss at 5221 iteration is: 0.14170849453595113\n",
            "The running loss at 5222 iteration is: 0.14118540620787887\n",
            "The running loss at 5223 iteration is: 0.14068916510711746\n",
            "The running loss at 5224 iteration is: 0.14020976829855705\n",
            "The running loss at 5225 iteration is: 0.13975537302988314\n",
            "The running loss at 5226 iteration is: 0.1392925268617285\n",
            "The running loss at 5227 iteration is: 0.13878271094697406\n",
            "The running loss at 5228 iteration is: 0.13831755101150603\n",
            "The running loss at 5229 iteration is: 0.1378300803503829\n",
            "The running loss at 5230 iteration is: 0.13735131744330475\n",
            "The running loss at 5231 iteration is: 0.1369251007414002\n",
            "The running loss at 5232 iteration is: 0.13642959133284413\n",
            "The running loss at 5233 iteration is: 0.13596960681217488\n",
            "The running loss at 5234 iteration is: 0.1355157938475001\n",
            "The running loss at 5235 iteration is: 0.1350528236778018\n",
            "The running loss at 5236 iteration is: 0.13460262300385661\n",
            "The running loss at 5237 iteration is: 0.13417935112451684\n",
            "The running loss at 5238 iteration is: 0.1337120928821358\n",
            "The running loss at 5239 iteration is: 0.13324249348144823\n",
            "The running loss at 5240 iteration is: 0.13281465824362448\n",
            "The running loss at 5241 iteration is: 0.13239809499602873\n",
            "The running loss at 5242 iteration is: 0.13194948644367796\n",
            "The running loss at 5243 iteration is: 0.13151827326357038\n",
            "The running loss at 5244 iteration is: 0.1310437522761093\n",
            "The running loss at 5245 iteration is: 0.1306069775882441\n",
            "The running loss at 5246 iteration is: 0.13015779169716257\n",
            "The running loss at 5247 iteration is: 0.1297537460006181\n",
            "The running loss at 5248 iteration is: 0.1293214154553636\n",
            "The running loss at 5249 iteration is: 0.12888836134296805\n",
            "The running loss at 5250 iteration is: 0.12851601926439812\n",
            "The running loss at 5251 iteration is: 0.12808712031757993\n",
            "The running loss at 5252 iteration is: 0.12764956372741265\n",
            "The running loss at 5253 iteration is: 0.12723837556260081\n",
            "The running loss at 5254 iteration is: 0.1268059631398326\n",
            "The running loss at 5255 iteration is: 0.12639103321833736\n",
            "The running loss at 5256 iteration is: 0.12599309620328192\n",
            "The running loss at 5257 iteration is: 0.1255720803275049\n",
            "The running loss at 5258 iteration is: 0.12512890016547906\n",
            "The running loss at 5259 iteration is: 0.12480388605426002\n",
            "The running loss at 5260 iteration is: 0.12438976855692822\n",
            "The running loss at 5261 iteration is: 0.1239520388430233\n",
            "The running loss at 5262 iteration is: 0.12359550492070537\n",
            "The running loss at 5263 iteration is: 0.12314734627534416\n",
            "The running loss at 5264 iteration is: 0.12276177772500119\n",
            "The running loss at 5265 iteration is: 0.12234509888474819\n",
            "The running loss at 5266 iteration is: 0.12200651716133981\n",
            "The running loss at 5267 iteration is: 0.12158773582934103\n",
            "The running loss at 5268 iteration is: 0.12123443302077418\n",
            "The running loss at 5269 iteration is: 0.12087402848093774\n",
            "The running loss at 5270 iteration is: 0.12048705722475396\n",
            "The running loss at 5271 iteration is: 0.12009356415847619\n",
            "The running loss at 5272 iteration is: 0.11971085832209591\n",
            "The running loss at 5273 iteration is: 0.11931832078108602\n",
            "The running loss at 5274 iteration is: 0.11900084870748052\n",
            "The running loss at 5275 iteration is: 0.11866117661123234\n",
            "The running loss at 5276 iteration is: 0.11829735324641678\n",
            "The running loss at 5277 iteration is: 0.11798201904118515\n",
            "The running loss at 5278 iteration is: 0.11762693048280584\n",
            "The running loss at 5279 iteration is: 0.11728415255631298\n",
            "The running loss at 5280 iteration is: 0.1170253915935704\n",
            "The running loss at 5281 iteration is: 0.11675347420672318\n",
            "The running loss at 5282 iteration is: 0.11654563800456667\n",
            "The running loss at 5283 iteration is: 0.11643097448874738\n",
            "The running loss at 5284 iteration is: 0.1165246968036165\n",
            "The running loss at 5285 iteration is: 0.11686698546804518\n",
            "The running loss at 5286 iteration is: 0.11775962016363524\n",
            "The running loss at 5287 iteration is: 0.11948088634677605\n",
            "The running loss at 5288 iteration is: 0.1228269636213431\n",
            "The running loss at 5289 iteration is: 0.1290683137761204\n",
            "The running loss at 5290 iteration is: 0.14089720693621766\n",
            "The running loss at 5291 iteration is: 0.16290763091688495\n",
            "The running loss at 5292 iteration is: 0.20589809232041287\n",
            "The running loss at 5293 iteration is: 0.2870436128752102\n",
            "The running loss at 5294 iteration is: 0.4529803630413718\n",
            "The running loss at 5295 iteration is: 0.7689569934659946\n",
            "The running loss at 5296 iteration is: 1.4513003522269006\n",
            "The running loss at 5297 iteration is: 2.713180512826202\n",
            "The running loss at 5298 iteration is: 5.593510301176383\n",
            "The running loss at 5299 iteration is: 10.17249772445629\n",
            "The running loss at 5300 iteration is: 20.69099604909638\n",
            "The running loss at 5301 iteration is: 29.06165608751134\n",
            "The running loss at 5302 iteration is: 43.66713880471562\n",
            "The running loss at 5303 iteration is: 26.324537292465152\n",
            "The running loss at 5304 iteration is: 7.628716258205\n",
            "The running loss at 5305 iteration is: 0.7618992456387529\n",
            "The running loss at 5306 iteration is: 12.493174065489677\n",
            "The running loss at 5307 iteration is: 17.80723094663107\n",
            "The running loss at 5308 iteration is: 3.2799623019125472\n",
            "The running loss at 5309 iteration is: 2.566356033831664\n",
            "The running loss at 5310 iteration is: 13.007598744980072\n",
            "The running loss at 5311 iteration is: 8.97890638058885\n",
            "The running loss at 5312 iteration is: 1.0995406085064525\n",
            "The running loss at 5313 iteration is: 2.3275942396170106\n",
            "The running loss at 5314 iteration is: 7.971634889405947\n",
            "The running loss at 5315 iteration is: 8.06807650573452\n",
            "The running loss at 5316 iteration is: 1.7774386340859933\n",
            "The running loss at 5317 iteration is: 0.8841507693616386\n",
            "The running loss at 5318 iteration is: 5.113719753547381\n",
            "The running loss at 5319 iteration is: 5.716183729637782\n",
            "The running loss at 5320 iteration is: 2.575301776143574\n",
            "The running loss at 5321 iteration is: 0.2961617754165135\n",
            "The running loss at 5322 iteration is: 2.137320960241971\n",
            "The running loss at 5323 iteration is: 4.208791367852361\n",
            "The running loss at 5324 iteration is: 2.4347128914635685\n",
            "The running loss at 5325 iteration is: 0.39739500684522067\n",
            "The running loss at 5326 iteration is: 1.0099021481323476\n",
            "The running loss at 5327 iteration is: 2.4077934727932653\n",
            "The running loss at 5328 iteration is: 1.9646115511700855\n",
            "The running loss at 5329 iteration is: 0.4538036904157521\n",
            "The running loss at 5330 iteration is: 0.6360194225034744\n",
            "The running loss at 5331 iteration is: 1.6871728404417543\n",
            "The running loss at 5332 iteration is: 1.303469636945718\n",
            "The running loss at 5333 iteration is: 0.3665436701925448\n",
            "The running loss at 5334 iteration is: 0.513772702595437\n",
            "The running loss at 5335 iteration is: 1.1329193726185343\n",
            "The running loss at 5336 iteration is: 0.902379019691485\n",
            "The running loss at 5337 iteration is: 0.29642730195377026\n",
            "The running loss at 5338 iteration is: 0.47964369744727886\n",
            "The running loss at 5339 iteration is: 0.8781076546407435\n",
            "The running loss at 5340 iteration is: 0.5902829679142289\n",
            "The running loss at 5341 iteration is: 0.25351604787380866\n",
            "The running loss at 5342 iteration is: 0.4597020693886943\n",
            "The running loss at 5343 iteration is: 0.6409042816268258\n",
            "The running loss at 5344 iteration is: 0.40783808742312894\n",
            "The running loss at 5345 iteration is: 0.24671959530612422\n",
            "The running loss at 5346 iteration is: 0.42694144745872264\n",
            "The running loss at 5347 iteration is: 0.4970678338898099\n",
            "The running loss at 5348 iteration is: 0.30273908563824603\n",
            "The running loss at 5349 iteration is: 0.25527252519424426\n",
            "The running loss at 5350 iteration is: 0.3929378945817845\n",
            "The running loss at 5351 iteration is: 0.3792327285700295\n",
            "The running loss at 5352 iteration is: 0.25043209109253495\n",
            "The running loss at 5353 iteration is: 0.2624798720653026\n",
            "The running loss at 5354 iteration is: 0.3461258190415437\n",
            "The running loss at 5355 iteration is: 0.3063510334630586\n",
            "The running loss at 5356 iteration is: 0.22994908885755933\n",
            "The running loss at 5357 iteration is: 0.2642000743754277\n",
            "The running loss at 5358 iteration is: 0.30836575872518907\n",
            "The running loss at 5359 iteration is: 0.26015882899675513\n",
            "The running loss at 5360 iteration is: 0.22330212703769048\n",
            "The running loss at 5361 iteration is: 0.25871077063162545\n",
            "The running loss at 5362 iteration is: 0.27296493559794\n",
            "The running loss at 5363 iteration is: 0.2340917728894074\n",
            "The running loss at 5364 iteration is: 0.22114391971105501\n",
            "The running loss at 5365 iteration is: 0.24818523974587578\n",
            "The running loss at 5366 iteration is: 0.24810424055795\n",
            "The running loss at 5367 iteration is: 0.2200623595420785\n",
            "The running loss at 5368 iteration is: 0.21926039287512372\n",
            "The running loss at 5369 iteration is: 0.2369560905788059\n",
            "The running loss at 5370 iteration is: 0.22987755470891555\n",
            "The running loss at 5371 iteration is: 0.21219089956363474\n",
            "The running loss at 5372 iteration is: 0.2159915284490515\n",
            "The running loss at 5373 iteration is: 0.22581821450383557\n",
            "The running loss at 5374 iteration is: 0.21779200964032122\n",
            "The running loss at 5375 iteration is: 0.2072194723918064\n",
            "The running loss at 5376 iteration is: 0.2117806412811998\n",
            "The running loss at 5377 iteration is: 0.21660420054824522\n",
            "The running loss at 5378 iteration is: 0.2093130835852975\n",
            "The running loss at 5379 iteration is: 0.2033752387944809\n",
            "The running loss at 5380 iteration is: 0.20706646804446321\n",
            "The running loss at 5381 iteration is: 0.208824141042105\n",
            "The running loss at 5382 iteration is: 0.20318873196710496\n",
            "The running loss at 5383 iteration is: 0.19982734269990424\n",
            "The running loss at 5384 iteration is: 0.20238284395801998\n",
            "The running loss at 5385 iteration is: 0.20276027402174177\n",
            "The running loss at 5386 iteration is: 0.19852000006778006\n",
            "The running loss at 5387 iteration is: 0.19636984810037963\n",
            "The running loss at 5388 iteration is: 0.19801225908647005\n",
            "The running loss at 5389 iteration is: 0.1976530663274997\n",
            "The running loss at 5390 iteration is: 0.1944993216090586\n",
            "The running loss at 5391 iteration is: 0.19295746364737462\n",
            "The running loss at 5392 iteration is: 0.1937298582910211\n",
            "The running loss at 5393 iteration is: 0.1931500144899707\n",
            "The running loss at 5394 iteration is: 0.19082840963127182\n",
            "The running loss at 5395 iteration is: 0.1896185755846201\n",
            "The running loss at 5396 iteration is: 0.18978992313333157\n",
            "The running loss at 5397 iteration is: 0.18922154460346105\n",
            "The running loss at 5398 iteration is: 0.18745429807565986\n",
            "The running loss at 5399 iteration is: 0.1863466191547201\n",
            "The running loss at 5400 iteration is: 0.1862085811097269\n",
            "The running loss at 5401 iteration is: 0.1856659980290539\n",
            "The running loss at 5402 iteration is: 0.18423691575699827\n",
            "The running loss at 5403 iteration is: 0.18314658587274132\n",
            "The running loss at 5404 iteration is: 0.1828157904226159\n",
            "The running loss at 5405 iteration is: 0.18228100128854227\n",
            "The running loss at 5406 iteration is: 0.18114323452130013\n",
            "The running loss at 5407 iteration is: 0.1801201760256731\n",
            "The running loss at 5408 iteration is: 0.17960907407824528\n",
            "The running loss at 5409 iteration is: 0.17907349916511447\n",
            "The running loss at 5410 iteration is: 0.17815282630866075\n",
            "The running loss at 5411 iteration is: 0.17716373360103269\n",
            "The running loss at 5412 iteration is: 0.17656685980640266\n",
            "The running loss at 5413 iteration is: 0.17597631532338281\n",
            "The running loss at 5414 iteration is: 0.17525246430789673\n",
            "The running loss at 5415 iteration is: 0.17430736970163552\n",
            "The running loss at 5416 iteration is: 0.17357790124241068\n",
            "The running loss at 5417 iteration is: 0.17300865684554712\n",
            "The running loss at 5418 iteration is: 0.17233580617012634\n",
            "The running loss at 5419 iteration is: 0.1715226881686713\n",
            "The running loss at 5420 iteration is: 0.17079422929398685\n",
            "The running loss at 5421 iteration is: 0.17013872153515114\n",
            "The running loss at 5422 iteration is: 0.16948500586135187\n",
            "The running loss at 5423 iteration is: 0.16877638239257306\n",
            "The running loss at 5424 iteration is: 0.16807388501733928\n",
            "The running loss at 5425 iteration is: 0.1674510825326414\n",
            "The running loss at 5426 iteration is: 0.1668141339628483\n",
            "The running loss at 5427 iteration is: 0.1661487565590901\n",
            "The running loss at 5428 iteration is: 0.16541072367681584\n",
            "The running loss at 5429 iteration is: 0.1647523521400745\n",
            "The running loss at 5430 iteration is: 0.1641489021194457\n",
            "The running loss at 5431 iteration is: 0.1634850434357339\n",
            "The running loss at 5432 iteration is: 0.16285957367691237\n",
            "The running loss at 5433 iteration is: 0.1621673209630519\n",
            "The running loss at 5434 iteration is: 0.16150846318649936\n",
            "The running loss at 5435 iteration is: 0.16094536589276484\n",
            "The running loss at 5436 iteration is: 0.16030614161076487\n",
            "The running loss at 5437 iteration is: 0.1596456343689503\n",
            "The running loss at 5438 iteration is: 0.1590510198007191\n",
            "The running loss at 5439 iteration is: 0.1584285932226733\n",
            "The running loss at 5440 iteration is: 0.15783865236102734\n",
            "The running loss at 5441 iteration is: 0.15720363736785975\n",
            "The running loss at 5442 iteration is: 0.15661697728517762\n",
            "The running loss at 5443 iteration is: 0.15597702540006728\n",
            "The running loss at 5444 iteration is: 0.15540006241096174\n",
            "The running loss at 5445 iteration is: 0.154832314507111\n",
            "The running loss at 5446 iteration is: 0.1542241784969747\n",
            "The running loss at 5447 iteration is: 0.15360463569080815\n",
            "The running loss at 5448 iteration is: 0.15302443532181811\n",
            "The running loss at 5449 iteration is: 0.15245123460029436\n",
            "The running loss at 5450 iteration is: 0.15186904209161517\n",
            "The running loss at 5451 iteration is: 0.1512954804591382\n",
            "The running loss at 5452 iteration is: 0.15072193279741142\n",
            "The running loss at 5453 iteration is: 0.15013911747734152\n",
            "The running loss at 5454 iteration is: 0.14963061058976407\n",
            "The running loss at 5455 iteration is: 0.14905668326729654\n",
            "The running loss at 5456 iteration is: 0.14846745523408508\n",
            "The running loss at 5457 iteration is: 0.1479278209808224\n",
            "The running loss at 5458 iteration is: 0.14740349688616958\n",
            "The running loss at 5459 iteration is: 0.1468479786840366\n",
            "The running loss at 5460 iteration is: 0.14627279824582276\n",
            "The running loss at 5461 iteration is: 0.1457417796910991\n",
            "The running loss at 5462 iteration is: 0.1451840721962378\n",
            "The running loss at 5463 iteration is: 0.14466250005603068\n",
            "The running loss at 5464 iteration is: 0.1441219965966618\n",
            "The running loss at 5465 iteration is: 0.14360480540178003\n",
            "The running loss at 5466 iteration is: 0.14309276870617688\n",
            "The running loss at 5467 iteration is: 0.14254292158702833\n",
            "The running loss at 5468 iteration is: 0.14202754216633773\n",
            "The running loss at 5469 iteration is: 0.14151898680101832\n",
            "The running loss at 5470 iteration is: 0.141017002395926\n",
            "The running loss at 5471 iteration is: 0.1405184988900776\n",
            "The running loss at 5472 iteration is: 0.14000258669265936\n",
            "The running loss at 5473 iteration is: 0.13947322927782738\n",
            "The running loss at 5474 iteration is: 0.13895332548530548\n",
            "The running loss at 5475 iteration is: 0.1384531452101527\n",
            "The running loss at 5476 iteration is: 0.13794920772267258\n",
            "The running loss at 5477 iteration is: 0.13750345284742185\n",
            "The running loss at 5478 iteration is: 0.13700390019251918\n",
            "The running loss at 5479 iteration is: 0.13650750037045759\n",
            "The running loss at 5480 iteration is: 0.13602679794918504\n",
            "The running loss at 5481 iteration is: 0.13551418760586031\n",
            "The running loss at 5482 iteration is: 0.13505676632189242\n",
            "The running loss at 5483 iteration is: 0.134559387677485\n",
            "The running loss at 5484 iteration is: 0.13405052732047085\n",
            "The running loss at 5485 iteration is: 0.13365055450407268\n",
            "The running loss at 5486 iteration is: 0.13315455150300687\n",
            "The running loss at 5487 iteration is: 0.1326765241910251\n",
            "The running loss at 5488 iteration is: 0.13222401942712234\n",
            "The running loss at 5489 iteration is: 0.13174123807263757\n",
            "The running loss at 5490 iteration is: 0.13128177676656896\n",
            "The running loss at 5491 iteration is: 0.13080872274886415\n",
            "The running loss at 5492 iteration is: 0.13034071834467148\n",
            "The running loss at 5493 iteration is: 0.12989977347187226\n",
            "The running loss at 5494 iteration is: 0.12941389953171117\n",
            "The running loss at 5495 iteration is: 0.12898654926292874\n",
            "The running loss at 5496 iteration is: 0.12853077519744238\n",
            "The running loss at 5497 iteration is: 0.12812265207309048\n",
            "The running loss at 5498 iteration is: 0.12764982927717566\n",
            "The running loss at 5499 iteration is: 0.12718586556714018\n",
            "The running loss at 5500 iteration is: 0.1267872734155493\n",
            "The running loss at 5501 iteration is: 0.12632567589059684\n",
            "The running loss at 5502 iteration is: 0.1259158181378341\n",
            "The running loss at 5503 iteration is: 0.12549636919451856\n",
            "The running loss at 5504 iteration is: 0.1250782305288034\n",
            "The running loss at 5505 iteration is: 0.12464610866466722\n",
            "The running loss at 5506 iteration is: 0.12425573355017142\n",
            "The running loss at 5507 iteration is: 0.12382692734932028\n",
            "The running loss at 5508 iteration is: 0.12343207986402335\n",
            "The running loss at 5509 iteration is: 0.12308305769601999\n",
            "The running loss at 5510 iteration is: 0.12269563018747735\n",
            "The running loss at 5511 iteration is: 0.12241687770947234\n",
            "The running loss at 5512 iteration is: 0.12221273486898937\n",
            "The running loss at 5513 iteration is: 0.12207504354269233\n",
            "The running loss at 5514 iteration is: 0.12221875829761338\n",
            "The running loss at 5515 iteration is: 0.12281707432258293\n",
            "The running loss at 5516 iteration is: 0.12418529108128357\n",
            "The running loss at 5517 iteration is: 0.12707767631723962\n",
            "The running loss at 5518 iteration is: 0.13297650988440327\n",
            "The running loss at 5519 iteration is: 0.14455064661375133\n",
            "The running loss at 5520 iteration is: 0.16782756521599249\n",
            "The running loss at 5521 iteration is: 0.2138068929385528\n",
            "The running loss at 5522 iteration is: 0.3097835663059554\n",
            "The running loss at 5523 iteration is: 0.5020250020451775\n",
            "The running loss at 5524 iteration is: 0.9255033572908464\n",
            "The running loss at 5525 iteration is: 1.767121262309863\n",
            "The running loss at 5526 iteration is: 3.732802965786932\n",
            "The running loss at 5527 iteration is: 7.316403981424633\n",
            "The running loss at 5528 iteration is: 16.03923730665395\n",
            "The running loss at 5529 iteration is: 26.84135582136081\n",
            "The running loss at 5530 iteration is: 50.36860772594139\n",
            "The running loss at 5531 iteration is: 44.4908903186947\n",
            "The running loss at 5532 iteration is: 33.325952234502154\n",
            "The running loss at 5533 iteration is: 3.442870695020593\n",
            "The running loss at 5534 iteration is: 6.9798360651312095\n",
            "The running loss at 5535 iteration is: 28.118161250213245\n",
            "The running loss at 5536 iteration is: 16.624035142448193\n",
            "The running loss at 5537 iteration is: 1.5777789171484116\n",
            "The running loss at 5538 iteration is: 4.48361957004292\n",
            "The running loss at 5539 iteration is: 15.7015602764428\n",
            "The running loss at 5540 iteration is: 18.85673605940424\n",
            "The running loss at 5541 iteration is: 5.70992969005048\n",
            "The running loss at 5542 iteration is: 0.3901938160982811\n",
            "The running loss at 5543 iteration is: 6.65886744389778\n",
            "The running loss at 5544 iteration is: 11.71121022191511\n",
            "The running loss at 5545 iteration is: 10.054741306123459\n",
            "The running loss at 5546 iteration is: 2.113431651783519\n",
            "The running loss at 5547 iteration is: 0.9337328146690179\n",
            "The running loss at 5548 iteration is: 5.856088691532155\n",
            "The running loss at 5549 iteration is: 6.568350956098226\n",
            "The running loss at 5550 iteration is: 2.789705217986563\n",
            "The running loss at 5551 iteration is: 0.28803374530370124\n",
            "The running loss at 5552 iteration is: 2.774179252739485\n",
            "The running loss at 5553 iteration is: 4.678120761714638\n",
            "The running loss at 5554 iteration is: 1.8532182793477612\n",
            "The running loss at 5555 iteration is: 0.29332130211566293\n",
            "The running loss at 5556 iteration is: 2.1156013162576355\n",
            "The running loss at 5557 iteration is: 2.653984453947658\n",
            "The running loss at 5558 iteration is: 0.9369492368398545\n",
            "The running loss at 5559 iteration is: 0.36184095176452397\n",
            "The running loss at 5560 iteration is: 1.6502075945636732\n",
            "The running loss at 5561 iteration is: 1.7451351652483476\n",
            "The running loss at 5562 iteration is: 0.42629043517374826\n",
            "The running loss at 5563 iteration is: 0.5374266155046697\n",
            "The running loss at 5564 iteration is: 1.4216881591061679\n",
            "The running loss at 5565 iteration is: 0.9244019173600352\n",
            "The running loss at 5566 iteration is: 0.22970791054189588\n",
            "The running loss at 5567 iteration is: 0.6449926922874144\n",
            "The running loss at 5568 iteration is: 0.9808830734069839\n",
            "The running loss at 5569 iteration is: 0.4917201740120766\n",
            "The running loss at 5570 iteration is: 0.23453863355736468\n",
            "The running loss at 5571 iteration is: 0.6302294409164193\n",
            "The running loss at 5572 iteration is: 0.6940457052455801\n",
            "The running loss at 5573 iteration is: 0.28626104346911674\n",
            "The running loss at 5574 iteration is: 0.2878592176761703\n",
            "The running loss at 5575 iteration is: 0.5685984268831625\n",
            "The running loss at 5576 iteration is: 0.44955772707060215\n",
            "The running loss at 5577 iteration is: 0.21251779608639781\n",
            "The running loss at 5578 iteration is: 0.31571240828106245\n",
            "The running loss at 5579 iteration is: 0.4503521956378608\n",
            "The running loss at 5580 iteration is: 0.3140793620277829\n",
            "The running loss at 5581 iteration is: 0.20098828460425944\n",
            "The running loss at 5582 iteration is: 0.3115677738356707\n",
            "The running loss at 5583 iteration is: 0.36388886077722327\n",
            "The running loss at 5584 iteration is: 0.24278874545618748\n",
            "The running loss at 5585 iteration is: 0.20460431183706748\n",
            "The running loss at 5586 iteration is: 0.29133807186159677\n",
            "The running loss at 5587 iteration is: 0.29251439495719705\n",
            "The running loss at 5588 iteration is: 0.20944707363226983\n",
            "The running loss at 5589 iteration is: 0.206237215823564\n",
            "The running loss at 5590 iteration is: 0.2632504114453981\n",
            "The running loss at 5591 iteration is: 0.24989011151160473\n",
            "The running loss at 5592 iteration is: 0.19511392089981436\n",
            "The running loss at 5593 iteration is: 0.20375507206016558\n",
            "The running loss at 5594 iteration is: 0.23959517473165717\n",
            "The running loss at 5595 iteration is: 0.22171953949752474\n",
            "The running loss at 5596 iteration is: 0.18782866424308708\n",
            "The running loss at 5597 iteration is: 0.19833191032589417\n",
            "The running loss at 5598 iteration is: 0.2196800400081396\n",
            "The running loss at 5599 iteration is: 0.20536985335360794\n",
            "The running loss at 5600 iteration is: 0.18359714839046018\n",
            "The running loss at 5601 iteration is: 0.19178434392736374\n",
            "The running loss at 5602 iteration is: 0.2055335968406147\n",
            "The running loss at 5603 iteration is: 0.19481609832751717\n",
            "The running loss at 5604 iteration is: 0.1802201017758733\n",
            "The running loss at 5605 iteration is: 0.18537943925473188\n",
            "The running loss at 5606 iteration is: 0.1942396795426349\n",
            "The running loss at 5607 iteration is: 0.18752929838106655\n",
            "The running loss at 5608 iteration is: 0.17721911655108757\n",
            "The running loss at 5609 iteration is: 0.1795938976197746\n",
            "The running loss at 5610 iteration is: 0.18565402136295586\n",
            "The running loss at 5611 iteration is: 0.1819397001929268\n",
            "The running loss at 5612 iteration is: 0.17453146598262612\n",
            "The running loss at 5613 iteration is: 0.17475586021540712\n",
            "The running loss at 5614 iteration is: 0.17882433256802235\n",
            "The running loss at 5615 iteration is: 0.17729716932014508\n",
            "The running loss at 5616 iteration is: 0.17203638421080128\n",
            "The running loss at 5617 iteration is: 0.17075255445791476\n",
            "The running loss at 5618 iteration is: 0.17321352922783254\n",
            "The running loss at 5619 iteration is: 0.1730783272638624\n",
            "The running loss at 5620 iteration is: 0.1696053929627548\n",
            "The running loss at 5621 iteration is: 0.16756925486961557\n",
            "The running loss at 5622 iteration is: 0.1686533098640942\n",
            "The running loss at 5623 iteration is: 0.1691595800245752\n",
            "The running loss at 5624 iteration is: 0.16713165096545657\n",
            "The running loss at 5625 iteration is: 0.1649300061154971\n",
            "The running loss at 5626 iteration is: 0.16475988548122852\n",
            "The running loss at 5627 iteration is: 0.16535108440050694\n",
            "The running loss at 5628 iteration is: 0.1644484703138193\n",
            "The running loss at 5629 iteration is: 0.16257339927705775\n",
            "The running loss at 5630 iteration is: 0.16162182671562655\n",
            "The running loss at 5631 iteration is: 0.16168388474083867\n",
            "The running loss at 5632 iteration is: 0.16140351181698553\n",
            "The running loss at 5633 iteration is: 0.16023817608447805\n",
            "The running loss at 5634 iteration is: 0.15907368318750323\n",
            "The running loss at 5635 iteration is: 0.15859937071618893\n",
            "The running loss at 5636 iteration is: 0.15841475239139674\n",
            "The running loss at 5637 iteration is: 0.15785509399610403\n",
            "The running loss at 5638 iteration is: 0.1568763190412072\n",
            "The running loss at 5639 iteration is: 0.15599622139643493\n",
            "The running loss at 5640 iteration is: 0.15546051427093924\n",
            "The running loss at 5641 iteration is: 0.15513031665988347\n",
            "The running loss at 5642 iteration is: 0.15453998091735677\n",
            "The running loss at 5643 iteration is: 0.1537264302854225\n",
            "The running loss at 5644 iteration is: 0.15294730955624256\n",
            "The running loss at 5645 iteration is: 0.15244829908350135\n",
            "The running loss at 5646 iteration is: 0.15197118386368447\n",
            "The running loss at 5647 iteration is: 0.1514326220414649\n",
            "The running loss at 5648 iteration is: 0.15072277812279788\n",
            "The running loss at 5649 iteration is: 0.15004857800215102\n",
            "The running loss at 5650 iteration is: 0.14949966865226377\n",
            "The running loss at 5651 iteration is: 0.14904443438874723\n",
            "The running loss at 5652 iteration is: 0.1484566789487748\n",
            "The running loss at 5653 iteration is: 0.14785093956835246\n",
            "The running loss at 5654 iteration is: 0.14724066845888323\n",
            "The running loss at 5655 iteration is: 0.14664915477269172\n",
            "The running loss at 5656 iteration is: 0.1461551760350581\n",
            "The running loss at 5657 iteration is: 0.14563048900348521\n",
            "The running loss at 5658 iteration is: 0.14509125565408193\n",
            "The running loss at 5659 iteration is: 0.14451635602568963\n",
            "The running loss at 5660 iteration is: 0.14390867955385772\n",
            "The running loss at 5661 iteration is: 0.14339671592302908\n",
            "The running loss at 5662 iteration is: 0.1428696379764573\n",
            "The running loss at 5663 iteration is: 0.14236691330150536\n",
            "The running loss at 5664 iteration is: 0.14181315578855977\n",
            "The running loss at 5665 iteration is: 0.1412492830670813\n",
            "The running loss at 5666 iteration is: 0.1407264792517044\n",
            "The running loss at 5667 iteration is: 0.14021508256079848\n",
            "The running loss at 5668 iteration is: 0.13969420880538752\n",
            "The running loss at 5669 iteration is: 0.13919978897228988\n",
            "The running loss at 5670 iteration is: 0.13866496473979348\n",
            "The running loss at 5671 iteration is: 0.1381962821790811\n",
            "The running loss at 5672 iteration is: 0.13763230430039203\n",
            "The running loss at 5673 iteration is: 0.13715130743833123\n",
            "The running loss at 5674 iteration is: 0.1366510049909572\n",
            "The running loss at 5675 iteration is: 0.1361737246572891\n",
            "The running loss at 5676 iteration is: 0.13565341533863967\n",
            "The running loss at 5677 iteration is: 0.1351946113816569\n",
            "The running loss at 5678 iteration is: 0.1346825337583835\n",
            "The running loss at 5679 iteration is: 0.13417054010621288\n",
            "The running loss at 5680 iteration is: 0.13369367851787256\n",
            "The running loss at 5681 iteration is: 0.13319180231001732\n",
            "The running loss at 5682 iteration is: 0.132729436668924\n",
            "The running loss at 5683 iteration is: 0.13226935189684755\n",
            "The running loss at 5684 iteration is: 0.13175695985581742\n",
            "The running loss at 5685 iteration is: 0.13132643321115098\n",
            "The running loss at 5686 iteration is: 0.13081406080423244\n",
            "The running loss at 5687 iteration is: 0.13038150877604082\n",
            "The running loss at 5688 iteration is: 0.1298634986212474\n",
            "The running loss at 5689 iteration is: 0.12942776669898776\n",
            "The running loss at 5690 iteration is: 0.12897661308280087\n",
            "The running loss at 5691 iteration is: 0.12852130730750486\n",
            "The running loss at 5692 iteration is: 0.1280624342723612\n",
            "The running loss at 5693 iteration is: 0.12762915290268265\n",
            "The running loss at 5694 iteration is: 0.12721081072744445\n",
            "The running loss at 5695 iteration is: 0.12672395797442534\n",
            "The running loss at 5696 iteration is: 0.126276671369616\n",
            "The running loss at 5697 iteration is: 0.12584083484340086\n",
            "The running loss at 5698 iteration is: 0.125388026862359\n",
            "The running loss at 5699 iteration is: 0.12496809739489731\n",
            "The running loss at 5700 iteration is: 0.12451538311785436\n",
            "The running loss at 5701 iteration is: 0.12409306020820626\n",
            "The running loss at 5702 iteration is: 0.12364389118216973\n",
            "The running loss at 5703 iteration is: 0.12323372890416039\n",
            "The running loss at 5704 iteration is: 0.12279927048287892\n",
            "The running loss at 5705 iteration is: 0.12236463042934997\n",
            "The running loss at 5706 iteration is: 0.12192621533372412\n",
            "The running loss at 5707 iteration is: 0.12152532696995094\n",
            "The running loss at 5708 iteration is: 0.1211464634390971\n",
            "The running loss at 5709 iteration is: 0.12073482989791556\n",
            "The running loss at 5710 iteration is: 0.12033851013130158\n",
            "The running loss at 5711 iteration is: 0.11995545642067999\n",
            "The running loss at 5712 iteration is: 0.11960743738774006\n",
            "The running loss at 5713 iteration is: 0.11934418515139414\n",
            "The running loss at 5714 iteration is: 0.11914533004287264\n",
            "The running loss at 5715 iteration is: 0.11912103714781272\n",
            "The running loss at 5716 iteration is: 0.11936666597379286\n",
            "The running loss at 5717 iteration is: 0.12018554471795491\n",
            "The running loss at 5718 iteration is: 0.12211826406583974\n",
            "The running loss at 5719 iteration is: 0.12633289477155285\n",
            "The running loss at 5720 iteration is: 0.13524367627599876\n",
            "The running loss at 5721 iteration is: 0.15411578140843593\n",
            "The running loss at 5722 iteration is: 0.19315723220428171\n",
            "The running loss at 5723 iteration is: 0.2779524399367463\n",
            "The running loss at 5724 iteration is: 0.455749563903917\n",
            "The running loss at 5725 iteration is: 0.861593373441772\n",
            "The running loss at 5726 iteration is: 1.7062885224225988\n",
            "The running loss at 5727 iteration is: 3.7494409493686987\n",
            "The running loss at 5728 iteration is: 7.656384904284508\n",
            "The running loss at 5729 iteration is: 17.48836540268346\n",
            "The running loss at 5730 iteration is: 29.948993129731505\n",
            "The running loss at 5731 iteration is: 57.12985564647956\n",
            "The running loss at 5732 iteration is: 46.69088612813404\n",
            "The running loss at 5733 iteration is: 29.007545042740187\n",
            "The running loss at 5734 iteration is: 1.0929726798534387\n",
            "The running loss at 5735 iteration is: 13.146405340536155\n",
            "The running loss at 5736 iteration is: 34.65058237436033\n",
            "The running loss at 5737 iteration is: 12.547283186227064\n",
            "The running loss at 5738 iteration is: 0.42735962827229673\n",
            "The running loss at 5739 iteration is: 13.77149354710901\n",
            "The running loss at 5740 iteration is: 20.48260453417954\n",
            "The running loss at 5741 iteration is: 14.055990750061223\n",
            "The running loss at 5742 iteration is: 1.2669904141617887\n",
            "The running loss at 5743 iteration is: 4.076446144503462\n",
            "The running loss at 5744 iteration is: 14.895178972170488\n",
            "The running loss at 5745 iteration is: 12.399289369950157\n",
            "The running loss at 5746 iteration is: 4.1261358077259\n",
            "The running loss at 5747 iteration is: 0.4637884948612409\n",
            "The running loss at 5748 iteration is: 5.7768369381888\n",
            "The running loss at 5749 iteration is: 9.371688453203374\n",
            "The running loss at 5750 iteration is: 3.3152503260261557\n",
            "The running loss at 5751 iteration is: 0.3821825591572592\n",
            "The running loss at 5752 iteration is: 4.08219211486791\n",
            "The running loss at 5753 iteration is: 4.785136194599467\n",
            "The running loss at 5754 iteration is: 1.3966718731544636\n",
            "The running loss at 5755 iteration is: 0.6104536313838427\n",
            "The running loss at 5756 iteration is: 3.1114905434132023\n",
            "The running loss at 5757 iteration is: 2.8488453259590774\n",
            "The running loss at 5758 iteration is: 0.3835102020929401\n",
            "The running loss at 5759 iteration is: 1.2594729598430034\n",
            "The running loss at 5760 iteration is: 2.6828373980168356\n",
            "The running loss at 5761 iteration is: 1.030324199747019\n",
            "The running loss at 5762 iteration is: 0.34015326490310144\n",
            "The running loss at 5763 iteration is: 1.6427018534017936\n",
            "The running loss at 5764 iteration is: 1.3781782770223574\n",
            "The running loss at 5765 iteration is: 0.2750343970001583\n",
            "The running loss at 5766 iteration is: 0.6984587427971188\n",
            "The running loss at 5767 iteration is: 1.2445742123481278\n",
            "The running loss at 5768 iteration is: 0.5795489899411733\n",
            "The running loss at 5769 iteration is: 0.25599242612129103\n",
            "The running loss at 5770 iteration is: 0.8344476502705171\n",
            "The running loss at 5771 iteration is: 0.7992850009684814\n",
            "The running loss at 5772 iteration is: 0.24324448637686416\n",
            "The running loss at 5773 iteration is: 0.4288397516115126\n",
            "The running loss at 5774 iteration is: 0.7458607212928\n",
            "The running loss at 5775 iteration is: 0.3979212053999814\n",
            "The running loss at 5776 iteration is: 0.21953352935656967\n",
            "The running loss at 5777 iteration is: 0.5065085000129788\n",
            "The running loss at 5778 iteration is: 0.4920141949891707\n",
            "The running loss at 5779 iteration is: 0.22516826762697625\n",
            "The running loss at 5780 iteration is: 0.2817350168081218\n",
            "The running loss at 5781 iteration is: 0.4429634710622267\n",
            "The running loss at 5782 iteration is: 0.3175599671293514\n",
            "The running loss at 5783 iteration is: 0.19504981313899022\n",
            "The running loss at 5784 iteration is: 0.31218695773243393\n",
            "The running loss at 5785 iteration is: 0.3560616268601842\n",
            "The running loss at 5786 iteration is: 0.2249352851558694\n",
            "The running loss at 5787 iteration is: 0.20991289354745976\n",
            "The running loss at 5788 iteration is: 0.3031812597102702\n",
            "The running loss at 5789 iteration is: 0.27292611274813067\n",
            "The running loss at 5790 iteration is: 0.1914546818888713\n",
            "The running loss at 5791 iteration is: 0.22195205917841138\n",
            "The running loss at 5792 iteration is: 0.26874464267996595\n",
            "The running loss at 5793 iteration is: 0.22337085162176995\n",
            "The running loss at 5794 iteration is: 0.1848001722123192\n",
            "The running loss at 5795 iteration is: 0.22138878613994373\n",
            "The running loss at 5796 iteration is: 0.23762897535566868\n",
            "The running loss at 5797 iteration is: 0.19684695600953106\n",
            "The running loss at 5798 iteration is: 0.18454211874740778\n",
            "The running loss at 5799 iteration is: 0.21325409006165857\n",
            "The running loss at 5800 iteration is: 0.21224972917360665\n",
            "The running loss at 5801 iteration is: 0.1839783015021504\n",
            "The running loss at 5802 iteration is: 0.1835722643619307\n",
            "The running loss at 5803 iteration is: 0.20193345901940102\n",
            "The running loss at 5804 iteration is: 0.1959497108474616\n",
            "The running loss at 5805 iteration is: 0.17764204900001848\n",
            "The running loss at 5806 iteration is: 0.1808288523850328\n",
            "The running loss at 5807 iteration is: 0.19212031790629194\n",
            "The running loss at 5808 iteration is: 0.1854774930280596\n",
            "The running loss at 5809 iteration is: 0.17381231216785756\n",
            "The running loss at 5810 iteration is: 0.17677385679538118\n",
            "The running loss at 5811 iteration is: 0.18335255346044307\n",
            "The running loss at 5812 iteration is: 0.17828829441876323\n",
            "The running loss at 5813 iteration is: 0.17070713092971207\n",
            "The running loss at 5814 iteration is: 0.17272835075472695\n",
            "The running loss at 5815 iteration is: 0.17677263314252953\n",
            "The running loss at 5816 iteration is: 0.1732946353892002\n",
            "The running loss at 5817 iteration is: 0.1679488786508556\n",
            "The running loss at 5818 iteration is: 0.16881428951140134\n",
            "The running loss at 5819 iteration is: 0.17142504517644813\n",
            "The running loss at 5820 iteration is: 0.16922396382646054\n",
            "The running loss at 5821 iteration is: 0.16538135787459957\n",
            "The running loss at 5822 iteration is: 0.1652981130314604\n",
            "The running loss at 5823 iteration is: 0.16701963058535707\n",
            "The running loss at 5824 iteration is: 0.16588777884396214\n",
            "The running loss at 5825 iteration is: 0.16292918824444644\n",
            "The running loss at 5826 iteration is: 0.16211936177788486\n",
            "The running loss at 5827 iteration is: 0.16308585311009005\n",
            "The running loss at 5828 iteration is: 0.16268846776375867\n",
            "The running loss at 5829 iteration is: 0.1606482287489612\n",
            "The running loss at 5830 iteration is: 0.15930716000971393\n",
            "The running loss at 5831 iteration is: 0.15958901966499053\n",
            "The running loss at 5832 iteration is: 0.1596289076917721\n",
            "The running loss at 5833 iteration is: 0.15832743800122706\n",
            "The running loss at 5834 iteration is: 0.15687299277134487\n",
            "The running loss at 5835 iteration is: 0.156495470296256\n",
            "The running loss at 5836 iteration is: 0.15649205762048896\n",
            "The running loss at 5837 iteration is: 0.15580578175181287\n",
            "The running loss at 5838 iteration is: 0.15465184816119631\n",
            "The running loss at 5839 iteration is: 0.15385517550486483\n",
            "The running loss at 5840 iteration is: 0.15357651960591132\n",
            "The running loss at 5841 iteration is: 0.15324443188801348\n",
            "The running loss at 5842 iteration is: 0.15245668760416775\n",
            "The running loss at 5843 iteration is: 0.15155515006644554\n",
            "The running loss at 5844 iteration is: 0.15093531465638804\n",
            "The running loss at 5845 iteration is: 0.15061488771467432\n",
            "The running loss at 5846 iteration is: 0.1501820418396142\n",
            "The running loss at 5847 iteration is: 0.14943789206345517\n",
            "The running loss at 5848 iteration is: 0.14867357045902882\n",
            "The running loss at 5849 iteration is: 0.14809464962343125\n",
            "The running loss at 5850 iteration is: 0.14769642618216008\n",
            "The running loss at 5851 iteration is: 0.14725179670019212\n",
            "The running loss at 5852 iteration is: 0.14657542283209032\n",
            "The running loss at 5853 iteration is: 0.14588628308145193\n",
            "The running loss at 5854 iteration is: 0.1453199941514651\n",
            "The running loss at 5855 iteration is: 0.14482084807002332\n",
            "The running loss at 5856 iteration is: 0.1443662783989268\n",
            "The running loss at 5857 iteration is: 0.14380717500560034\n",
            "The running loss at 5858 iteration is: 0.14318743542592868\n",
            "The running loss at 5859 iteration is: 0.1426430360136202\n",
            "The running loss at 5860 iteration is: 0.14212405713685372\n",
            "The running loss at 5861 iteration is: 0.14161644180368976\n",
            "The running loss at 5862 iteration is: 0.1411095295621708\n",
            "The running loss at 5863 iteration is: 0.14057037099825817\n",
            "The running loss at 5864 iteration is: 0.1400094122571347\n",
            "The running loss at 5865 iteration is: 0.13950400047845468\n",
            "The running loss at 5866 iteration is: 0.13902061203746435\n",
            "The running loss at 5867 iteration is: 0.13850393352525564\n",
            "The running loss at 5868 iteration is: 0.13800238619676622\n",
            "The running loss at 5869 iteration is: 0.13746372262876147\n",
            "The running loss at 5870 iteration is: 0.13693616878524764\n",
            "The running loss at 5871 iteration is: 0.13648331306979958\n",
            "The running loss at 5872 iteration is: 0.13598883801193026\n",
            "The running loss at 5873 iteration is: 0.13551722630242283\n",
            "The running loss at 5874 iteration is: 0.13505577545038414\n",
            "The running loss at 5875 iteration is: 0.1345373066052417\n",
            "The running loss at 5876 iteration is: 0.134002996762107\n",
            "The running loss at 5877 iteration is: 0.1335482711455653\n",
            "The running loss at 5878 iteration is: 0.13305325262850604\n",
            "The running loss at 5879 iteration is: 0.13261195862198877\n",
            "The running loss at 5880 iteration is: 0.1321088394359026\n",
            "The running loss at 5881 iteration is: 0.13166825780276117\n",
            "The running loss at 5882 iteration is: 0.13115664145313827\n",
            "The running loss at 5883 iteration is: 0.13070535378877016\n",
            "The running loss at 5884 iteration is: 0.13018802899221393\n",
            "The running loss at 5885 iteration is: 0.12975195448021992\n",
            "The running loss at 5886 iteration is: 0.1293133248420116\n",
            "The running loss at 5887 iteration is: 0.12884436442487684\n",
            "The running loss at 5888 iteration is: 0.12837958345531233\n",
            "The running loss at 5889 iteration is: 0.12792905037581537\n",
            "The running loss at 5890 iteration is: 0.12749763345806775\n",
            "The running loss at 5891 iteration is: 0.1270381821536561\n",
            "The running loss at 5892 iteration is: 0.1266101836889433\n",
            "The running loss at 5893 iteration is: 0.12614080255190221\n",
            "The running loss at 5894 iteration is: 0.12571790515442485\n",
            "The running loss at 5895 iteration is: 0.12528400286589034\n",
            "The running loss at 5896 iteration is: 0.12487265891099314\n",
            "The running loss at 5897 iteration is: 0.12442492359742803\n",
            "The running loss at 5898 iteration is: 0.12398553832212114\n",
            "The running loss at 5899 iteration is: 0.12355056306807667\n",
            "The running loss at 5900 iteration is: 0.12311132732918988\n",
            "The running loss at 5901 iteration is: 0.12268110698054453\n",
            "The running loss at 5902 iteration is: 0.12229809289423386\n",
            "The running loss at 5903 iteration is: 0.12185302306130084\n",
            "The running loss at 5904 iteration is: 0.12143083457682687\n",
            "The running loss at 5905 iteration is: 0.12099489757667153\n",
            "The running loss at 5906 iteration is: 0.12060308558641414\n",
            "The running loss at 5907 iteration is: 0.1201839417123824\n",
            "The running loss at 5908 iteration is: 0.1197347689977815\n",
            "The running loss at 5909 iteration is: 0.11936485970143532\n",
            "The running loss at 5910 iteration is: 0.1189360447813905\n",
            "The running loss at 5911 iteration is: 0.11854286634569235\n",
            "The running loss at 5912 iteration is: 0.11814035515691161\n",
            "The running loss at 5913 iteration is: 0.11774265688853709\n",
            "The running loss at 5914 iteration is: 0.11732058839601486\n",
            "The running loss at 5915 iteration is: 0.11694398800274451\n",
            "The running loss at 5916 iteration is: 0.1165620640195859\n",
            "The running loss at 5917 iteration is: 0.1161350952716737\n",
            "The running loss at 5918 iteration is: 0.11577346704040965\n",
            "The running loss at 5919 iteration is: 0.11541145850024633\n",
            "The running loss at 5920 iteration is: 0.11496139481856642\n",
            "The running loss at 5921 iteration is: 0.1146177205710068\n",
            "The running loss at 5922 iteration is: 0.11422757924698673\n",
            "The running loss at 5923 iteration is: 0.11387595919992137\n",
            "The running loss at 5924 iteration is: 0.11353331721968019\n",
            "The running loss at 5925 iteration is: 0.11318201891944858\n",
            "The running loss at 5926 iteration is: 0.11290553644823709\n",
            "The running loss at 5927 iteration is: 0.1128125801894971\n",
            "The running loss at 5928 iteration is: 0.11295431583904525\n",
            "The running loss at 5929 iteration is: 0.11364658800800324\n",
            "The running loss at 5930 iteration is: 0.11574025845009463\n",
            "The running loss at 5931 iteration is: 0.12110772025236757\n",
            "The running loss at 5932 iteration is: 0.1341413531187081\n",
            "The running loss at 5933 iteration is: 0.16596313001907975\n",
            "The running loss at 5934 iteration is: 0.24233015036334732\n",
            "The running loss at 5935 iteration is: 0.4346287250836766\n",
            "The running loss at 5936 iteration is: 0.8979981218264185\n",
            "The running loss at 5937 iteration is: 2.124810219669089\n",
            "The running loss at 5938 iteration is: 4.965417514852815\n",
            "The running loss at 5939 iteration is: 12.888542765971648\n",
            "The running loss at 5940 iteration is: 27.14635088524657\n",
            "The running loss at 5941 iteration is: 63.94233541882163\n",
            "The running loss at 5942 iteration is: 68.69127850863859\n",
            "The running loss at 5943 iteration is: 66.18467811669976\n",
            "The running loss at 5944 iteration is: 5.706888780312048\n",
            "The running loss at 5945 iteration is: 18.671108560148166\n",
            "The running loss at 5946 iteration is: 59.615995229952176\n",
            "The running loss at 5947 iteration is: 15.641802850133507\n",
            "The running loss at 5948 iteration is: 3.7519169941652213\n",
            "The running loss at 5949 iteration is: 36.290865725235676\n",
            "The running loss at 5950 iteration is: 31.304060430763254\n",
            "The running loss at 5951 iteration is: 10.200305631234967\n",
            "The running loss at 5952 iteration is: 1.080661816493397\n",
            "The running loss at 5953 iteration is: 17.106701414958923\n",
            "The running loss at 5954 iteration is: 27.813202097556832\n",
            "The running loss at 5955 iteration is: 6.760127192856756\n",
            "The running loss at 5956 iteration is: 2.0521714027329594\n",
            "The running loss at 5957 iteration is: 15.214405492451673\n",
            "The running loss at 5958 iteration is: 9.454908298643998\n",
            "The running loss at 5959 iteration is: 0.3562707531435072\n",
            "The running loss at 5960 iteration is: 5.779141947463231\n",
            "The running loss at 5961 iteration is: 7.3774118565857565\n",
            "The running loss at 5962 iteration is: 1.1218487484576865\n",
            "The running loss at 5963 iteration is: 2.2310804170159217\n",
            "The running loss at 5964 iteration is: 5.335403952766338\n",
            "The running loss at 5965 iteration is: 1.6350403054979752\n",
            "The running loss at 5966 iteration is: 0.9431402383343548\n",
            "The running loss at 5967 iteration is: 3.9031853843224904\n",
            "The running loss at 5968 iteration is: 1.8530843210136234\n",
            "The running loss at 5969 iteration is: 0.43552896260504137\n",
            "The running loss at 5970 iteration is: 2.8810310518532893\n",
            "The running loss at 5971 iteration is: 1.9165754969917785\n",
            "The running loss at 5972 iteration is: 0.24017809579819202\n",
            "The running loss at 5973 iteration is: 2.093389891542933\n",
            "The running loss at 5974 iteration is: 1.877091772729207\n",
            "The running loss at 5975 iteration is: 0.20285561335192565\n",
            "The running loss at 5976 iteration is: 1.459561670950002\n",
            "The running loss at 5977 iteration is: 1.7452414920590877\n",
            "The running loss at 5978 iteration is: 0.255222534534341\n",
            "The running loss at 5979 iteration is: 0.9504059927169018\n",
            "The running loss at 5980 iteration is: 1.523921647825282\n",
            "The running loss at 5981 iteration is: 0.350486992861402\n",
            "The running loss at 5982 iteration is: 0.5701697697365085\n",
            "The running loss at 5983 iteration is: 1.2369432617393965\n",
            "The running loss at 5984 iteration is: 0.447504741649788\n",
            "The running loss at 5985 iteration is: 0.3250076140934246\n",
            "The running loss at 5986 iteration is: 0.9269959633896274\n",
            "The running loss at 5987 iteration is: 0.515140145507527\n",
            "The running loss at 5988 iteration is: 0.2050712056227663\n",
            "The running loss at 5989 iteration is: 0.6380200744295191\n",
            "The running loss at 5990 iteration is: 0.534263356861073\n",
            "The running loss at 5991 iteration is: 0.18424071218210736\n",
            "The running loss at 5992 iteration is: 0.40777965046697084\n",
            "The running loss at 5993 iteration is: 0.5008414234584077\n",
            "The running loss at 5994 iteration is: 0.22203499347521374\n",
            "The running loss at 5995 iteration is: 0.2582321343185659\n",
            "The running loss at 5996 iteration is: 0.4263032793780598\n",
            "The running loss at 5997 iteration is: 0.273850227319349\n",
            "The running loss at 5998 iteration is: 0.18774884960723837\n",
            "The running loss at 5999 iteration is: 0.33288150394599464\n",
            "The running loss at 6000 iteration is: 0.304125466258648\n",
            "The running loss at 6001 iteration is: 0.17754045653258096\n",
            "The running loss at 6002 iteration is: 0.24804631543196487\n",
            "The running loss at 6003 iteration is: 0.29817448917431033\n",
            "The running loss at 6004 iteration is: 0.19793512072652295\n",
            "The running loss at 6005 iteration is: 0.19206771741637493\n",
            "The running loss at 6006 iteration is: 0.2621177264690819\n",
            "The running loss at 6007 iteration is: 0.2202546498189416\n",
            "The running loss at 6008 iteration is: 0.1714869799054564\n",
            "The running loss at 6009 iteration is: 0.21622090892168924\n",
            "The running loss at 6010 iteration is: 0.2260090545036907\n",
            "The running loss at 6011 iteration is: 0.177118197000372\n",
            "The running loss at 6012 iteration is: 0.18128098692872546\n",
            "The running loss at 6013 iteration is: 0.21205631927676807\n",
            "The running loss at 6014 iteration is: 0.19008834523424795\n",
            "The running loss at 6015 iteration is: 0.1674241649507214\n",
            "The running loss at 6016 iteration is: 0.18867909374186564\n",
            "The running loss at 6017 iteration is: 0.194611244460539\n",
            "The running loss at 6018 iteration is: 0.1703157529987929\n",
            "The running loss at 6019 iteration is: 0.16975475660187542\n",
            "The running loss at 6020 iteration is: 0.18545686859659113\n",
            "The running loss at 6021 iteration is: 0.17691856156601754\n",
            "The running loss at 6022 iteration is: 0.16345396891111527\n",
            "The running loss at 6023 iteration is: 0.17120596931430682\n",
            "The running loss at 6024 iteration is: 0.17683068986344952\n",
            "The running loss at 6025 iteration is: 0.16615465663413\n",
            "The running loss at 6026 iteration is: 0.161736026023071\n",
            "The running loss at 6027 iteration is: 0.16906381584102995\n",
            "The running loss at 6028 iteration is: 0.1685986560408256\n",
            "The running loss at 6029 iteration is: 0.16051077241902661\n",
            "The running loss at 6030 iteration is: 0.1606065657130709\n",
            "The running loss at 6031 iteration is: 0.16511898835816566\n",
            "The running loss at 6032 iteration is: 0.1622383881573356\n",
            "The running loss at 6033 iteration is: 0.1573392990907027\n",
            "The running loss at 6034 iteration is: 0.15884923017739586\n",
            "The running loss at 6035 iteration is: 0.16095688945456338\n",
            "The running loss at 6036 iteration is: 0.1579012416210878\n",
            "The running loss at 6037 iteration is: 0.15503497840515176\n",
            "The running loss at 6038 iteration is: 0.15643868484237086\n",
            "The running loss at 6039 iteration is: 0.15719840849064276\n",
            "The running loss at 6040 iteration is: 0.1546647193708735\n",
            "The running loss at 6041 iteration is: 0.15294327696677812\n",
            "The running loss at 6042 iteration is: 0.15382324853864854\n",
            "The running loss at 6043 iteration is: 0.15394893601472764\n",
            "The running loss at 6044 iteration is: 0.15206352884690727\n",
            "The running loss at 6045 iteration is: 0.15083968375454565\n",
            "The running loss at 6046 iteration is: 0.15127534813888194\n",
            "The running loss at 6047 iteration is: 0.1511579243111287\n",
            "The running loss at 6048 iteration is: 0.14979530729108864\n",
            "The running loss at 6049 iteration is: 0.1487994679582446\n",
            "The running loss at 6050 iteration is: 0.14886946224687778\n",
            "The running loss at 6051 iteration is: 0.14874030495293158\n",
            "The running loss at 6052 iteration is: 0.14767432385142437\n",
            "The running loss at 6053 iteration is: 0.14678029295687764\n",
            "The running loss at 6054 iteration is: 0.1465817290663373\n",
            "The running loss at 6055 iteration is: 0.14645407988611175\n",
            "The running loss at 6056 iteration is: 0.14565125516070626\n",
            "The running loss at 6057 iteration is: 0.1447628236579293\n",
            "The running loss at 6058 iteration is: 0.14440714493985907\n",
            "The running loss at 6059 iteration is: 0.14420035411807866\n",
            "The running loss at 6060 iteration is: 0.14365493413276834\n",
            "The running loss at 6061 iteration is: 0.14286284549838602\n",
            "The running loss at 6062 iteration is: 0.14236971443094396\n",
            "The running loss at 6063 iteration is: 0.14205791664068673\n",
            "The running loss at 6064 iteration is: 0.14165229858747183\n",
            "The running loss at 6065 iteration is: 0.14098413738865434\n",
            "The running loss at 6066 iteration is: 0.14041869202379492\n",
            "The running loss at 6067 iteration is: 0.14003301294101797\n",
            "The running loss at 6068 iteration is: 0.13963332023253325\n",
            "The running loss at 6069 iteration is: 0.13914481075886556\n",
            "The running loss at 6070 iteration is: 0.13855856673950345\n",
            "The running loss at 6071 iteration is: 0.13808540701519564\n",
            "The running loss at 6072 iteration is: 0.13769183275925972\n",
            "The running loss at 6073 iteration is: 0.137290964455219\n",
            "The running loss at 6074 iteration is: 0.1367903489163837\n",
            "The running loss at 6075 iteration is: 0.13626114296800537\n",
            "The running loss at 6076 iteration is: 0.13583655925575416\n",
            "The running loss at 6077 iteration is: 0.13542598749666726\n",
            "The running loss at 6078 iteration is: 0.13499296869956026\n",
            "The running loss at 6079 iteration is: 0.13449964359180094\n",
            "The running loss at 6080 iteration is: 0.1340155888793231\n",
            "The running loss at 6081 iteration is: 0.13360413668693416\n",
            "The running loss at 6082 iteration is: 0.13316757078558594\n",
            "The running loss at 6083 iteration is: 0.13274068832664165\n",
            "The running loss at 6084 iteration is: 0.13231136113342462\n",
            "The running loss at 6085 iteration is: 0.13185373999344646\n",
            "The running loss at 6086 iteration is: 0.1314071532955837\n",
            "The running loss at 6087 iteration is: 0.13103211680926616\n",
            "The running loss at 6088 iteration is: 0.13060654629559595\n",
            "The running loss at 6089 iteration is: 0.13015324737666506\n",
            "The running loss at 6090 iteration is: 0.1296912860091845\n",
            "The running loss at 6091 iteration is: 0.12928281172328404\n",
            "The running loss at 6092 iteration is: 0.12885587609240798\n",
            "The running loss at 6093 iteration is: 0.12846479602878771\n",
            "The running loss at 6094 iteration is: 0.12803044967300184\n",
            "The running loss at 6095 iteration is: 0.12764569804732775\n",
            "The running loss at 6096 iteration is: 0.12720047311210084\n",
            "The running loss at 6097 iteration is: 0.12680292036020333\n",
            "The running loss at 6098 iteration is: 0.12636866955877443\n",
            "The running loss at 6099 iteration is: 0.12598160793696841\n",
            "The running loss at 6100 iteration is: 0.12560531882455614\n",
            "The running loss at 6101 iteration is: 0.12517961492277901\n",
            "The running loss at 6102 iteration is: 0.12479192836842616\n",
            "The running loss at 6103 iteration is: 0.12439864689430397\n",
            "The running loss at 6104 iteration is: 0.12401354256548404\n",
            "The running loss at 6105 iteration is: 0.12359275221906588\n",
            "The running loss at 6106 iteration is: 0.12319617453956778\n",
            "The running loss at 6107 iteration is: 0.12282075003133039\n",
            "The running loss at 6108 iteration is: 0.12242742266188084\n",
            "The running loss at 6109 iteration is: 0.12206551931210247\n",
            "The running loss at 6110 iteration is: 0.12162335904145026\n",
            "The running loss at 6111 iteration is: 0.12124664567462572\n",
            "The running loss at 6112 iteration is: 0.12086258988516105\n",
            "The running loss at 6113 iteration is: 0.12050348498041921\n",
            "The running loss at 6114 iteration is: 0.12011543984510743\n",
            "The running loss at 6115 iteration is: 0.1197256228860944\n",
            "The running loss at 6116 iteration is: 0.11936007261959387\n",
            "The running loss at 6117 iteration is: 0.11897653787311356\n",
            "The running loss at 6118 iteration is: 0.11862604188462726\n",
            "The running loss at 6119 iteration is: 0.11823094046155093\n",
            "The running loss at 6120 iteration is: 0.1178972268460959\n",
            "The running loss at 6121 iteration is: 0.11751979788423673\n",
            "The running loss at 6122 iteration is: 0.11715786402367732\n",
            "The running loss at 6123 iteration is: 0.11678896333458375\n",
            "The running loss at 6124 iteration is: 0.11640935831481801\n",
            "The running loss at 6125 iteration is: 0.11609084267879215\n",
            "The running loss at 6126 iteration is: 0.11571477380158064\n",
            "The running loss at 6127 iteration is: 0.11533400983001446\n",
            "The running loss at 6128 iteration is: 0.11496601973163345\n",
            "The running loss at 6129 iteration is: 0.11465784721810429\n",
            "The running loss at 6130 iteration is: 0.11427209981205232\n",
            "The running loss at 6131 iteration is: 0.11392598287723077\n",
            "The running loss at 6132 iteration is: 0.11359938864599865\n",
            "The running loss at 6133 iteration is: 0.11323208548534712\n",
            "The running loss at 6134 iteration is: 0.11290695184611918\n",
            "The running loss at 6135 iteration is: 0.11254310168806109\n",
            "The running loss at 6136 iteration is: 0.1121760297985687\n",
            "The running loss at 6137 iteration is: 0.11186437000119367\n",
            "The running loss at 6138 iteration is: 0.11150015473981928\n",
            "The running loss at 6139 iteration is: 0.11118699290298723\n",
            "The running loss at 6140 iteration is: 0.11086292544239396\n",
            "The running loss at 6141 iteration is: 0.11050324397293056\n",
            "The running loss at 6142 iteration is: 0.11020153243532364\n",
            "The running loss at 6143 iteration is: 0.10983867393638273\n",
            "The running loss at 6144 iteration is: 0.10954549507689879\n",
            "The running loss at 6145 iteration is: 0.10918703579680897\n",
            "The running loss at 6146 iteration is: 0.10889131930848002\n",
            "The running loss at 6147 iteration is: 0.10863527098513852\n",
            "The running loss at 6148 iteration is: 0.10837565566275964\n",
            "The running loss at 6149 iteration is: 0.1081693565663278\n",
            "The running loss at 6150 iteration is: 0.10808655693988653\n",
            "The running loss at 6151 iteration is: 0.10825077705026495\n",
            "The running loss at 6152 iteration is: 0.10884881362003648\n",
            "The running loss at 6153 iteration is: 0.11038577483419616\n",
            "The running loss at 6154 iteration is: 0.1137894842155657\n",
            "The running loss at 6155 iteration is: 0.12134955369113022\n",
            "The running loss at 6156 iteration is: 0.13756163630382212\n",
            "The running loss at 6157 iteration is: 0.17305638262026382\n",
            "The running loss at 6158 iteration is: 0.24966008109267132\n",
            "The running loss at 6159 iteration is: 0.4248847070094919\n",
            "The running loss at 6160 iteration is: 0.8054564303492283\n",
            "The running loss at 6161 iteration is: 1.7235813783181524\n",
            "The running loss at 6162 iteration is: 3.6452614953894464\n",
            "The running loss at 6163 iteration is: 8.523372368298356\n",
            "The running loss at 6164 iteration is: 16.84906405224704\n",
            "The running loss at 6165 iteration is: 37.19971400131524\n",
            "The running loss at 6166 iteration is: 46.70398636304404\n",
            "The running loss at 6167 iteration is: 57.4131327046236\n",
            "The running loss at 6168 iteration is: 14.770355177006511\n",
            "The running loss at 6169 iteration is: 1.7126928283674552\n",
            "The running loss at 6170 iteration is: 23.001319355385174\n",
            "The running loss at 6171 iteration is: 14.59729676847498\n",
            "The running loss at 6172 iteration is: 0.3478479646994668\n",
            "The running loss at 6173 iteration is: 9.211510901106763\n",
            "The running loss at 6174 iteration is: 12.951755414585017\n",
            "The running loss at 6175 iteration is: 3.8015496858215894\n",
            "The running loss at 6176 iteration is: 1.2903046649696617\n",
            "The running loss at 6177 iteration is: 9.339786773914325\n",
            "The running loss at 6178 iteration is: 11.456595992822328\n",
            "The running loss at 6179 iteration is: 2.659800447007516\n",
            "The running loss at 6180 iteration is: 1.1265441497711055\n",
            "The running loss at 6181 iteration is: 7.1830032560898465\n",
            "The running loss at 6182 iteration is: 7.936993691292922\n",
            "The running loss at 6183 iteration is: 3.5547364683537808\n",
            "The running loss at 6184 iteration is: 0.36005044961466365\n",
            "The running loss at 6185 iteration is: 3.0090835594926393\n",
            "The running loss at 6186 iteration is: 5.956151912485194\n",
            "The running loss at 6187 iteration is: 3.268568349993382\n",
            "The running loss at 6188 iteration is: 0.4532019431042988\n",
            "The running loss at 6189 iteration is: 1.5131273645199792\n",
            "The running loss at 6190 iteration is: 3.32683287444027\n",
            "The running loss at 6191 iteration is: 2.3820685222857283\n",
            "The running loss at 6192 iteration is: 0.42063594292679657\n",
            "The running loss at 6193 iteration is: 1.1497435931684297\n",
            "The running loss at 6194 iteration is: 2.428473193755635\n",
            "The running loss at 6195 iteration is: 1.2936602946860476\n",
            "The running loss at 6196 iteration is: 0.31993438664618934\n",
            "The running loss at 6197 iteration is: 1.154782221138902\n",
            "The running loss at 6198 iteration is: 1.4936770963122084\n",
            "The running loss at 6199 iteration is: 0.6052916827608601\n",
            "The running loss at 6200 iteration is: 0.4046557134246847\n",
            "The running loss at 6201 iteration is: 1.0623920599581451\n",
            "The running loss at 6202 iteration is: 0.9110601873616625\n",
            "The running loss at 6203 iteration is: 0.31307984547540973\n",
            "The running loss at 6204 iteration is: 0.5918753973711898\n",
            "The running loss at 6205 iteration is: 0.8845250262935147\n",
            "The running loss at 6206 iteration is: 0.4434311498834687\n",
            "The running loss at 6207 iteration is: 0.33693430055462004\n",
            "The running loss at 6208 iteration is: 0.6676466469754689\n",
            "The running loss at 6209 iteration is: 0.5310865880559938\n",
            "The running loss at 6210 iteration is: 0.27801267025150267\n",
            "The running loss at 6211 iteration is: 0.45154532374558926\n",
            "The running loss at 6212 iteration is: 0.5225643369416734\n",
            "The running loss at 6213 iteration is: 0.3108663526992739\n",
            "The running loss at 6214 iteration is: 0.31860362043293006\n",
            "The running loss at 6215 iteration is: 0.45365401998608307\n",
            "The running loss at 6216 iteration is: 0.3524968515387651\n",
            "The running loss at 6217 iteration is: 0.2668923357870056\n",
            "The running loss at 6218 iteration is: 0.3703735224601242\n",
            "The running loss at 6219 iteration is: 0.36571063131974213\n",
            "The running loss at 6220 iteration is: 0.2633969856784688\n",
            "The running loss at 6221 iteration is: 0.30403649281365086\n",
            "The running loss at 6222 iteration is: 0.3491502508594598\n",
            "The running loss at 6223 iteration is: 0.2758582779731497\n",
            "The running loss at 6224 iteration is: 0.2647182241139495\n",
            "The running loss at 6225 iteration is: 0.3164335063570495\n",
            "The running loss at 6226 iteration is: 0.2849188043915236\n",
            "The running loss at 6227 iteration is: 0.24900294270371073\n",
            "The running loss at 6228 iteration is: 0.28305660865010845\n",
            "The running loss at 6229 iteration is: 0.284032503951111\n",
            "The running loss at 6230 iteration is: 0.2472850922578009\n",
            "The running loss at 6231 iteration is: 0.2574604515973574\n",
            "The running loss at 6232 iteration is: 0.2740511410071106\n",
            "The running loss at 6233 iteration is: 0.2500665419252303\n",
            "The running loss at 6234 iteration is: 0.24270103760694406\n",
            "The running loss at 6235 iteration is: 0.25994585218758803\n",
            "The running loss at 6236 iteration is: 0.2510978653826348\n",
            "The running loss at 6237 iteration is: 0.23627428949131182\n",
            "The running loss at 6238 iteration is: 0.24626443303381135\n",
            "The running loss at 6239 iteration is: 0.24823885119984634\n",
            "The running loss at 6240 iteration is: 0.2344801585381796\n",
            "The running loss at 6241 iteration is: 0.23577451816742823\n",
            "The running loss at 6242 iteration is: 0.24206691926732357\n",
            "The running loss at 6243 iteration is: 0.23370569129843832\n",
            "The running loss at 6244 iteration is: 0.22912805492459898\n",
            "The running loss at 6245 iteration is: 0.23462499171081316\n",
            "The running loss at 6246 iteration is: 0.23203194885798648\n",
            "The running loss at 6247 iteration is: 0.2255237632662558\n",
            "The running loss at 6248 iteration is: 0.2276650368969271\n",
            "The running loss at 6249 iteration is: 0.22873576071790658\n",
            "The running loss at 6250 iteration is: 0.22346579606087957\n",
            "The running loss at 6251 iteration is: 0.22221532246915549\n",
            "The running loss at 6252 iteration is: 0.22418544859739625\n",
            "The running loss at 6253 iteration is: 0.22140769446267827\n",
            "The running loss at 6254 iteration is: 0.21847207579862601\n",
            "The running loss at 6255 iteration is: 0.21957004472903524\n",
            "The running loss at 6256 iteration is: 0.21894407648268263\n",
            "The running loss at 6257 iteration is: 0.215837968643367\n",
            "The running loss at 6258 iteration is: 0.2153148597369604\n",
            "The running loss at 6259 iteration is: 0.2156848147420046\n",
            "The running loss at 6260 iteration is: 0.213560456115455\n",
            "The running loss at 6261 iteration is: 0.21193185067657005\n",
            "The running loss at 6262 iteration is: 0.2119982218967692\n",
            "The running loss at 6263 iteration is: 0.21106848707425532\n",
            "The running loss at 6264 iteration is: 0.20920506713677878\n",
            "The running loss at 6265 iteration is: 0.20861009863671115\n",
            "The running loss at 6266 iteration is: 0.20829250605994803\n",
            "The running loss at 6267 iteration is: 0.2068928113598063\n",
            "The running loss at 6268 iteration is: 0.20561051553241497\n",
            "The running loss at 6269 iteration is: 0.20529249735959731\n",
            "The running loss at 6270 iteration is: 0.2044004662570112\n",
            "The running loss at 6271 iteration is: 0.20307057363360892\n",
            "The running loss at 6272 iteration is: 0.20233964263700774\n",
            "The running loss at 6273 iteration is: 0.20178569315571213\n",
            "The running loss at 6274 iteration is: 0.20069060074894343\n",
            "The running loss at 6275 iteration is: 0.19971502976432567\n",
            "The running loss at 6276 iteration is: 0.19905969445108798\n",
            "The running loss at 6277 iteration is: 0.1982716583163904\n",
            "The running loss at 6278 iteration is: 0.1972621908471184\n",
            "The running loss at 6279 iteration is: 0.19645452323210838\n",
            "The running loss at 6280 iteration is: 0.19581576081362723\n",
            "The running loss at 6281 iteration is: 0.1948874144270662\n",
            "The running loss at 6282 iteration is: 0.19397177685402078\n",
            "The running loss at 6283 iteration is: 0.19329605451960463\n",
            "The running loss at 6284 iteration is: 0.19256099851241987\n",
            "The running loss at 6285 iteration is: 0.19165331086616125\n",
            "The running loss at 6286 iteration is: 0.19090157257806287\n",
            "The running loss at 6287 iteration is: 0.1901676291288761\n",
            "The running loss at 6288 iteration is: 0.18936163733691358\n",
            "The running loss at 6289 iteration is: 0.18855124924748828\n",
            "The running loss at 6290 iteration is: 0.18783320352430535\n",
            "The running loss at 6291 iteration is: 0.18709099923040076\n",
            "The running loss at 6292 iteration is: 0.18635734689097685\n",
            "The running loss at 6293 iteration is: 0.1855439194244286\n",
            "The running loss at 6294 iteration is: 0.1848596144423671\n",
            "The running loss at 6295 iteration is: 0.18416672114099245\n",
            "The running loss at 6296 iteration is: 0.18338227977932206\n",
            "The running loss at 6297 iteration is: 0.18260337838135943\n",
            "The running loss at 6298 iteration is: 0.18196255823976992\n",
            "The running loss at 6299 iteration is: 0.18122456134892803\n",
            "The running loss at 6300 iteration is: 0.1804960720245809\n",
            "The running loss at 6301 iteration is: 0.17976305891319147\n",
            "The running loss at 6302 iteration is: 0.1790740818623157\n",
            "The running loss at 6303 iteration is: 0.17840803628896873\n",
            "The running loss at 6304 iteration is: 0.17770422503224312\n",
            "The running loss at 6305 iteration is: 0.1769855606291059\n",
            "The running loss at 6306 iteration is: 0.1763122867800562\n",
            "The running loss at 6307 iteration is: 0.17560170744676115\n",
            "The running loss at 6308 iteration is: 0.17494380214718652\n",
            "The running loss at 6309 iteration is: 0.17426197297287785\n",
            "The running loss at 6310 iteration is: 0.17358600693576248\n",
            "The running loss at 6311 iteration is: 0.17293002490711892\n",
            "The running loss at 6312 iteration is: 0.17228081902293646\n",
            "The running loss at 6313 iteration is: 0.17160622692276595\n",
            "The running loss at 6314 iteration is: 0.17096015631560904\n",
            "The running loss at 6315 iteration is: 0.17034031184500936\n",
            "The running loss at 6316 iteration is: 0.16965167183979946\n",
            "The running loss at 6317 iteration is: 0.16899396652104856\n",
            "The running loss at 6318 iteration is: 0.16836916719665837\n",
            "The running loss at 6319 iteration is: 0.16773061366266193\n",
            "The running loss at 6320 iteration is: 0.16710930943521055\n",
            "The running loss at 6321 iteration is: 0.16650771889305083\n",
            "The running loss at 6322 iteration is: 0.16582694358999955\n",
            "The running loss at 6323 iteration is: 0.1652247276138881\n",
            "The running loss at 6324 iteration is: 0.16460685820168589\n",
            "The running loss at 6325 iteration is: 0.1640194341340343\n",
            "The running loss at 6326 iteration is: 0.16335903451598346\n",
            "The running loss at 6327 iteration is: 0.1627303157309383\n",
            "The running loss at 6328 iteration is: 0.16212919468681575\n",
            "The running loss at 6329 iteration is: 0.1615600102954678\n",
            "The running loss at 6330 iteration is: 0.16094053819459214\n",
            "The running loss at 6331 iteration is: 0.16032857179009202\n",
            "The running loss at 6332 iteration is: 0.15974137187186\n",
            "The running loss at 6333 iteration is: 0.15916828090911597\n",
            "The running loss at 6334 iteration is: 0.15860697825170142\n",
            "The running loss at 6335 iteration is: 0.15800237305924572\n",
            "The running loss at 6336 iteration is: 0.15739219362622686\n",
            "The running loss at 6337 iteration is: 0.1568468021236916\n",
            "The running loss at 6338 iteration is: 0.15628285215573015\n",
            "The running loss at 6339 iteration is: 0.15573189301911322\n",
            "The running loss at 6340 iteration is: 0.15514068527186467\n",
            "The running loss at 6341 iteration is: 0.15458666770436102\n",
            "The running loss at 6342 iteration is: 0.15400389459313663\n",
            "The running loss at 6343 iteration is: 0.15347865161352534\n",
            "The running loss at 6344 iteration is: 0.15289369714852946\n",
            "The running loss at 6345 iteration is: 0.1523449450144071\n",
            "The running loss at 6346 iteration is: 0.15178885247004878\n",
            "The running loss at 6347 iteration is: 0.15122297847985644\n",
            "The running loss at 6348 iteration is: 0.15068995678022434\n",
            "The running loss at 6349 iteration is: 0.15016797702670337\n",
            "The running loss at 6350 iteration is: 0.14962046649424002\n",
            "The running loss at 6351 iteration is: 0.14910165055176947\n",
            "The running loss at 6352 iteration is: 0.14857874207810196\n",
            "The running loss at 6353 iteration is: 0.14804569206808502\n",
            "The running loss at 6354 iteration is: 0.1475175281614771\n",
            "The running loss at 6355 iteration is: 0.14697291071136415\n",
            "The running loss at 6356 iteration is: 0.14645399921604316\n",
            "The running loss at 6357 iteration is: 0.14594561774934203\n",
            "The running loss at 6358 iteration is: 0.14539488586165264\n",
            "The running loss at 6359 iteration is: 0.1448909627394475\n",
            "The running loss at 6360 iteration is: 0.14442866424914816\n",
            "The running loss at 6361 iteration is: 0.14388254400486022\n",
            "The running loss at 6362 iteration is: 0.1433831943647589\n",
            "The running loss at 6363 iteration is: 0.14285236840930526\n",
            "The running loss at 6364 iteration is: 0.14240218880317287\n",
            "The running loss at 6365 iteration is: 0.14188087568470462\n",
            "The running loss at 6366 iteration is: 0.14140154100591348\n",
            "The running loss at 6367 iteration is: 0.14089667890130303\n",
            "The running loss at 6368 iteration is: 0.14042743571636607\n",
            "The running loss at 6369 iteration is: 0.13992923059869097\n",
            "The running loss at 6370 iteration is: 0.13944886088701472\n",
            "The running loss at 6371 iteration is: 0.13891047072114382\n",
            "The running loss at 6372 iteration is: 0.13846683597087653\n",
            "The running loss at 6373 iteration is: 0.1380033105163299\n",
            "The running loss at 6374 iteration is: 0.13751805774346504\n",
            "The running loss at 6375 iteration is: 0.1370372138247905\n",
            "The running loss at 6376 iteration is: 0.13658134679715414\n",
            "The running loss at 6377 iteration is: 0.1361260398606343\n",
            "The running loss at 6378 iteration is: 0.13562414844043819\n",
            "The running loss at 6379 iteration is: 0.13514143228548584\n",
            "The running loss at 6380 iteration is: 0.13470030751410117\n",
            "The running loss at 6381 iteration is: 0.13424229584544087\n",
            "The running loss at 6382 iteration is: 0.13378656118187984\n",
            "The running loss at 6383 iteration is: 0.1333359217161734\n",
            "The running loss at 6384 iteration is: 0.13288983818303202\n",
            "The running loss at 6385 iteration is: 0.1324293484696339\n",
            "The running loss at 6386 iteration is: 0.13195646441774866\n",
            "The running loss at 6387 iteration is: 0.13152645990389766\n",
            "The running loss at 6388 iteration is: 0.13108027439689945\n",
            "The running loss at 6389 iteration is: 0.13066091043151778\n",
            "The running loss at 6390 iteration is: 0.13023453977207275\n",
            "The running loss at 6391 iteration is: 0.12977759040156384\n",
            "The running loss at 6392 iteration is: 0.1293258445756389\n",
            "The running loss at 6393 iteration is: 0.12894289740840598\n",
            "The running loss at 6394 iteration is: 0.12847239591360457\n",
            "The running loss at 6395 iteration is: 0.12803007609076303\n",
            "The running loss at 6396 iteration is: 0.1276464305703267\n",
            "The running loss at 6397 iteration is: 0.12722153785409188\n",
            "The running loss at 6398 iteration is: 0.12677232187567583\n",
            "The running loss at 6399 iteration is: 0.1263757529367064\n",
            "The running loss at 6400 iteration is: 0.12590749878578428\n",
            "The running loss at 6401 iteration is: 0.1255186670522617\n",
            "The running loss at 6402 iteration is: 0.12508157513651186\n",
            "The running loss at 6403 iteration is: 0.12468419675399302\n",
            "The running loss at 6404 iteration is: 0.12426300196828062\n",
            "The running loss at 6405 iteration is: 0.1238613434783381\n",
            "The running loss at 6406 iteration is: 0.12347881155767747\n",
            "The running loss at 6407 iteration is: 0.12303338100889583\n",
            "The running loss at 6408 iteration is: 0.1226470603884092\n",
            "The running loss at 6409 iteration is: 0.12226514784601462\n",
            "The running loss at 6410 iteration is: 0.12182944424893376\n",
            "The running loss at 6411 iteration is: 0.12144451833588077\n",
            "The running loss at 6412 iteration is: 0.12107089175617251\n",
            "The running loss at 6413 iteration is: 0.12067090879748613\n",
            "The running loss at 6414 iteration is: 0.12025245099100972\n",
            "The running loss at 6415 iteration is: 0.11991101956897905\n",
            "The running loss at 6416 iteration is: 0.1194885502894126\n",
            "The running loss at 6417 iteration is: 0.11910030078168934\n",
            "The running loss at 6418 iteration is: 0.11875960695605922\n",
            "The running loss at 6419 iteration is: 0.11838197432410663\n",
            "The running loss at 6420 iteration is: 0.11803381854633584\n",
            "The running loss at 6421 iteration is: 0.1176753313916124\n",
            "The running loss at 6422 iteration is: 0.11734574414765553\n",
            "The running loss at 6423 iteration is: 0.11702569097574199\n",
            "The running loss at 6424 iteration is: 0.11678532097535808\n",
            "The running loss at 6425 iteration is: 0.1166544585549866\n",
            "The running loss at 6426 iteration is: 0.11669986729331201\n",
            "The running loss at 6427 iteration is: 0.11709706450654864\n",
            "The running loss at 6428 iteration is: 0.11822895907950404\n",
            "The running loss at 6429 iteration is: 0.12065305678811962\n",
            "The running loss at 6430 iteration is: 0.12574928743427669\n",
            "The running loss at 6431 iteration is: 0.1360290439320873\n",
            "The running loss at 6432 iteration is: 0.15734327427726655\n",
            "The running loss at 6433 iteration is: 0.2005717616093138\n",
            "The running loss at 6434 iteration is: 0.2930748458728561\n",
            "The running loss at 6435 iteration is: 0.48336717042245725\n",
            "The running loss at 6436 iteration is: 0.9118837584826162\n",
            "The running loss at 6437 iteration is: 1.785519542837888\n",
            "The running loss at 6438 iteration is: 3.868059593618063\n",
            "The running loss at 6439 iteration is: 7.75044125905291\n",
            "The running loss at 6440 iteration is: 17.385103876491822\n",
            "The running loss at 6441 iteration is: 29.270891723258067\n",
            "The running loss at 6442 iteration is: 55.153768091630155\n",
            "The running loss at 6443 iteration is: 45.664054572601586\n",
            "The running loss at 6444 iteration is: 29.410628860218544\n",
            "The running loss at 6445 iteration is: 1.3509119183505862\n",
            "The running loss at 6446 iteration is: 11.96005182106997\n",
            "The running loss at 6447 iteration is: 31.586407306765697\n",
            "The running loss at 6448 iteration is: 9.793470072052395\n",
            "The running loss at 6449 iteration is: 1.2699962939493195\n",
            "The running loss at 6450 iteration is: 16.60395324466064\n",
            "The running loss at 6451 iteration is: 15.842623332569842\n",
            "The running loss at 6452 iteration is: 4.349609338305107\n",
            "The running loss at 6453 iteration is: 1.1353535271214903\n",
            "The running loss at 6454 iteration is: 10.063901520928354\n",
            "The running loss at 6455 iteration is: 15.173396395064128\n",
            "The running loss at 6456 iteration is: 5.439009413721758\n",
            "The running loss at 6457 iteration is: 0.34509394830107837\n",
            "The running loss at 6458 iteration is: 5.151171638943124\n",
            "The running loss at 6459 iteration is: 8.602105459988572\n",
            "The running loss at 6460 iteration is: 5.655617167362858\n",
            "The running loss at 6461 iteration is: 0.5698385383330249\n",
            "The running loss at 6462 iteration is: 2.274065051292314\n",
            "The running loss at 6463 iteration is: 6.0414750365935275\n",
            "The running loss at 6464 iteration is: 3.5825331301458663\n",
            "The running loss at 6465 iteration is: 0.41974580451896987\n",
            "The running loss at 6466 iteration is: 1.653813804579304\n",
            "The running loss at 6467 iteration is: 3.477268050121446\n",
            "The running loss at 6468 iteration is: 2.0595255535624184\n",
            "The running loss at 6469 iteration is: 0.28942203214999784\n",
            "The running loss at 6470 iteration is: 1.6031258808231585\n",
            "The running loss at 6471 iteration is: 2.534566322293275\n",
            "The running loss at 6472 iteration is: 0.857655524313073\n",
            "The running loss at 6473 iteration is: 0.43557575839367607\n",
            "The running loss at 6474 iteration is: 1.6492785018674772\n",
            "The running loss at 6475 iteration is: 1.3142639738220523\n",
            "The running loss at 6476 iteration is: 0.3053636373629357\n",
            "The running loss at 6477 iteration is: 0.7087413597745579\n",
            "The running loss at 6478 iteration is: 1.2090898269422043\n",
            "The running loss at 6479 iteration is: 0.5984983848050339\n",
            "The running loss at 6480 iteration is: 0.28792123897074035\n",
            "The running loss at 6481 iteration is: 0.8155506411557139\n",
            "The running loss at 6482 iteration is: 0.7896664868027824\n",
            "The running loss at 6483 iteration is: 0.27823321987748667\n",
            "The running loss at 6484 iteration is: 0.448282348849056\n",
            "The running loss at 6485 iteration is: 0.7302069869061011\n",
            "The running loss at 6486 iteration is: 0.40312144783362835\n",
            "The running loss at 6487 iteration is: 0.26113005462248795\n",
            "The running loss at 6488 iteration is: 0.5278659160108232\n",
            "The running loss at 6489 iteration is: 0.47832275723637296\n",
            "The running loss at 6490 iteration is: 0.24401116958780503\n",
            "The running loss at 6491 iteration is: 0.3365243664897095\n",
            "The running loss at 6492 iteration is: 0.4521785083956339\n",
            "The running loss at 6493 iteration is: 0.3026963957978168\n",
            "The running loss at 6494 iteration is: 0.2389023282295463\n",
            "The running loss at 6495 iteration is: 0.36214985637227737\n",
            "The running loss at 6496 iteration is: 0.3445621530247872\n",
            "The running loss at 6497 iteration is: 0.22958698778078787\n",
            "The running loss at 6498 iteration is: 0.27241708127785536\n",
            "The running loss at 6499 iteration is: 0.33251897804739894\n",
            "The running loss at 6500 iteration is: 0.2576158350483088\n",
            "The running loss at 6501 iteration is: 0.22418196109327126\n",
            "The running loss at 6502 iteration is: 0.28393803613803403\n",
            "The running loss at 6503 iteration is: 0.2758054017847166\n",
            "The running loss at 6504 iteration is: 0.21980085974056432\n",
            "The running loss at 6505 iteration is: 0.23601571330561907\n",
            "The running loss at 6506 iteration is: 0.2663292332980479\n",
            "The running loss at 6507 iteration is: 0.2343985996574507\n",
            "The running loss at 6508 iteration is: 0.21281447524342592\n",
            "The running loss at 6509 iteration is: 0.2391037268384892\n",
            "The running loss at 6510 iteration is: 0.24159418158648177\n",
            "The running loss at 6511 iteration is: 0.21311773370928333\n",
            "The running loss at 6512 iteration is: 0.2148006394284669\n",
            "The running loss at 6513 iteration is: 0.23212205526882423\n",
            "The running loss at 6514 iteration is: 0.2202556773534859\n",
            "The running loss at 6515 iteration is: 0.20525757914476328\n",
            "The running loss at 6516 iteration is: 0.2148258088109537\n",
            "The running loss at 6517 iteration is: 0.21988970259617516\n",
            "The running loss at 6518 iteration is: 0.2069049926036469\n",
            "The running loss at 6519 iteration is: 0.20283597615223184\n",
            "The running loss at 6520 iteration is: 0.21119547121530788\n",
            "The running loss at 6521 iteration is: 0.2091388963544716\n",
            "The running loss at 6522 iteration is: 0.19979494039068602\n",
            "The running loss at 6523 iteration is: 0.20092166147484572\n",
            "The running loss at 6524 iteration is: 0.2055246561819515\n",
            "The running loss at 6525 iteration is: 0.20080803034291136\n",
            "The running loss at 6526 iteration is: 0.1956104258380886\n",
            "The running loss at 6527 iteration is: 0.1981174635483575\n",
            "The running loss at 6528 iteration is: 0.19941398470389518\n",
            "The running loss at 6529 iteration is: 0.1948374988369885\n",
            "The running loss at 6530 iteration is: 0.19246860723233197\n",
            "The running loss at 6531 iteration is: 0.19456543022521872\n",
            "The running loss at 6532 iteration is: 0.19397016538651946\n",
            "The running loss at 6533 iteration is: 0.19049121511054828\n",
            "The running loss at 6534 iteration is: 0.18959216860808448\n",
            "The running loss at 6535 iteration is: 0.19076417461848072\n",
            "The running loss at 6536 iteration is: 0.18949345049894137\n",
            "The running loss at 6537 iteration is: 0.18689608680536082\n",
            "The running loss at 6538 iteration is: 0.18657281103760084\n",
            "The running loss at 6539 iteration is: 0.1870408060751199\n",
            "The running loss at 6540 iteration is: 0.1856093618468521\n",
            "The running loss at 6541 iteration is: 0.18379328347395882\n",
            "The running loss at 6542 iteration is: 0.18351680957112368\n",
            "The running loss at 6543 iteration is: 0.18341966489621073\n",
            "The running loss at 6544 iteration is: 0.1821457591640092\n",
            "The running loss at 6545 iteration is: 0.1807744076144361\n",
            "The running loss at 6546 iteration is: 0.18044649945641314\n",
            "The running loss at 6547 iteration is: 0.18013328638800993\n",
            "The running loss at 6548 iteration is: 0.17896060757341775\n",
            "The running loss at 6549 iteration is: 0.17789685382835943\n",
            "The running loss at 6550 iteration is: 0.17746323804766376\n",
            "The running loss at 6551 iteration is: 0.1770262912641014\n",
            "The running loss at 6552 iteration is: 0.17601143020541918\n",
            "The running loss at 6553 iteration is: 0.17507101939069986\n",
            "The running loss at 6554 iteration is: 0.17452470236209364\n",
            "The running loss at 6555 iteration is: 0.17404123192379906\n",
            "The running loss at 6556 iteration is: 0.17319943866144738\n",
            "The running loss at 6557 iteration is: 0.1722816977840576\n",
            "The running loss at 6558 iteration is: 0.17173179676741265\n",
            "The running loss at 6559 iteration is: 0.17114637993600978\n",
            "The running loss at 6560 iteration is: 0.17042243254541847\n",
            "The running loss at 6561 iteration is: 0.16962417391391568\n",
            "The running loss at 6562 iteration is: 0.1689789291725838\n",
            "The running loss at 6563 iteration is: 0.1684420690666655\n",
            "The running loss at 6564 iteration is: 0.16770738749617395\n",
            "The running loss at 6565 iteration is: 0.16696966707814745\n",
            "The running loss at 6566 iteration is: 0.1662964685915747\n",
            "The running loss at 6567 iteration is: 0.1657785399338731\n",
            "The running loss at 6568 iteration is: 0.165139323402316\n",
            "The running loss at 6569 iteration is: 0.16446744701835914\n",
            "The running loss at 6570 iteration is: 0.16376169275562022\n",
            "The running loss at 6571 iteration is: 0.1631269623784205\n",
            "The running loss at 6572 iteration is: 0.16255399850305324\n",
            "The running loss at 6573 iteration is: 0.16192621141200536\n",
            "The running loss at 6574 iteration is: 0.16130826894722675\n",
            "The running loss at 6575 iteration is: 0.1606859938570431\n",
            "The running loss at 6576 iteration is: 0.16008175503482713\n",
            "The running loss at 6577 iteration is: 0.15944385331776012\n",
            "The running loss at 6578 iteration is: 0.15885634007823005\n",
            "The running loss at 6579 iteration is: 0.1582503081024037\n",
            "The running loss at 6580 iteration is: 0.1576328149917046\n",
            "The running loss at 6581 iteration is: 0.15705208451426542\n",
            "The running loss at 6582 iteration is: 0.15644888575565005\n",
            "The running loss at 6583 iteration is: 0.15582535073170864\n",
            "The running loss at 6584 iteration is: 0.155285339281961\n",
            "The running loss at 6585 iteration is: 0.1546812841315741\n",
            "The running loss at 6586 iteration is: 0.1540829974868384\n",
            "The running loss at 6587 iteration is: 0.15354720103969866\n",
            "The running loss at 6588 iteration is: 0.15293975093739304\n",
            "The running loss at 6589 iteration is: 0.15238269373262345\n",
            "The running loss at 6590 iteration is: 0.1518271557263293\n",
            "The running loss at 6591 iteration is: 0.15126709511278724\n",
            "The running loss at 6592 iteration is: 0.15067729747902092\n",
            "The running loss at 6593 iteration is: 0.15014192655773378\n",
            "The running loss at 6594 iteration is: 0.1495911634775072\n",
            "The running loss at 6595 iteration is: 0.14902572108831227\n",
            "The running loss at 6596 iteration is: 0.1484972051102168\n",
            "The running loss at 6597 iteration is: 0.147932826837285\n",
            "The running loss at 6598 iteration is: 0.1474113143815321\n",
            "The running loss at 6599 iteration is: 0.14687282251706935\n",
            "The running loss at 6600 iteration is: 0.1463353783687278\n",
            "The running loss at 6601 iteration is: 0.14582074633061948\n",
            "The running loss at 6602 iteration is: 0.14527078231288557\n",
            "The running loss at 6603 iteration is: 0.14475412592992648\n",
            "The running loss at 6604 iteration is: 0.14422343114903416\n",
            "The running loss at 6605 iteration is: 0.14369315120784162\n",
            "The running loss at 6606 iteration is: 0.14315100080226303\n",
            "The running loss at 6607 iteration is: 0.14264161188449095\n",
            "The running loss at 6608 iteration is: 0.14211470070002102\n",
            "The running loss at 6609 iteration is: 0.14164065195869302\n",
            "The running loss at 6610 iteration is: 0.14109689460088998\n",
            "The running loss at 6611 iteration is: 0.14058325078223144\n",
            "The running loss at 6612 iteration is: 0.1400838883348093\n",
            "The running loss at 6613 iteration is: 0.1395762949024903\n",
            "The running loss at 6614 iteration is: 0.13906140274340154\n",
            "The running loss at 6615 iteration is: 0.1385813811593445\n",
            "The running loss at 6616 iteration is: 0.1380668056418212\n",
            "The running loss at 6617 iteration is: 0.13759159290047704\n",
            "The running loss at 6618 iteration is: 0.13714075282478744\n",
            "The running loss at 6619 iteration is: 0.13661762333503955\n",
            "The running loss at 6620 iteration is: 0.13616387832878554\n",
            "The running loss at 6621 iteration is: 0.13569932666575069\n",
            "The running loss at 6622 iteration is: 0.1351976583620557\n",
            "The running loss at 6623 iteration is: 0.13470812595940895\n",
            "The running loss at 6624 iteration is: 0.13424659382883886\n",
            "The running loss at 6625 iteration is: 0.13378658194545376\n",
            "The running loss at 6626 iteration is: 0.13326503363801906\n",
            "The running loss at 6627 iteration is: 0.13281284522068595\n",
            "The running loss at 6628 iteration is: 0.1323503420034739\n",
            "The running loss at 6629 iteration is: 0.13190774353033086\n",
            "The running loss at 6630 iteration is: 0.13144707352903973\n",
            "The running loss at 6631 iteration is: 0.13097699196237544\n",
            "The running loss at 6632 iteration is: 0.13051496149862712\n",
            "The running loss at 6633 iteration is: 0.1300805603364456\n",
            "The running loss at 6634 iteration is: 0.1296255649323125\n",
            "The running loss at 6635 iteration is: 0.12916314557319136\n",
            "The running loss at 6636 iteration is: 0.128729509050687\n",
            "The running loss at 6637 iteration is: 0.12831202207089573\n",
            "The running loss at 6638 iteration is: 0.1278481003426933\n",
            "The running loss at 6639 iteration is: 0.1273961107424753\n",
            "The running loss at 6640 iteration is: 0.12697601913525533\n",
            "The running loss at 6641 iteration is: 0.12655869521664043\n",
            "The running loss at 6642 iteration is: 0.12611832510928303\n",
            "The running loss at 6643 iteration is: 0.12566182415043284\n",
            "The running loss at 6644 iteration is: 0.12526358722542594\n",
            "The running loss at 6645 iteration is: 0.12480367587790181\n",
            "The running loss at 6646 iteration is: 0.1244229863796551\n",
            "The running loss at 6647 iteration is: 0.1239919203307437\n",
            "The running loss at 6648 iteration is: 0.12359717090271786\n",
            "The running loss at 6649 iteration is: 0.12315272108623362\n",
            "The running loss at 6650 iteration is: 0.12276499194209213\n",
            "The running loss at 6651 iteration is: 0.12231688319423674\n",
            "The running loss at 6652 iteration is: 0.12193697082098372\n",
            "The running loss at 6653 iteration is: 0.12153936617476799\n",
            "The running loss at 6654 iteration is: 0.12115337043384126\n",
            "The running loss at 6655 iteration is: 0.12078190274816167\n",
            "The running loss at 6656 iteration is: 0.12040507322637312\n",
            "The running loss at 6657 iteration is: 0.12012274393218406\n",
            "The running loss at 6658 iteration is: 0.11985436931743139\n",
            "The running loss at 6659 iteration is: 0.11967086664409984\n",
            "The running loss at 6660 iteration is: 0.11970595047824176\n",
            "The running loss at 6661 iteration is: 0.12005802948852971\n",
            "The running loss at 6662 iteration is: 0.1210056260997777\n",
            "The running loss at 6663 iteration is: 0.1230806114222157\n",
            "The running loss at 6664 iteration is: 0.12731071107105926\n",
            "The running loss at 6665 iteration is: 0.13554152798734814\n",
            "The running loss at 6666 iteration is: 0.15212324278693753\n",
            "The running loss at 6667 iteration is: 0.18464669189565464\n",
            "The running loss at 6668 iteration is: 0.25159124172604075\n",
            "The running loss at 6669 iteration is: 0.38468641913510104\n",
            "The running loss at 6670 iteration is: 0.6724379192813398\n",
            "The running loss at 6671 iteration is: 1.2439250849100199\n",
            "The running loss at 6672 iteration is: 2.553360575502352\n",
            "The running loss at 6673 iteration is: 5.011701353733326\n",
            "The running loss at 6674 iteration is: 10.947947310506944\n",
            "The running loss at 6675 iteration is: 19.527501350956726\n",
            "The running loss at 6676 iteration is: 39.35963933736131\n",
            "The running loss at 6677 iteration is: 44.0515979725492\n",
            "The running loss at 6678 iteration is: 48.59176272074825\n",
            "The running loss at 6679 iteration is: 12.88219445392917\n",
            "The running loss at 6680 iteration is: 0.8855132715649758\n",
            "The running loss at 6681 iteration is: 17.836868064896322\n",
            "The running loss at 6682 iteration is: 18.260230465063056\n",
            "The running loss at 6683 iteration is: 4.866373866961101\n",
            "The running loss at 6684 iteration is: 1.5642594801165604\n",
            "The running loss at 6685 iteration is: 12.427335321170942\n",
            "The running loss at 6686 iteration is: 16.196301727677298\n",
            "The running loss at 6687 iteration is: 3.8784952745909402\n",
            "The running loss at 6688 iteration is: 1.1391366034356414\n",
            "The running loss at 6689 iteration is: 9.337466376923024\n",
            "The running loss at 6690 iteration is: 11.179681552559583\n",
            "The running loss at 6691 iteration is: 6.04619844095895\n",
            "The running loss at 6692 iteration is: 0.46850432780419926\n",
            "The running loss at 6693 iteration is: 2.9611607934434394\n",
            "The running loss at 6694 iteration is: 7.962086275769978\n",
            "The running loss at 6695 iteration is: 5.77798246640457\n",
            "The running loss at 6696 iteration is: 1.3510340482029155\n",
            "The running loss at 6697 iteration is: 0.7576372684818667\n",
            "The running loss at 6698 iteration is: 3.7274329718168575\n",
            "The running loss at 6699 iteration is: 4.582632050590817\n",
            "The running loss at 6700 iteration is: 1.4562170549565734\n",
            "The running loss at 6701 iteration is: 0.43877093765467545\n",
            "The running loss at 6702 iteration is: 2.41443686778229\n",
            "The running loss at 6703 iteration is: 2.7076085423467724\n",
            "The running loss at 6704 iteration is: 0.9523555420363734\n",
            "The running loss at 6705 iteration is: 0.40083145427647743\n",
            "The running loss at 6706 iteration is: 1.6641807613532749\n",
            "The running loss at 6707 iteration is: 1.839305183305275\n",
            "The running loss at 6708 iteration is: 0.5295373264020123\n",
            "The running loss at 6709 iteration is: 0.5006905127348806\n",
            "The running loss at 6710 iteration is: 1.393476923288782\n",
            "The running loss at 6711 iteration is: 1.0360922066076887\n",
            "The running loss at 6712 iteration is: 0.2919846549451088\n",
            "The running loss at 6713 iteration is: 0.5927067764973663\n",
            "The running loss at 6714 iteration is: 1.0014390760780023\n",
            "The running loss at 6715 iteration is: 0.5857721917967064\n",
            "The running loss at 6716 iteration is: 0.25898658219979365\n",
            "The running loss at 6717 iteration is: 0.6193154673257647\n",
            "The running loss at 6718 iteration is: 0.732277654108057\n",
            "The running loss at 6719 iteration is: 0.33692588837647997\n",
            "The running loss at 6720 iteration is: 0.3113078614668221\n",
            "The running loss at 6721 iteration is: 0.5856109802967657\n",
            "The running loss at 6722 iteration is: 0.4726666978361213\n",
            "The running loss at 6723 iteration is: 0.24549592333839554\n",
            "The running loss at 6724 iteration is: 0.35692148799351536\n",
            "The running loss at 6725 iteration is: 0.4709724387851854\n",
            "The running loss at 6726 iteration is: 0.32067010273015384\n",
            "The running loss at 6727 iteration is: 0.24034150012944913\n",
            "The running loss at 6728 iteration is: 0.36201374833180056\n",
            "The running loss at 6729 iteration is: 0.372471177494188\n",
            "The running loss at 6730 iteration is: 0.24826517408799598\n",
            "The running loss at 6731 iteration is: 0.25805332277637855\n",
            "The running loss at 6732 iteration is: 0.33901519936376934\n",
            "The running loss at 6733 iteration is: 0.29149192103473975\n",
            "The running loss at 6734 iteration is: 0.2251132797199148\n",
            "The running loss at 6735 iteration is: 0.26687761731868476\n",
            "The running loss at 6736 iteration is: 0.2968415457996036\n",
            "The running loss at 6737 iteration is: 0.24491567772299003\n",
            "The running loss at 6738 iteration is: 0.22310188057341826\n",
            "The running loss at 6739 iteration is: 0.26167432203541857\n",
            "The running loss at 6740 iteration is: 0.2618261555913362\n",
            "The running loss at 6741 iteration is: 0.2223605939509949\n",
            "The running loss at 6742 iteration is: 0.2248010276406218\n",
            "The running loss at 6743 iteration is: 0.24939952080808525\n",
            "The running loss at 6744 iteration is: 0.23532396945271802\n",
            "The running loss at 6745 iteration is: 0.21291618010974453\n",
            "The running loss at 6746 iteration is: 0.22360320924605334\n",
            "The running loss at 6747 iteration is: 0.23437036737588898\n",
            "The running loss at 6748 iteration is: 0.2190310414227143\n",
            "The running loss at 6749 iteration is: 0.20884552342317433\n",
            "The running loss at 6750 iteration is: 0.2192669089720503\n",
            "The running loss at 6751 iteration is: 0.2217732116072718\n",
            "The running loss at 6752 iteration is: 0.2091608632712715\n",
            "The running loss at 6753 iteration is: 0.20601182031117743\n",
            "The running loss at 6754 iteration is: 0.21351026072424367\n",
            "The running loss at 6755 iteration is: 0.21176519420107123\n",
            "The running loss at 6756 iteration is: 0.20310464895428837\n",
            "The running loss at 6757 iteration is: 0.20283930537235206\n",
            "The running loss at 6758 iteration is: 0.2071331585618953\n",
            "The running loss at 6759 iteration is: 0.20414979722358031\n",
            "The running loss at 6760 iteration is: 0.19863951487128212\n",
            "The running loss at 6761 iteration is: 0.19942273027721177\n",
            "The running loss at 6762 iteration is: 0.20160612358049346\n",
            "The running loss at 6763 iteration is: 0.19858613234577738\n",
            "The running loss at 6764 iteration is: 0.19496914946832677\n",
            "The running loss at 6765 iteration is: 0.19564161333364147\n",
            "The running loss at 6766 iteration is: 0.19650654859942587\n",
            "The running loss at 6767 iteration is: 0.19401744046194078\n",
            "The running loss at 6768 iteration is: 0.19159707363201536\n",
            "The running loss at 6769 iteration is: 0.19193272964937177\n",
            "The running loss at 6770 iteration is: 0.19217178422775075\n",
            "The running loss at 6771 iteration is: 0.1901053858731274\n",
            "The running loss at 6772 iteration is: 0.18831615052614348\n",
            "The running loss at 6773 iteration is: 0.18826463991283465\n",
            "The running loss at 6774 iteration is: 0.1881559944472853\n",
            "The running loss at 6775 iteration is: 0.18656812944986798\n",
            "The running loss at 6776 iteration is: 0.18509605949198\n",
            "The running loss at 6777 iteration is: 0.18482873501806993\n",
            "The running loss at 6778 iteration is: 0.1845355984167465\n",
            "The running loss at 6779 iteration is: 0.18332478932718801\n",
            "The running loss at 6780 iteration is: 0.1819761536321032\n",
            "The running loss at 6781 iteration is: 0.18152362884848483\n",
            "The running loss at 6782 iteration is: 0.18117641447145025\n",
            "The running loss at 6783 iteration is: 0.18015259178905887\n",
            "The running loss at 6784 iteration is: 0.17893485433765366\n",
            "The running loss at 6785 iteration is: 0.1783745831993098\n",
            "The running loss at 6786 iteration is: 0.1778546854587198\n",
            "The running loss at 6787 iteration is: 0.17705251993522597\n",
            "The running loss at 6788 iteration is: 0.1760227049062958\n",
            "The running loss at 6789 iteration is: 0.17530617094545262\n",
            "The running loss at 6790 iteration is: 0.174787423822519\n",
            "The running loss at 6791 iteration is: 0.17409987677737265\n",
            "The running loss at 6792 iteration is: 0.17320547032906697\n",
            "The running loss at 6793 iteration is: 0.17243844428420166\n",
            "The running loss at 6794 iteration is: 0.17183396378327073\n",
            "The running loss at 6795 iteration is: 0.17116784670747032\n",
            "The running loss at 6796 iteration is: 0.17042770553307163\n",
            "The running loss at 6797 iteration is: 0.16964153531247422\n",
            "The running loss at 6798 iteration is: 0.16897095920489194\n",
            "The running loss at 6799 iteration is: 0.16838823695548108\n",
            "The running loss at 6800 iteration is: 0.16768330244727075\n",
            "The running loss at 6801 iteration is: 0.16692940476844784\n",
            "The running loss at 6802 iteration is: 0.16622921538840674\n",
            "The running loss at 6803 iteration is: 0.1656384941401137\n",
            "The running loss at 6804 iteration is: 0.16501334537329146\n",
            "The running loss at 6805 iteration is: 0.1643350582781006\n",
            "The running loss at 6806 iteration is: 0.16361316052633917\n",
            "The running loss at 6807 iteration is: 0.16296657782411103\n",
            "The running loss at 6808 iteration is: 0.16235249305779084\n",
            "The running loss at 6809 iteration is: 0.16172136235923493\n",
            "The running loss at 6810 iteration is: 0.1610447951208668\n",
            "The running loss at 6811 iteration is: 0.1604204298809138\n",
            "The running loss at 6812 iteration is: 0.1597853708712165\n",
            "The running loss at 6813 iteration is: 0.1591641476935712\n",
            "The running loss at 6814 iteration is: 0.1585386763680146\n",
            "The running loss at 6815 iteration is: 0.1579127726645714\n",
            "The running loss at 6816 iteration is: 0.1572713807026606\n",
            "The running loss at 6817 iteration is: 0.15668964633372365\n",
            "The running loss at 6818 iteration is: 0.15609773141033084\n",
            "The running loss at 6819 iteration is: 0.15547385124509353\n",
            "The running loss at 6820 iteration is: 0.15485872580806687\n",
            "The running loss at 6821 iteration is: 0.15424463043921383\n",
            "The running loss at 6822 iteration is: 0.1536527402286005\n",
            "The running loss at 6823 iteration is: 0.15305749494465476\n",
            "The running loss at 6824 iteration is: 0.1524859840279469\n",
            "The running loss at 6825 iteration is: 0.1518768375855477\n",
            "The running loss at 6826 iteration is: 0.15128659563276622\n",
            "The running loss at 6827 iteration is: 0.15074239738207534\n",
            "The running loss at 6828 iteration is: 0.1501450394278097\n",
            "The running loss at 6829 iteration is: 0.14958214817502652\n",
            "The running loss at 6830 iteration is: 0.14898012629583432\n",
            "The running loss at 6831 iteration is: 0.14844894909932382\n",
            "The running loss at 6832 iteration is: 0.14784301137761877\n",
            "The running loss at 6833 iteration is: 0.14730546407158857\n",
            "The running loss at 6834 iteration is: 0.14676421561810918\n",
            "The running loss at 6835 iteration is: 0.1462085402973285\n",
            "The running loss at 6836 iteration is: 0.14561972383355473\n",
            "The running loss at 6837 iteration is: 0.145067226571074\n",
            "The running loss at 6838 iteration is: 0.14454552465908274\n",
            "The running loss at 6839 iteration is: 0.1439948076214727\n",
            "The running loss at 6840 iteration is: 0.14348748965495897\n",
            "The running loss at 6841 iteration is: 0.14292758440483833\n",
            "The running loss at 6842 iteration is: 0.14240942481414823\n",
            "The running loss at 6843 iteration is: 0.14186994817416237\n",
            "The running loss at 6844 iteration is: 0.14135470815606277\n",
            "The running loss at 6845 iteration is: 0.14080331301652854\n",
            "The running loss at 6846 iteration is: 0.1402619891065405\n",
            "The running loss at 6847 iteration is: 0.1397715978287524\n",
            "The running loss at 6848 iteration is: 0.13926501305779487\n",
            "The running loss at 6849 iteration is: 0.1387491362617517\n",
            "The running loss at 6850 iteration is: 0.13820625693506272\n",
            "The running loss at 6851 iteration is: 0.1377406909123148\n",
            "The running loss at 6852 iteration is: 0.13719931515114164\n",
            "The running loss at 6853 iteration is: 0.1366990706822858\n",
            "The running loss at 6854 iteration is: 0.13620418516568764\n",
            "The running loss at 6855 iteration is: 0.13569880541270635\n",
            "The running loss at 6856 iteration is: 0.1351988912707568\n",
            "The running loss at 6857 iteration is: 0.13474044134392407\n",
            "The running loss at 6858 iteration is: 0.13422479760297132\n",
            "The running loss at 6859 iteration is: 0.13375409703232985\n",
            "The running loss at 6860 iteration is: 0.13324622202087572\n",
            "The running loss at 6861 iteration is: 0.13277786093419913\n",
            "The running loss at 6862 iteration is: 0.13228248142882096\n",
            "The running loss at 6863 iteration is: 0.13182250311960875\n",
            "The running loss at 6864 iteration is: 0.13131880540130533\n",
            "The running loss at 6865 iteration is: 0.1308449171196011\n",
            "The running loss at 6866 iteration is: 0.1303724812699981\n",
            "The running loss at 6867 iteration is: 0.12993661097236667\n",
            "The running loss at 6868 iteration is: 0.12945494591337314\n",
            "The running loss at 6869 iteration is: 0.1290251805305125\n",
            "The running loss at 6870 iteration is: 0.12854891368509902\n",
            "The running loss at 6871 iteration is: 0.12811409870073154\n",
            "The running loss at 6872 iteration is: 0.12766333471971802\n",
            "The running loss at 6873 iteration is: 0.12719791851865575\n",
            "The running loss at 6874 iteration is: 0.1267367737947205\n",
            "The running loss at 6875 iteration is: 0.1262794249319179\n",
            "The running loss at 6876 iteration is: 0.125865279908788\n",
            "The running loss at 6877 iteration is: 0.1253983892159786\n",
            "The running loss at 6878 iteration is: 0.12493387528683696\n",
            "The running loss at 6879 iteration is: 0.12450510926845101\n",
            "The running loss at 6880 iteration is: 0.12409813401960743\n",
            "The running loss at 6881 iteration is: 0.12364919195222494\n",
            "The running loss at 6882 iteration is: 0.12321693193865456\n",
            "The running loss at 6883 iteration is: 0.122820889860023\n",
            "The running loss at 6884 iteration is: 0.12237055546769396\n",
            "The running loss at 6885 iteration is: 0.12196577784146528\n",
            "The running loss at 6886 iteration is: 0.12162182698100935\n",
            "The running loss at 6887 iteration is: 0.12128489941140357\n",
            "The running loss at 6888 iteration is: 0.1210189084422166\n",
            "The running loss at 6889 iteration is: 0.1209444607560613\n",
            "The running loss at 6890 iteration is: 0.1211561637845589\n",
            "The running loss at 6891 iteration is: 0.12189411798574104\n",
            "The running loss at 6892 iteration is: 0.1237653783886747\n",
            "The running loss at 6893 iteration is: 0.1278219384520646\n",
            "The running loss at 6894 iteration is: 0.13641455470708744\n",
            "The running loss at 6895 iteration is: 0.1548400262010092\n",
            "The running loss at 6896 iteration is: 0.19334624397720446\n",
            "The running loss at 6897 iteration is: 0.2776634194700531\n",
            "The running loss at 6898 iteration is: 0.45585371511001127\n",
            "The running loss at 6899 iteration is: 0.8647530775387984\n",
            "The running loss at 6900 iteration is: 1.7208320813188116\n",
            "The running loss at 6901 iteration is: 3.8021328430163366\n",
            "The running loss at 6902 iteration is: 7.820822132346068\n",
            "The running loss at 6903 iteration is: 18.041672240866745\n",
            "The running loss at 6904 iteration is: 31.312953246439672\n",
            "The running loss at 6905 iteration is: 61.17320036162939\n",
            "The running loss at 6906 iteration is: 51.186013385487\n",
            "The running loss at 6907 iteration is: 34.038732080504076\n",
            "The running loss at 6908 iteration is: 1.590875262676838\n",
            "The running loss at 6909 iteration is: 13.775763920408204\n",
            "The running loss at 6910 iteration is: 37.995233594790314\n",
            "The running loss at 6911 iteration is: 13.087805654075682\n",
            "The running loss at 6912 iteration is: 0.6821762682556839\n",
            "The running loss at 6913 iteration is: 16.62287329856706\n",
            "The running loss at 6914 iteration is: 21.295846584410775\n",
            "The running loss at 6915 iteration is: 11.575833702226433\n",
            "The running loss at 6916 iteration is: 0.44336710721607775\n",
            "The running loss at 6917 iteration is: 7.040485322993399\n",
            "The running loss at 6918 iteration is: 18.270803733578425\n",
            "The running loss at 6919 iteration is: 11.284536162912463\n",
            "The running loss at 6920 iteration is: 1.8178032056008369\n",
            "The running loss at 6921 iteration is: 1.875877119695973\n",
            "The running loss at 6922 iteration is: 8.240622252950125\n",
            "The running loss at 6923 iteration is: 9.01065531526265\n",
            "The running loss at 6924 iteration is: 1.7670468346175794\n",
            "The running loss at 6925 iteration is: 1.3639665123053824\n",
            "The running loss at 6926 iteration is: 6.1733607700655595\n",
            "The running loss at 6927 iteration is: 4.32644571608217\n",
            "The running loss at 6928 iteration is: 0.48839389931511584\n",
            "The running loss at 6929 iteration is: 1.7798076989743064\n",
            "The running loss at 6930 iteration is: 3.785773261524475\n",
            "The running loss at 6931 iteration is: 1.8620826941887967\n",
            "The running loss at 6932 iteration is: 0.3043290567527047\n",
            "The running loss at 6933 iteration is: 2.1842986368660364\n",
            "The running loss at 6934 iteration is: 2.51876507780527\n",
            "The running loss at 6935 iteration is: 0.46328207758483486\n",
            "The running loss at 6936 iteration is: 0.9138399462631263\n",
            "The running loss at 6937 iteration is: 2.1442687382226726\n",
            "The running loss at 6938 iteration is: 0.907996739756345\n",
            "The running loss at 6939 iteration is: 0.31580391209530023\n",
            "The running loss at 6940 iteration is: 1.3474655628098555\n",
            "The running loss at 6941 iteration is: 1.1176949591215821\n",
            "The running loss at 6942 iteration is: 0.25658727773388945\n",
            "The running loss at 6943 iteration is: 0.6570793297179627\n",
            "The running loss at 6944 iteration is: 1.0283099908458997\n",
            "The running loss at 6945 iteration is: 0.4454934855266502\n",
            "The running loss at 6946 iteration is: 0.2915957522899463\n",
            "The running loss at 6947 iteration is: 0.7548974865543834\n",
            "The running loss at 6948 iteration is: 0.6133576431103112\n",
            "The running loss at 6949 iteration is: 0.22130281124418213\n",
            "The running loss at 6950 iteration is: 0.45754900619113603\n",
            "The running loss at 6951 iteration is: 0.6239655592844646\n",
            "The running loss at 6952 iteration is: 0.3012146507941137\n",
            "The running loss at 6953 iteration is: 0.2623733705067048\n",
            "The running loss at 6954 iteration is: 0.49510392657593727\n",
            "The running loss at 6955 iteration is: 0.38502443829270044\n",
            "The running loss at 6956 iteration is: 0.20822158987020847\n",
            "The running loss at 6957 iteration is: 0.33125031532400195\n",
            "The running loss at 6958 iteration is: 0.39499006832971506\n",
            "The running loss at 6959 iteration is: 0.2469156518716328\n",
            "The running loss at 6960 iteration is: 0.2248793693443124\n",
            "The running loss at 6961 iteration is: 0.3330201127056314\n",
            "The running loss at 6962 iteration is: 0.295063298629833\n",
            "The running loss at 6963 iteration is: 0.20210762942443958\n",
            "The running loss at 6964 iteration is: 0.2506006801895389\n",
            "The running loss at 6965 iteration is: 0.29618446791836456\n",
            "The running loss at 6966 iteration is: 0.22775475178384288\n",
            "The running loss at 6967 iteration is: 0.20140014942777937\n",
            "The running loss at 6968 iteration is: 0.25362225753978257\n",
            "The running loss at 6969 iteration is: 0.2483248784508527\n",
            "The running loss at 6970 iteration is: 0.1987029090886756\n",
            "The running loss at 6971 iteration is: 0.20844312939704707\n",
            "The running loss at 6972 iteration is: 0.23830799198453806\n",
            "The running loss at 6973 iteration is: 0.21584968535138538\n",
            "The running loss at 6974 iteration is: 0.19062548193353523\n",
            "The running loss at 6975 iteration is: 0.20938595835538876\n",
            "The running loss at 6976 iteration is: 0.21995653898818543\n",
            "The running loss at 6977 iteration is: 0.19695938538424437\n",
            "The running loss at 6978 iteration is: 0.18898104947338848\n",
            "The running loss at 6979 iteration is: 0.20502720365091176\n",
            "The running loss at 6980 iteration is: 0.2041257577271407\n",
            "The running loss at 6981 iteration is: 0.18753909481858677\n",
            "The running loss at 6982 iteration is: 0.18753187857422257\n",
            "The running loss at 6983 iteration is: 0.19763874540620377\n",
            "The running loss at 6984 iteration is: 0.19294671177324357\n",
            "The running loss at 6985 iteration is: 0.18242685177375625\n",
            "The running loss at 6986 iteration is: 0.18501793429200122\n",
            "The running loss at 6987 iteration is: 0.19053458594942\n",
            "The running loss at 6988 iteration is: 0.18526853206581512\n",
            "The running loss at 6989 iteration is: 0.17893396333948383\n",
            "The running loss at 6990 iteration is: 0.18157052605559124\n",
            "The running loss at 6991 iteration is: 0.1842437313376613\n",
            "The running loss at 6992 iteration is: 0.1799262629351973\n",
            "The running loss at 6993 iteration is: 0.17603090215930017\n",
            "The running loss at 6994 iteration is: 0.17786071582492421\n",
            "The running loss at 6995 iteration is: 0.1791822819719035\n",
            "The running loss at 6996 iteration is: 0.1758974130528881\n",
            "The running loss at 6997 iteration is: 0.17317627997319834\n",
            "The running loss at 6998 iteration is: 0.1741869991247661\n",
            "The running loss at 6999 iteration is: 0.1747974144692841\n",
            "The running loss at 7000 iteration is: 0.17245946800067266\n",
            "The running loss at 7001 iteration is: 0.1703991950535788\n",
            "The running loss at 7002 iteration is: 0.17073369058794352\n",
            "The running loss at 7003 iteration is: 0.17097223686977245\n",
            "The running loss at 7004 iteration is: 0.16935144105067224\n",
            "The running loss at 7005 iteration is: 0.16767248260022735\n",
            "The running loss at 7006 iteration is: 0.16757441250233726\n",
            "The running loss at 7007 iteration is: 0.16766573892818146\n",
            "The running loss at 7008 iteration is: 0.16652153501988348\n",
            "The running loss at 7009 iteration is: 0.1650890936989686\n",
            "The running loss at 7010 iteration is: 0.16461188075486805\n",
            "The running loss at 7011 iteration is: 0.16448872102630943\n",
            "The running loss at 7012 iteration is: 0.16374593866262385\n",
            "The running loss at 7013 iteration is: 0.16255644703683386\n",
            "The running loss at 7014 iteration is: 0.1618486238351176\n",
            "The running loss at 7015 iteration is: 0.16156809420780807\n",
            "The running loss at 7016 iteration is: 0.16103026630827033\n",
            "The running loss at 7017 iteration is: 0.16011439560362004\n",
            "The running loss at 7018 iteration is: 0.15925357087691497\n",
            "The running loss at 7019 iteration is: 0.15883814097046497\n",
            "The running loss at 7020 iteration is: 0.15843034728426628\n",
            "The running loss at 7021 iteration is: 0.15768584562314533\n",
            "The running loss at 7022 iteration is: 0.15682393891601829\n",
            "The running loss at 7023 iteration is: 0.15626293227320584\n",
            "The running loss at 7024 iteration is: 0.1557475528864794\n",
            "The running loss at 7025 iteration is: 0.15521727908818397\n",
            "The running loss at 7026 iteration is: 0.15453602177450088\n",
            "The running loss at 7027 iteration is: 0.1538498670636015\n",
            "The running loss at 7028 iteration is: 0.15325234469714802\n",
            "The running loss at 7029 iteration is: 0.15276754379364912\n",
            "The running loss at 7030 iteration is: 0.15219457472618222\n",
            "The running loss at 7031 iteration is: 0.1515337201557898\n",
            "The running loss at 7032 iteration is: 0.1509164301555014\n",
            "The running loss at 7033 iteration is: 0.15032528169596565\n",
            "The running loss at 7034 iteration is: 0.1498462626480783\n",
            "The running loss at 7035 iteration is: 0.14928275200152616\n",
            "The running loss at 7036 iteration is: 0.1486337253626745\n",
            "The running loss at 7037 iteration is: 0.14805398399800165\n",
            "The running loss at 7038 iteration is: 0.14751902127057115\n",
            "The running loss at 7039 iteration is: 0.146992137904262\n",
            "The running loss at 7040 iteration is: 0.14642044817824443\n",
            "The running loss at 7041 iteration is: 0.14587555354689716\n",
            "The running loss at 7042 iteration is: 0.14527251946135283\n",
            "The running loss at 7043 iteration is: 0.14475290332785684\n",
            "The running loss at 7044 iteration is: 0.1442178518735033\n",
            "The running loss at 7045 iteration is: 0.14367536456169588\n",
            "The running loss at 7046 iteration is: 0.1431475792881244\n",
            "The running loss at 7047 iteration is: 0.1425943183662462\n",
            "The running loss at 7048 iteration is: 0.14205306580606172\n",
            "The running loss at 7049 iteration is: 0.14155532491514097\n",
            "The running loss at 7050 iteration is: 0.14102013779523542\n",
            "The running loss at 7051 iteration is: 0.1404974453653308\n",
            "The running loss at 7052 iteration is: 0.13996378841989746\n",
            "The running loss at 7053 iteration is: 0.1394864805070226\n",
            "The running loss at 7054 iteration is: 0.1389465904954377\n",
            "The running loss at 7055 iteration is: 0.13845269266984284\n",
            "The running loss at 7056 iteration is: 0.13794845505997272\n",
            "The running loss at 7057 iteration is: 0.13741505902386272\n",
            "The running loss at 7058 iteration is: 0.13693804983167127\n",
            "The running loss at 7059 iteration is: 0.13646458974725134\n",
            "The running loss at 7060 iteration is: 0.135932501879635\n",
            "The running loss at 7061 iteration is: 0.13542920172776085\n",
            "The running loss at 7062 iteration is: 0.1348982509511555\n",
            "The running loss at 7063 iteration is: 0.1344326219907104\n",
            "The running loss at 7064 iteration is: 0.13397524059430443\n",
            "The running loss at 7065 iteration is: 0.13346810382041918\n",
            "The running loss at 7066 iteration is: 0.13298280456371186\n",
            "The running loss at 7067 iteration is: 0.13250769139937615\n",
            "The running loss at 7068 iteration is: 0.13204910165485437\n",
            "The running loss at 7069 iteration is: 0.13157710721105503\n",
            "The running loss at 7070 iteration is: 0.1311234223393388\n",
            "The running loss at 7071 iteration is: 0.13063703521508238\n",
            "The running loss at 7072 iteration is: 0.13017081187544552\n",
            "The running loss at 7073 iteration is: 0.12969132868740102\n",
            "The running loss at 7074 iteration is: 0.12925844923041188\n",
            "The running loss at 7075 iteration is: 0.12876922413973285\n",
            "The running loss at 7076 iteration is: 0.128309027153962\n",
            "The running loss at 7077 iteration is: 0.1278528345390652\n",
            "The running loss at 7078 iteration is: 0.1273838906094532\n",
            "The running loss at 7079 iteration is: 0.12691685234451047\n",
            "The running loss at 7080 iteration is: 0.1265108814609362\n",
            "The running loss at 7081 iteration is: 0.12605710991599728\n",
            "The running loss at 7082 iteration is: 0.1256007338684124\n",
            "The running loss at 7083 iteration is: 0.12519022157708826\n",
            "The running loss at 7084 iteration is: 0.12474238945338818\n",
            "The running loss at 7085 iteration is: 0.12425731781777966\n",
            "The running loss at 7086 iteration is: 0.12388034532309268\n",
            "The running loss at 7087 iteration is: 0.12343753057565428\n",
            "The running loss at 7088 iteration is: 0.12298488684937993\n",
            "The running loss at 7089 iteration is: 0.12258826570417768\n",
            "The running loss at 7090 iteration is: 0.1221396209996275\n",
            "The running loss at 7091 iteration is: 0.12173080483292348\n",
            "The running loss at 7092 iteration is: 0.12136678275277714\n",
            "The running loss at 7093 iteration is: 0.12090873376911612\n",
            "The running loss at 7094 iteration is: 0.12055719228062733\n",
            "The running loss at 7095 iteration is: 0.12021183810211891\n",
            "The running loss at 7096 iteration is: 0.11986855746912287\n",
            "The running loss at 7097 iteration is: 0.11957118506464692\n",
            "The running loss at 7098 iteration is: 0.11937676998630442\n",
            "The running loss at 7099 iteration is: 0.1193497700357293\n",
            "The running loss at 7100 iteration is: 0.11960866762584635\n",
            "The running loss at 7101 iteration is: 0.12028272548012842\n",
            "The running loss at 7102 iteration is: 0.12185815421418403\n",
            "The running loss at 7103 iteration is: 0.12492981994926822\n",
            "The running loss at 7104 iteration is: 0.13106644594838698\n",
            "The running loss at 7105 iteration is: 0.14271374959921068\n",
            "The running loss at 7106 iteration is: 0.1657758166133138\n",
            "The running loss at 7107 iteration is: 0.21042196805608418\n",
            "The running loss at 7108 iteration is: 0.3023499831588447\n",
            "The running loss at 7109 iteration is: 0.48307616401382814\n",
            "The running loss at 7110 iteration is: 0.8747111454634483\n",
            "The running loss at 7111 iteration is: 1.6388626585809156\n",
            "The running loss at 7112 iteration is: 3.395216400006931\n",
            "The running loss at 7113 iteration is: 6.555960711409178\n",
            "The running loss at 7114 iteration is: 14.13146787761611\n",
            "The running loss at 7115 iteration is: 23.558712896882987\n",
            "The running loss at 7116 iteration is: 43.7936479529112\n",
            "The running loss at 7117 iteration is: 39.86459605046228\n",
            "The running loss at 7118 iteration is: 30.54889578663363\n",
            "The running loss at 7119 iteration is: 3.365806494359541\n",
            "The running loss at 7120 iteration is: 6.135441599059396\n",
            "The running loss at 7121 iteration is: 23.650446985959565\n",
            "The running loss at 7122 iteration is: 11.142876449168545\n",
            "The running loss at 7123 iteration is: 0.24749946201679074\n",
            "The running loss at 7124 iteration is: 8.629047953020036\n",
            "The running loss at 7125 iteration is: 12.912230444114423\n",
            "The running loss at 7126 iteration is: 6.008661299530593\n",
            "The running loss at 7127 iteration is: 0.3243032862433798\n",
            "The running loss at 7128 iteration is: 6.175838423593452\n",
            "The running loss at 7129 iteration is: 11.891956629893416\n",
            "The running loss at 7130 iteration is: 5.689966717218387\n",
            "The running loss at 7131 iteration is: 0.4622994363734861\n",
            "The running loss at 7132 iteration is: 2.697257342441644\n",
            "The running loss at 7133 iteration is: 6.59120258906163\n",
            "The running loss at 7134 iteration is: 6.059826482201459\n",
            "The running loss at 7135 iteration is: 1.3907802964153317\n",
            "The running loss at 7136 iteration is: 0.7712945241291211\n",
            "The running loss at 7137 iteration is: 3.815986507803323\n",
            "The running loss at 7138 iteration is: 4.06828857126645\n",
            "The running loss at 7139 iteration is: 1.5781106960079143\n",
            "The running loss at 7140 iteration is: 0.36010182565633003\n",
            "The running loss at 7141 iteration is: 2.0217614761806577\n",
            "The running loss at 7142 iteration is: 2.9469041631533393\n",
            "The running loss at 7143 iteration is: 1.1806304807669028\n",
            "The running loss at 7144 iteration is: 0.33785267555597503\n",
            "The running loss at 7145 iteration is: 1.4831701210517454\n",
            "The running loss at 7146 iteration is: 1.7891685334566672\n",
            "The running loss at 7147 iteration is: 0.717073993196788\n",
            "The running loss at 7148 iteration is: 0.3561396372510967\n",
            "The running loss at 7149 iteration is: 1.1414274853446458\n",
            "The running loss at 7150 iteration is: 1.2204103413610623\n",
            "The running loss at 7151 iteration is: 0.41839411110284175\n",
            "The running loss at 7152 iteration is: 0.4336012923591041\n",
            "The running loss at 7153 iteration is: 0.9751480090648593\n",
            "The running loss at 7154 iteration is: 0.7252046317886607\n",
            "The running loss at 7155 iteration is: 0.2782776238096925\n",
            "The running loss at 7156 iteration is: 0.49499341297763727\n",
            "The running loss at 7157 iteration is: 0.7260950966192672\n",
            "The running loss at 7158 iteration is: 0.442020777319188\n",
            "The running loss at 7159 iteration is: 0.26740379559558974\n",
            "The running loss at 7160 iteration is: 0.5021430807706924\n",
            "The running loss at 7161 iteration is: 0.5423596921719394\n",
            "The running loss at 7162 iteration is: 0.29641131267845605\n",
            "The running loss at 7163 iteration is: 0.3032322350452954\n",
            "The running loss at 7164 iteration is: 0.4672772184405035\n",
            "The running loss at 7165 iteration is: 0.3798100496368296\n",
            "The running loss at 7166 iteration is: 0.24747211781742853\n",
            "The running loss at 7167 iteration is: 0.3289990904987624\n",
            "The running loss at 7168 iteration is: 0.38779980989072127\n",
            "The running loss at 7169 iteration is: 0.28583550361404686\n",
            "The running loss at 7170 iteration is: 0.24777904504447554\n",
            "The running loss at 7171 iteration is: 0.3253343759930832\n",
            "The running loss at 7172 iteration is: 0.31948911908028627\n",
            "The running loss at 7173 iteration is: 0.24348796241034487\n",
            "The running loss at 7174 iteration is: 0.2583355512955528\n",
            "The running loss at 7175 iteration is: 0.3041211069979655\n",
            "The running loss at 7176 iteration is: 0.26746879955725056\n",
            "The running loss at 7177 iteration is: 0.23098675905760208\n",
            "The running loss at 7178 iteration is: 0.26102518984086864\n",
            "The running loss at 7179 iteration is: 0.273387841314265\n",
            "The running loss at 7180 iteration is: 0.23815039695977042\n",
            "The running loss at 7181 iteration is: 0.22990887934262888\n",
            "The running loss at 7182 iteration is: 0.25394988751073944\n",
            "The running loss at 7183 iteration is: 0.24829574446188324\n",
            "The running loss at 7184 iteration is: 0.22427078819477614\n",
            "The running loss at 7185 iteration is: 0.22965029361873257\n",
            "The running loss at 7186 iteration is: 0.24280096583798164\n",
            "The running loss at 7187 iteration is: 0.23031795716917872\n",
            "The running loss at 7188 iteration is: 0.21823533383166538\n",
            "The running loss at 7189 iteration is: 0.226986705890229\n",
            "The running loss at 7190 iteration is: 0.2307985424402558\n",
            "The running loss at 7191 iteration is: 0.21907864145987188\n",
            "The running loss at 7192 iteration is: 0.21484649319847918\n",
            "The running loss at 7193 iteration is: 0.22191577557211317\n",
            "The running loss at 7194 iteration is: 0.22063497441186194\n",
            "The running loss at 7195 iteration is: 0.21198596352581672\n",
            "The running loss at 7196 iteration is: 0.21165010366339618\n",
            "The running loss at 7197 iteration is: 0.21584047419390115\n",
            "The running loss at 7198 iteration is: 0.2125266636783427\n",
            "The running loss at 7199 iteration is: 0.20712262546304402\n",
            "The running loss at 7200 iteration is: 0.20808346300799938\n",
            "The running loss at 7201 iteration is: 0.20989966655681105\n",
            "The running loss at 7202 iteration is: 0.20640984006697347\n",
            "The running loss at 7203 iteration is: 0.2031059517706597\n",
            "The running loss at 7204 iteration is: 0.20413352182618064\n",
            "The running loss at 7205 iteration is: 0.20459159703932275\n",
            "The running loss at 7206 iteration is: 0.20154001207594688\n",
            "The running loss at 7207 iteration is: 0.1994789314652042\n",
            "The running loss at 7208 iteration is: 0.20009699624518226\n",
            "The running loss at 7209 iteration is: 0.19970887614571345\n",
            "The running loss at 7210 iteration is: 0.1973370881665727\n",
            "The running loss at 7211 iteration is: 0.19593874327570743\n",
            "The running loss at 7212 iteration is: 0.1961018835492845\n",
            "The running loss at 7213 iteration is: 0.1954335784365206\n",
            "The running loss at 7214 iteration is: 0.19351053437039933\n",
            "The running loss at 7215 iteration is: 0.1924113867246338\n",
            "The running loss at 7216 iteration is: 0.19225823374739276\n",
            "The running loss at 7217 iteration is: 0.19148921672082994\n",
            "The running loss at 7218 iteration is: 0.18995674913457386\n",
            "The running loss at 7219 iteration is: 0.18904007252843744\n",
            "The running loss at 7220 iteration is: 0.18867130504272034\n",
            "The running loss at 7221 iteration is: 0.187897953003474\n",
            "The running loss at 7222 iteration is: 0.18662213476634215\n",
            "The running loss at 7223 iteration is: 0.1856493507913419\n",
            "The running loss at 7224 iteration is: 0.1851675725531422\n",
            "The running loss at 7225 iteration is: 0.18446954977287014\n",
            "The running loss at 7226 iteration is: 0.18340207526017643\n",
            "The running loss at 7227 iteration is: 0.18240921271192628\n",
            "The running loss at 7228 iteration is: 0.18182196556320693\n",
            "The running loss at 7229 iteration is: 0.1811579450913927\n",
            "The running loss at 7230 iteration is: 0.18020200672180361\n",
            "The running loss at 7231 iteration is: 0.1792991043815578\n",
            "The running loss at 7232 iteration is: 0.1786480028734875\n",
            "The running loss at 7233 iteration is: 0.1779969884122046\n",
            "The running loss at 7234 iteration is: 0.17712872081817316\n",
            "The running loss at 7235 iteration is: 0.1762769059231464\n",
            "The running loss at 7236 iteration is: 0.17555507538201182\n",
            "The running loss at 7237 iteration is: 0.17486092242566864\n",
            "The running loss at 7238 iteration is: 0.17411733623893538\n",
            "The running loss at 7239 iteration is: 0.17330808233719522\n",
            "The running loss at 7240 iteration is: 0.17257642338234047\n",
            "The running loss at 7241 iteration is: 0.1719268285946052\n",
            "The running loss at 7242 iteration is: 0.17119301506181778\n",
            "The running loss at 7243 iteration is: 0.17043696098114125\n",
            "The running loss at 7244 iteration is: 0.169666176817106\n",
            "The running loss at 7245 iteration is: 0.1689982045584029\n",
            "The running loss at 7246 iteration is: 0.16829330629216474\n",
            "The running loss at 7247 iteration is: 0.16761901039599866\n",
            "The running loss at 7248 iteration is: 0.16686871445615306\n",
            "The running loss at 7249 iteration is: 0.1661547617922151\n",
            "The running loss at 7250 iteration is: 0.16548943033271124\n",
            "The running loss at 7251 iteration is: 0.16485365404663452\n",
            "The running loss at 7252 iteration is: 0.16416393686135602\n",
            "The running loss at 7253 iteration is: 0.16347446034754362\n",
            "The running loss at 7254 iteration is: 0.16276917116100464\n",
            "The running loss at 7255 iteration is: 0.16213023829175274\n",
            "The running loss at 7256 iteration is: 0.16146027938087992\n",
            "The running loss at 7257 iteration is: 0.16083260995048798\n",
            "The running loss at 7258 iteration is: 0.16012155610734052\n",
            "The running loss at 7259 iteration is: 0.15950332128692843\n",
            "The running loss at 7260 iteration is: 0.15884422720829988\n",
            "The running loss at 7261 iteration is: 0.1582013479520295\n",
            "The running loss at 7262 iteration is: 0.15755428891317966\n",
            "The running loss at 7263 iteration is: 0.15693088322324217\n",
            "The running loss at 7264 iteration is: 0.15632109124880786\n",
            "The running loss at 7265 iteration is: 0.15566891727766413\n",
            "The running loss at 7266 iteration is: 0.15507334311736037\n",
            "The running loss at 7267 iteration is: 0.15441435423998548\n",
            "The running loss at 7268 iteration is: 0.15382237214362474\n",
            "The running loss at 7269 iteration is: 0.15321122764762524\n",
            "The running loss at 7270 iteration is: 0.15258975022225066\n",
            "The running loss at 7271 iteration is: 0.1519844796627372\n",
            "The running loss at 7272 iteration is: 0.15137793019795678\n",
            "The running loss at 7273 iteration is: 0.15074815234696057\n",
            "The running loss at 7274 iteration is: 0.1501922070172423\n",
            "The running loss at 7275 iteration is: 0.14956778314644506\n",
            "The running loss at 7276 iteration is: 0.14902308998080993\n",
            "The running loss at 7277 iteration is: 0.14840627921094382\n",
            "The running loss at 7278 iteration is: 0.14779754036420745\n",
            "The running loss at 7279 iteration is: 0.14721991543965093\n",
            "The running loss at 7280 iteration is: 0.14667558538604616\n",
            "The running loss at 7281 iteration is: 0.14608958713910483\n",
            "The running loss at 7282 iteration is: 0.14553358881316378\n",
            "The running loss at 7283 iteration is: 0.14497474668794386\n",
            "The running loss at 7284 iteration is: 0.14438609267514066\n",
            "The running loss at 7285 iteration is: 0.14381383301366105\n",
            "The running loss at 7286 iteration is: 0.14326630620299433\n",
            "The running loss at 7287 iteration is: 0.1427356887264426\n",
            "The running loss at 7288 iteration is: 0.14217051610752995\n",
            "The running loss at 7289 iteration is: 0.14163885462080975\n",
            "The running loss at 7290 iteration is: 0.14108504283958923\n",
            "The running loss at 7291 iteration is: 0.14054579418525448\n",
            "The running loss at 7292 iteration is: 0.14000011715879743\n",
            "The running loss at 7293 iteration is: 0.13945965740770055\n",
            "The running loss at 7294 iteration is: 0.1389211802127218\n",
            "The running loss at 7295 iteration is: 0.1383865480841819\n",
            "The running loss at 7296 iteration is: 0.1378563318618828\n",
            "The running loss at 7297 iteration is: 0.1373533296467246\n",
            "The running loss at 7298 iteration is: 0.13681628498765241\n",
            "The running loss at 7299 iteration is: 0.13626730531305328\n",
            "The running loss at 7300 iteration is: 0.13580240973087931\n",
            "The running loss at 7301 iteration is: 0.13527269530472072\n",
            "The running loss at 7302 iteration is: 0.13477959177074153\n",
            "The running loss at 7303 iteration is: 0.13426951818266009\n",
            "The running loss at 7304 iteration is: 0.13376945878523078\n",
            "The running loss at 7305 iteration is: 0.13325980916017294\n",
            "The running loss at 7306 iteration is: 0.13274369396326466\n",
            "The running loss at 7307 iteration is: 0.13223293761244032\n",
            "The running loss at 7308 iteration is: 0.1317736849098918\n",
            "The running loss at 7309 iteration is: 0.1312822574523821\n",
            "The running loss at 7310 iteration is: 0.1307681193653098\n",
            "The running loss at 7311 iteration is: 0.1302594294945243\n",
            "The running loss at 7312 iteration is: 0.12979318879004237\n",
            "The running loss at 7313 iteration is: 0.12933617824973798\n",
            "The running loss at 7314 iteration is: 0.1288452970028059\n",
            "The running loss at 7315 iteration is: 0.1283968882393774\n",
            "The running loss at 7316 iteration is: 0.12785112950486793\n",
            "The running loss at 7317 iteration is: 0.12745327740304718\n",
            "The running loss at 7318 iteration is: 0.12693942003969455\n",
            "The running loss at 7319 iteration is: 0.1265114560466274\n",
            "The running loss at 7320 iteration is: 0.126034195389792\n",
            "The running loss at 7321 iteration is: 0.12557499802591635\n",
            "The running loss at 7322 iteration is: 0.1251185688005837\n",
            "The running loss at 7323 iteration is: 0.1246511066301862\n",
            "The running loss at 7324 iteration is: 0.1241656305360994\n",
            "The running loss at 7325 iteration is: 0.1237214334903318\n",
            "The running loss at 7326 iteration is: 0.12331552634482959\n",
            "The running loss at 7327 iteration is: 0.12284648961059424\n",
            "The running loss at 7328 iteration is: 0.12238978917174852\n",
            "The running loss at 7329 iteration is: 0.1219747364640945\n",
            "The running loss at 7330 iteration is: 0.12153656569167474\n",
            "The running loss at 7331 iteration is: 0.1210836261146302\n",
            "The running loss at 7332 iteration is: 0.12074071678350806\n",
            "The running loss at 7333 iteration is: 0.12036418753536732\n",
            "The running loss at 7334 iteration is: 0.12015075446647216\n",
            "The running loss at 7335 iteration is: 0.12012081990594077\n",
            "The running loss at 7336 iteration is: 0.12050359030194975\n",
            "The running loss at 7337 iteration is: 0.12179049783190478\n",
            "The running loss at 7338 iteration is: 0.1251762104502799\n",
            "The running loss at 7339 iteration is: 0.13293027448506123\n",
            "The running loss at 7340 iteration is: 0.15069313015791053\n",
            "The running loss at 7341 iteration is: 0.19055902715523168\n",
            "The running loss at 7342 iteration is: 0.2838326743885269\n",
            "The running loss at 7343 iteration is: 0.4955317032857844\n",
            "The running loss at 7344 iteration is: 1.0152563406738777\n",
            "The running loss at 7345 iteration is: 2.181447501081047\n",
            "The running loss at 7346 iteration is: 5.2130738339862575\n",
            "The running loss at 7347 iteration is: 11.33470942526468\n",
            "The running loss at 7348 iteration is: 27.741168943334944\n",
            "The running loss at 7349 iteration is: 46.384048744854006\n",
            "The running loss at 7350 iteration is: 85.38508786786404\n",
            "The running loss at 7351 iteration is: 47.94792871752368\n",
            "The running loss at 7352 iteration is: 10.717916473066978\n",
            "The running loss at 7353 iteration is: 3.9127608566343923\n",
            "The running loss at 7354 iteration is: 29.508083723955995\n",
            "The running loss at 7355 iteration is: 35.65596304728026\n",
            "The running loss at 7356 iteration is: 3.6484173866244856\n",
            "The running loss at 7357 iteration is: 9.593329968335633\n",
            "The running loss at 7358 iteration is: 37.03022409671579\n",
            "The running loss at 7359 iteration is: 23.485249394850396\n",
            "The running loss at 7360 iteration is: 4.186415889824391\n",
            "The running loss at 7361 iteration is: 2.869099992552659\n",
            "The running loss at 7362 iteration is: 16.365050405736458\n",
            "The running loss at 7363 iteration is: 21.23551020131801\n",
            "The running loss at 7364 iteration is: 4.58252759067592\n",
            "The running loss at 7365 iteration is: 1.9890627091663546\n",
            "The running loss at 7366 iteration is: 12.55428413011439\n",
            "The running loss at 7367 iteration is: 8.718402516817294\n",
            "The running loss at 7368 iteration is: 0.6921922960668735\n",
            "The running loss at 7369 iteration is: 3.5601290529849066\n",
            "The running loss at 7370 iteration is: 7.086599668530348\n",
            "The running loss at 7371 iteration is: 2.693005797311992\n",
            "The running loss at 7372 iteration is: 0.6552269514623037\n",
            "The running loss at 7373 iteration is: 4.705637030566841\n",
            "The running loss at 7374 iteration is: 3.8095384529017102\n",
            "The running loss at 7375 iteration is: 0.2530149768549678\n",
            "The running loss at 7376 iteration is: 2.873201911012506\n",
            "The running loss at 7377 iteration is: 3.869949595634748\n",
            "The running loss at 7378 iteration is: 0.495565387283583\n",
            "The running loss at 7379 iteration is: 1.5963241993509902\n",
            "The running loss at 7380 iteration is: 3.311748335544776\n",
            "The running loss at 7381 iteration is: 0.8143147427770651\n",
            "The running loss at 7382 iteration is: 0.7707331029316317\n",
            "The running loss at 7383 iteration is: 2.486800629449782\n",
            "The running loss at 7384 iteration is: 1.0483583049407568\n",
            "The running loss at 7385 iteration is: 0.3305972376161401\n",
            "The running loss at 7386 iteration is: 1.6386114551445652\n",
            "The running loss at 7387 iteration is: 1.14307477622714\n",
            "The running loss at 7388 iteration is: 0.20712460610107283\n",
            "The running loss at 7389 iteration is: 0.9335136228497465\n",
            "The running loss at 7390 iteration is: 1.0865273204759445\n",
            "The running loss at 7391 iteration is: 0.29258409268292934\n",
            "The running loss at 7392 iteration is: 0.46178205019119195\n",
            "The running loss at 7393 iteration is: 0.9019067595210293\n",
            "The running loss at 7394 iteration is: 0.44971280612300635\n",
            "The running loss at 7395 iteration is: 0.23584635502511997\n",
            "The running loss at 7396 iteration is: 0.6497581228653613\n",
            "The running loss at 7397 iteration is: 0.5565292839993575\n",
            "The running loss at 7398 iteration is: 0.19968805216385455\n",
            "The running loss at 7399 iteration is: 0.40985456741647097\n",
            "The running loss at 7400 iteration is: 0.552830189581559\n",
            "The running loss at 7401 iteration is: 0.2609390273601152\n",
            "The running loss at 7402 iteration is: 0.24790985471940075\n",
            "The running loss at 7403 iteration is: 0.45265965856331264\n",
            "The running loss at 7404 iteration is: 0.32970597426450143\n",
            "The running loss at 7405 iteration is: 0.18958616256431557\n",
            "The running loss at 7406 iteration is: 0.3193662331855182\n",
            "The running loss at 7407 iteration is: 0.3492558597240009\n",
            "The running loss at 7408 iteration is: 0.209997948764397\n",
            "The running loss at 7409 iteration is: 0.21929630879978626\n",
            "The running loss at 7410 iteration is: 0.309732397560911\n",
            "The running loss at 7411 iteration is: 0.25155101999871377\n",
            "The running loss at 7412 iteration is: 0.18403071825936704\n",
            "The running loss at 7413 iteration is: 0.24384586207565154\n",
            "The running loss at 7414 iteration is: 0.26561819890535754\n",
            "The running loss at 7415 iteration is: 0.1970534082727849\n",
            "The running loss at 7416 iteration is: 0.19326754729530504\n",
            "The running loss at 7417 iteration is: 0.24066018152036178\n",
            "The running loss at 7418 iteration is: 0.21902986180435788\n",
            "The running loss at 7419 iteration is: 0.17954423005183345\n",
            "The running loss at 7420 iteration is: 0.20115685125331592\n",
            "The running loss at 7421 iteration is: 0.21987427662208642\n",
            "The running loss at 7422 iteration is: 0.19112547316337564\n",
            "The running loss at 7423 iteration is: 0.17777771192607775\n",
            "The running loss at 7424 iteration is: 0.19967384435509378\n",
            "The running loss at 7425 iteration is: 0.20066502292305322\n",
            "The running loss at 7426 iteration is: 0.17798084543253165\n",
            "The running loss at 7427 iteration is: 0.17840297012944092\n",
            "The running loss at 7428 iteration is: 0.19287441445188114\n",
            "The running loss at 7429 iteration is: 0.18587726622510342\n",
            "The running loss at 7430 iteration is: 0.17216525059759658\n",
            "The running loss at 7431 iteration is: 0.17701024081223493\n",
            "The running loss at 7432 iteration is: 0.18439837320182847\n",
            "The running loss at 7433 iteration is: 0.1766427825754948\n",
            "The running loss at 7434 iteration is: 0.16915160067365956\n",
            "The running loss at 7435 iteration is: 0.17400532032802912\n",
            "The running loss at 7436 iteration is: 0.17721394622872147\n",
            "The running loss at 7437 iteration is: 0.1707135888991923\n",
            "The running loss at 7438 iteration is: 0.16664889068746103\n",
            "The running loss at 7439 iteration is: 0.17036446894525914\n",
            "The running loss at 7440 iteration is: 0.17147649741542426\n",
            "The running loss at 7441 iteration is: 0.16663176859958917\n",
            "The running loss at 7442 iteration is: 0.16420096142772328\n",
            "The running loss at 7443 iteration is: 0.16656503532167602\n",
            "The running loss at 7444 iteration is: 0.16698708959334357\n",
            "The running loss at 7445 iteration is: 0.16359127577988286\n",
            "The running loss at 7446 iteration is: 0.1616585047514968\n",
            "The running loss at 7447 iteration is: 0.16297411478740137\n",
            "The running loss at 7448 iteration is: 0.1631797930477166\n",
            "The running loss at 7449 iteration is: 0.16087665357427997\n",
            "The running loss at 7450 iteration is: 0.1592349960351343\n",
            "The running loss at 7451 iteration is: 0.15973767455866003\n",
            "The running loss at 7452 iteration is: 0.15990003809404493\n",
            "The running loss at 7453 iteration is: 0.15834390566926485\n",
            "The running loss at 7454 iteration is: 0.15684970427812012\n",
            "The running loss at 7455 iteration is: 0.1568356305921558\n",
            "The running loss at 7456 iteration is: 0.15694314774202778\n",
            "The running loss at 7457 iteration is: 0.1559500875423024\n",
            "The running loss at 7458 iteration is: 0.15462277973227223\n",
            "The running loss at 7459 iteration is: 0.15408373049062413\n",
            "The running loss at 7460 iteration is: 0.15410144163099213\n",
            "The running loss at 7461 iteration is: 0.1535489150144502\n",
            "The running loss at 7462 iteration is: 0.15244560606649293\n",
            "The running loss at 7463 iteration is: 0.15168694887718187\n",
            "The running loss at 7464 iteration is: 0.1514577586801283\n",
            "The running loss at 7465 iteration is: 0.15114237274608935\n",
            "The running loss at 7466 iteration is: 0.15038800828002433\n",
            "The running loss at 7467 iteration is: 0.1494828586806691\n",
            "The running loss at 7468 iteration is: 0.14895248989292315\n",
            "The running loss at 7469 iteration is: 0.14867087021433412\n",
            "The running loss at 7470 iteration is: 0.14816070615033947\n",
            "The running loss at 7471 iteration is: 0.1474323790819078\n",
            "The running loss at 7472 iteration is: 0.14676178702124237\n",
            "The running loss at 7473 iteration is: 0.14627520237458688\n",
            "The running loss at 7474 iteration is: 0.1458922683425556\n",
            "The running loss at 7475 iteration is: 0.14537304958050734\n",
            "The running loss at 7476 iteration is: 0.14467906218462961\n",
            "The running loss at 7477 iteration is: 0.14410641330070179\n",
            "The running loss at 7478 iteration is: 0.14364280437407492\n",
            "The running loss at 7479 iteration is: 0.14319980479419986\n",
            "The running loss at 7480 iteration is: 0.1426600406883361\n",
            "The running loss at 7481 iteration is: 0.142070284247249\n",
            "The running loss at 7482 iteration is: 0.14151369924848334\n",
            "The running loss at 7483 iteration is: 0.14106732283303158\n",
            "The running loss at 7484 iteration is: 0.1406035179770893\n",
            "The running loss at 7485 iteration is: 0.14009733569243032\n",
            "The running loss at 7486 iteration is: 0.13957394743642054\n",
            "The running loss at 7487 iteration is: 0.13899866708849548\n",
            "The running loss at 7488 iteration is: 0.13849859658944438\n",
            "The running loss at 7489 iteration is: 0.13806374311086553\n",
            "The running loss at 7490 iteration is: 0.13759465745580024\n",
            "The running loss at 7491 iteration is: 0.13709138287954797\n",
            "The running loss at 7492 iteration is: 0.13654705375937867\n",
            "The running loss at 7493 iteration is: 0.13605499308928232\n",
            "The running loss at 7494 iteration is: 0.13557651751214017\n",
            "The running loss at 7495 iteration is: 0.13512282959584954\n",
            "The running loss at 7496 iteration is: 0.13462243931110354\n",
            "The running loss at 7497 iteration is: 0.13415297981646937\n",
            "The running loss at 7498 iteration is: 0.13371707914073996\n",
            "The running loss at 7499 iteration is: 0.13320892349504931\n",
            "The running loss at 7500 iteration is: 0.1327324957720328\n",
            "The running loss at 7501 iteration is: 0.13230373982595856\n",
            "The running loss at 7502 iteration is: 0.1318281973876522\n",
            "The running loss at 7503 iteration is: 0.13134006009306304\n",
            "The running loss at 7504 iteration is: 0.13093044425856737\n",
            "The running loss at 7505 iteration is: 0.13045268349748657\n",
            "The running loss at 7506 iteration is: 0.12999250852914854\n",
            "The running loss at 7507 iteration is: 0.1295111574759295\n",
            "The running loss at 7508 iteration is: 0.12907753044010592\n",
            "The running loss at 7509 iteration is: 0.12862125325785453\n",
            "The running loss at 7510 iteration is: 0.12821009748920853\n",
            "The running loss at 7511 iteration is: 0.12774324095560174\n",
            "The running loss at 7512 iteration is: 0.12732889351915633\n",
            "The running loss at 7513 iteration is: 0.12686986561485147\n",
            "The running loss at 7514 iteration is: 0.12646162771674846\n",
            "The running loss at 7515 iteration is: 0.12598374528780024\n",
            "The running loss at 7516 iteration is: 0.12559139678211256\n",
            "The running loss at 7517 iteration is: 0.12511267928659095\n",
            "The running loss at 7518 iteration is: 0.12470593294607725\n",
            "The running loss at 7519 iteration is: 0.12429124752399706\n",
            "The running loss at 7520 iteration is: 0.12385900094172288\n",
            "The running loss at 7521 iteration is: 0.12343269755841631\n",
            "The running loss at 7522 iteration is: 0.1230313033622824\n",
            "The running loss at 7523 iteration is: 0.1226125504512637\n",
            "The running loss at 7524 iteration is: 0.12214661725405643\n",
            "The running loss at 7525 iteration is: 0.12175261923001827\n",
            "The running loss at 7526 iteration is: 0.1213253206461428\n",
            "The running loss at 7527 iteration is: 0.1209280772781355\n",
            "The running loss at 7528 iteration is: 0.120517875621312\n",
            "The running loss at 7529 iteration is: 0.12012810806565617\n",
            "The running loss at 7530 iteration is: 0.1196953186279298\n",
            "The running loss at 7531 iteration is: 0.11931277391031098\n",
            "The running loss at 7532 iteration is: 0.11890649349815603\n",
            "The running loss at 7533 iteration is: 0.11852059356510401\n",
            "The running loss at 7534 iteration is: 0.11811245414434299\n",
            "The running loss at 7535 iteration is: 0.11772377821488633\n",
            "The running loss at 7536 iteration is: 0.1173138457782469\n",
            "The running loss at 7537 iteration is: 0.11694606285090518\n",
            "The running loss at 7538 iteration is: 0.11661913365534352\n",
            "The running loss at 7539 iteration is: 0.11622158933070163\n",
            "The running loss at 7540 iteration is: 0.11589889576279794\n",
            "The running loss at 7541 iteration is: 0.1156211588863379\n",
            "The running loss at 7542 iteration is: 0.11538758468831974\n",
            "The running loss at 7543 iteration is: 0.11519932633029986\n",
            "The running loss at 7544 iteration is: 0.115204724216035\n",
            "The running loss at 7545 iteration is: 0.11549518108966855\n",
            "The running loss at 7546 iteration is: 0.116327009155061\n",
            "The running loss at 7547 iteration is: 0.1182149447415213\n",
            "The running loss at 7548 iteration is: 0.12208048097820022\n",
            "The running loss at 7549 iteration is: 0.1299729045079033\n",
            "The running loss at 7550 iteration is: 0.14581950897217943\n",
            "The running loss at 7551 iteration is: 0.17877512398481746\n",
            "The running loss at 7552 iteration is: 0.24569453627901905\n",
            "The running loss at 7553 iteration is: 0.39074893903825225\n",
            "The running loss at 7554 iteration is: 0.6880591795924255\n",
            "The running loss at 7555 iteration is: 1.3659675819528516\n",
            "The running loss at 7556 iteration is: 2.722255605818013\n",
            "The running loss at 7557 iteration is: 5.99619062031386\n",
            "The running loss at 7558 iteration is: 11.65549716861388\n",
            "The running loss at 7559 iteration is: 25.37853603869514\n",
            "The running loss at 7560 iteration is: 36.62572805709308\n",
            "The running loss at 7561 iteration is: 55.93429641523679\n",
            "The running loss at 7562 iteration is: 28.05954942782732\n",
            "The running loss at 7563 iteration is: 3.590524333542153\n",
            "The running loss at 7564 iteration is: 4.936752777307919\n",
            "The running loss at 7565 iteration is: 18.52085918969506\n",
            "The running loss at 7566 iteration is: 13.639049122684733\n",
            "The running loss at 7567 iteration is: 0.2677135796941378\n",
            "The running loss at 7568 iteration is: 10.74431159755345\n",
            "The running loss at 7569 iteration is: 18.63428234679447\n",
            "The running loss at 7570 iteration is: 3.677267131880659\n",
            "The running loss at 7571 iteration is: 2.7127556089227327\n",
            "The running loss at 7572 iteration is: 13.903830669777651\n",
            "The running loss at 7573 iteration is: 9.761727816761187\n",
            "The running loss at 7574 iteration is: 1.314130078524126\n",
            "The running loss at 7575 iteration is: 2.4028454830137442\n",
            "The running loss at 7576 iteration is: 8.217839198456966\n",
            "The running loss at 7577 iteration is: 7.726217548462453\n",
            "The running loss at 7578 iteration is: 1.2310676508292506\n",
            "The running loss at 7579 iteration is: 1.7165235372595142\n",
            "The running loss at 7580 iteration is: 6.2667748265270715\n",
            "The running loss at 7581 iteration is: 4.365591154921652\n",
            "The running loss at 7582 iteration is: 0.6323643349881776\n",
            "The running loss at 7583 iteration is: 1.4836782391364922\n",
            "The running loss at 7584 iteration is: 3.7659136005403004\n",
            "The running loss at 7585 iteration is: 2.5283393052923153\n",
            "The running loss at 7586 iteration is: 0.33520642684871915\n",
            "The running loss at 7587 iteration is: 1.6514608742405685\n",
            "The running loss at 7588 iteration is: 2.848595682812231\n",
            "The running loss at 7589 iteration is: 1.0072769240487822\n",
            "The running loss at 7590 iteration is: 0.4585503065230242\n",
            "The running loss at 7591 iteration is: 1.815198023936572\n",
            "The running loss at 7592 iteration is: 1.4397550204739167\n",
            "The running loss at 7593 iteration is: 0.3250117928433699\n",
            "The running loss at 7594 iteration is: 0.8178073715189801\n",
            "The running loss at 7595 iteration is: 1.3173576600981138\n",
            "The running loss at 7596 iteration is: 0.5919020491283797\n",
            "The running loss at 7597 iteration is: 0.34901211677754507\n",
            "The running loss at 7598 iteration is: 0.9416723820279056\n",
            "The running loss at 7599 iteration is: 0.7870186435480683\n",
            "The running loss at 7600 iteration is: 0.2746779826947573\n",
            "The running loss at 7601 iteration is: 0.5780789342098238\n",
            "The running loss at 7602 iteration is: 0.779225702674054\n",
            "The running loss at 7603 iteration is: 0.3546163001100288\n",
            "The running loss at 7604 iteration is: 0.34813377436206067\n",
            "The running loss at 7605 iteration is: 0.6347488361987984\n",
            "The running loss at 7606 iteration is: 0.4342180872877007\n",
            "The running loss at 7607 iteration is: 0.2587708069239628\n",
            "The running loss at 7608 iteration is: 0.4606664796712446\n",
            "The running loss at 7609 iteration is: 0.45570665829467677\n",
            "The running loss at 7610 iteration is: 0.26109656953069504\n",
            "The running loss at 7611 iteration is: 0.3272140320251263\n",
            "The running loss at 7612 iteration is: 0.4204963038626439\n",
            "The running loss at 7613 iteration is: 0.29546482117879613\n",
            "The running loss at 7614 iteration is: 0.2573137851013806\n",
            "The running loss at 7615 iteration is: 0.3567736003368428\n",
            "The running loss at 7616 iteration is: 0.31984581544265017\n",
            "The running loss at 7617 iteration is: 0.23902999908754605\n",
            "The running loss at 7618 iteration is: 0.2940829105284067\n",
            "The running loss at 7619 iteration is: 0.31799084269563394\n",
            "The running loss at 7620 iteration is: 0.24721132068585883\n",
            "The running loss at 7621 iteration is: 0.25080360553049\n",
            "The running loss at 7622 iteration is: 0.2948971791272501\n",
            "The running loss at 7623 iteration is: 0.2593429156510619\n",
            "The running loss at 7624 iteration is: 0.23110830578388042\n",
            "The running loss at 7625 iteration is: 0.2637868364423071\n",
            "The running loss at 7626 iteration is: 0.2624474208998904\n",
            "The running loss at 7627 iteration is: 0.22892617813446023\n",
            "The running loss at 7628 iteration is: 0.2381716421817664\n",
            "The running loss at 7629 iteration is: 0.25440334564143835\n",
            "The running loss at 7630 iteration is: 0.23341420985429673\n",
            "The running loss at 7631 iteration is: 0.22381119768880695\n",
            "The running loss at 7632 iteration is: 0.23978391066319607\n",
            "The running loss at 7633 iteration is: 0.23553893112783347\n",
            "The running loss at 7634 iteration is: 0.21962798071069384\n",
            "The running loss at 7635 iteration is: 0.22544156212545713\n",
            "The running loss at 7636 iteration is: 0.23171770629910646\n",
            "The running loss at 7637 iteration is: 0.2202501666512184\n",
            "The running loss at 7638 iteration is: 0.21615482388782403\n",
            "The running loss at 7639 iteration is: 0.22347772135084198\n",
            "The running loss at 7640 iteration is: 0.22039959133772635\n",
            "The running loss at 7641 iteration is: 0.21234166993632642\n",
            "The running loss at 7642 iteration is: 0.21473831894888987\n",
            "The running loss at 7643 iteration is: 0.21736313219843983\n",
            "The running loss at 7644 iteration is: 0.21155753845407618\n",
            "The running loss at 7645 iteration is: 0.20868498982168415\n",
            "The running loss at 7646 iteration is: 0.21178349991391743\n",
            "The running loss at 7647 iteration is: 0.21037711858903174\n",
            "The running loss at 7648 iteration is: 0.20582778829223058\n",
            "The running loss at 7649 iteration is: 0.20607520482335773\n",
            "The running loss at 7650 iteration is: 0.207356744860283\n",
            "The running loss at 7651 iteration is: 0.204384084329885\n",
            "The running loss at 7652 iteration is: 0.20200003875097697\n",
            "The running loss at 7653 iteration is: 0.20287632378450077\n",
            "The running loss at 7654 iteration is: 0.20243552649260158\n",
            "The running loss at 7655 iteration is: 0.19975200356228695\n",
            "The running loss at 7656 iteration is: 0.1987745612065698\n",
            "The running loss at 7657 iteration is: 0.19917121731297135\n",
            "The running loss at 7658 iteration is: 0.1978532639193658\n",
            "The running loss at 7659 iteration is: 0.1959296478942214\n",
            "The running loss at 7660 iteration is: 0.19561038776028794\n",
            "The running loss at 7661 iteration is: 0.19535238082469258\n",
            "The running loss at 7662 iteration is: 0.1938133318382161\n",
            "The running loss at 7663 iteration is: 0.19261613423670623\n",
            "The running loss at 7664 iteration is: 0.19227388614579452\n",
            "The running loss at 7665 iteration is: 0.19160615469151687\n",
            "The running loss at 7666 iteration is: 0.19026805106717476\n",
            "The running loss at 7667 iteration is: 0.1893864309383042\n",
            "The running loss at 7668 iteration is: 0.18899179668404467\n",
            "The running loss at 7669 iteration is: 0.18812944730605793\n",
            "The running loss at 7670 iteration is: 0.18698215139550833\n",
            "The running loss at 7671 iteration is: 0.1862248135433028\n",
            "The running loss at 7672 iteration is: 0.18570853190128508\n",
            "The running loss at 7673 iteration is: 0.18483094633271469\n",
            "The running loss at 7674 iteration is: 0.1838050015600745\n",
            "The running loss at 7675 iteration is: 0.18318310222298817\n",
            "The running loss at 7676 iteration is: 0.18253665062929825\n",
            "The running loss at 7677 iteration is: 0.1816760077696929\n",
            "The running loss at 7678 iteration is: 0.18079005297192244\n",
            "The running loss at 7679 iteration is: 0.18010805937584262\n",
            "The running loss at 7680 iteration is: 0.17947989749115723\n",
            "The running loss at 7681 iteration is: 0.17866024508620862\n",
            "The running loss at 7682 iteration is: 0.1778844670204729\n",
            "The running loss at 7683 iteration is: 0.1771380908537951\n",
            "The running loss at 7684 iteration is: 0.17647435624332103\n",
            "The running loss at 7685 iteration is: 0.17573117837870922\n",
            "The running loss at 7686 iteration is: 0.1749636045140607\n",
            "The running loss at 7687 iteration is: 0.17424747301177604\n",
            "The running loss at 7688 iteration is: 0.17359464531779636\n",
            "The running loss at 7689 iteration is: 0.17285408959045348\n",
            "The running loss at 7690 iteration is: 0.1721284882893266\n",
            "The running loss at 7691 iteration is: 0.17146965708859951\n",
            "The running loss at 7692 iteration is: 0.17078939700357493\n",
            "The running loss at 7693 iteration is: 0.1700920942286381\n",
            "The running loss at 7694 iteration is: 0.16940692968179685\n",
            "The running loss at 7695 iteration is: 0.16872477629636012\n",
            "The running loss at 7696 iteration is: 0.16804627639741654\n",
            "The running loss at 7697 iteration is: 0.1673822965615524\n",
            "The running loss at 7698 iteration is: 0.1666841620392414\n",
            "The running loss at 7699 iteration is: 0.16603385323022904\n",
            "The running loss at 7700 iteration is: 0.1653969168703554\n",
            "The running loss at 7701 iteration is: 0.1647444681554153\n",
            "The running loss at 7702 iteration is: 0.16406049447109353\n",
            "The running loss at 7703 iteration is: 0.16341211526509908\n",
            "The running loss at 7704 iteration is: 0.1627703506857433\n",
            "The running loss at 7705 iteration is: 0.16213802227434376\n",
            "The running loss at 7706 iteration is: 0.16149849460795074\n",
            "The running loss at 7707 iteration is: 0.16086746833280757\n",
            "The running loss at 7708 iteration is: 0.1602242301556654\n",
            "The running loss at 7709 iteration is: 0.15959967860945046\n",
            "The running loss at 7710 iteration is: 0.1589749789419831\n",
            "The running loss at 7711 iteration is: 0.1583619897301191\n",
            "The running loss at 7712 iteration is: 0.15776679748413916\n",
            "The running loss at 7713 iteration is: 0.15713024519439062\n",
            "The running loss at 7714 iteration is: 0.1565294854711287\n",
            "The running loss at 7715 iteration is: 0.15595373418878192\n",
            "The running loss at 7716 iteration is: 0.15531301434204345\n",
            "The running loss at 7717 iteration is: 0.15473053429219272\n",
            "The running loss at 7718 iteration is: 0.15411524313583433\n",
            "The running loss at 7719 iteration is: 0.15356038579765682\n",
            "The running loss at 7720 iteration is: 0.15293342750922273\n",
            "The running loss at 7721 iteration is: 0.15236341802468983\n",
            "The running loss at 7722 iteration is: 0.15175095500478544\n",
            "The running loss at 7723 iteration is: 0.1512037025284432\n",
            "The running loss at 7724 iteration is: 0.15062370000682698\n",
            "The running loss at 7725 iteration is: 0.15004820465175106\n",
            "The running loss at 7726 iteration is: 0.14948021277705928\n",
            "The running loss at 7727 iteration is: 0.1488903911942422\n",
            "The running loss at 7728 iteration is: 0.14831920761403525\n",
            "The running loss at 7729 iteration is: 0.1477817308469816\n",
            "The running loss at 7730 iteration is: 0.14718420629638396\n",
            "The running loss at 7731 iteration is: 0.1466466558628662\n",
            "The running loss at 7732 iteration is: 0.14609739841274547\n",
            "The running loss at 7733 iteration is: 0.14554665229652353\n",
            "The running loss at 7734 iteration is: 0.14502867813408712\n",
            "The running loss at 7735 iteration is: 0.14449006045490972\n",
            "The running loss at 7736 iteration is: 0.14393316119532745\n",
            "The running loss at 7737 iteration is: 0.14338128174338274\n",
            "The running loss at 7738 iteration is: 0.14287082903597556\n",
            "The running loss at 7739 iteration is: 0.14234703667015852\n",
            "The running loss at 7740 iteration is: 0.14178649823283052\n",
            "The running loss at 7741 iteration is: 0.14129072997937017\n",
            "The running loss at 7742 iteration is: 0.1407478443604968\n",
            "The running loss at 7743 iteration is: 0.14025761327036879\n",
            "The running loss at 7744 iteration is: 0.13970283848043324\n",
            "The running loss at 7745 iteration is: 0.13920622547553432\n",
            "The running loss at 7746 iteration is: 0.1386675531642994\n",
            "The running loss at 7747 iteration is: 0.138181650896273\n",
            "The running loss at 7748 iteration is: 0.13765978716424687\n",
            "The running loss at 7749 iteration is: 0.13714234942993567\n",
            "The running loss at 7750 iteration is: 0.13664036859822887\n",
            "The running loss at 7751 iteration is: 0.1361729130537287\n",
            "The running loss at 7752 iteration is: 0.13564924554915733\n",
            "The running loss at 7753 iteration is: 0.13513702930557028\n",
            "The running loss at 7754 iteration is: 0.13466670998180186\n",
            "The running loss at 7755 iteration is: 0.13416175706375336\n",
            "The running loss at 7756 iteration is: 0.13368565474855867\n",
            "The running loss at 7757 iteration is: 0.13320989781076947\n",
            "The running loss at 7758 iteration is: 0.13273714138062723\n",
            "The running loss at 7759 iteration is: 0.13225129906733976\n",
            "The running loss at 7760 iteration is: 0.13176983285607258\n",
            "The running loss at 7761 iteration is: 0.13127618549895187\n",
            "The running loss at 7762 iteration is: 0.13082197866845038\n",
            "The running loss at 7763 iteration is: 0.13034388822124127\n",
            "The running loss at 7764 iteration is: 0.12985700712183282\n",
            "The running loss at 7765 iteration is: 0.12942616873870033\n",
            "The running loss at 7766 iteration is: 0.1289589336385794\n",
            "The running loss at 7767 iteration is: 0.12847816433321893\n",
            "The running loss at 7768 iteration is: 0.1280021374524158\n",
            "The running loss at 7769 iteration is: 0.12759100447634025\n",
            "The running loss at 7770 iteration is: 0.12710085132207477\n",
            "The running loss at 7771 iteration is: 0.12663648770812608\n",
            "The running loss at 7772 iteration is: 0.12620554521998092\n",
            "The running loss at 7773 iteration is: 0.12575756405443622\n",
            "The running loss at 7774 iteration is: 0.12533490188102392\n",
            "The running loss at 7775 iteration is: 0.12487370146417366\n",
            "The running loss at 7776 iteration is: 0.12446448015262734\n",
            "The running loss at 7777 iteration is: 0.12398001720892116\n",
            "The running loss at 7778 iteration is: 0.12356677896283917\n",
            "The running loss at 7779 iteration is: 0.1231137041878513\n",
            "The running loss at 7780 iteration is: 0.1226897101269147\n",
            "The running loss at 7781 iteration is: 0.12228069702913463\n",
            "The running loss at 7782 iteration is: 0.1218121958712667\n",
            "The running loss at 7783 iteration is: 0.12140602925645316\n",
            "The running loss at 7784 iteration is: 0.1209940049445405\n",
            "The running loss at 7785 iteration is: 0.12057518983372391\n",
            "The running loss at 7786 iteration is: 0.12014126113128183\n",
            "The running loss at 7787 iteration is: 0.1197231625464564\n",
            "The running loss at 7788 iteration is: 0.11935072299743046\n",
            "The running loss at 7789 iteration is: 0.11897828623767479\n",
            "The running loss at 7790 iteration is: 0.11861108500415779\n",
            "The running loss at 7791 iteration is: 0.118331813416396\n",
            "The running loss at 7792 iteration is: 0.11807613099747372\n",
            "The running loss at 7793 iteration is: 0.11807063787273472\n",
            "The running loss at 7794 iteration is: 0.1184587431787581\n",
            "The running loss at 7795 iteration is: 0.1197700553908158\n",
            "The running loss at 7796 iteration is: 0.12289537145002558\n",
            "The running loss at 7797 iteration is: 0.1300994582229749\n",
            "The running loss at 7798 iteration is: 0.1460597131074025\n",
            "The running loss at 7799 iteration is: 0.1823350382764857\n",
            "The running loss at 7800 iteration is: 0.2630666384035274\n",
            "The running loss at 7801 iteration is: 0.45251292097009993\n",
            "The running loss at 7802 iteration is: 0.8746614066367305\n",
            "The running loss at 7803 iteration is: 1.9177339939748708\n",
            "The running loss at 7804 iteration is: 4.161191836248115\n",
            "The running loss at 7805 iteration is: 10.026029309954698\n",
            "The running loss at 7806 iteration is: 20.280722522017324\n",
            "The running loss at 7807 iteration is: 46.28799786033089\n",
            "The running loss at 7808 iteration is: 57.56255916769755\n",
            "The running loss at 7809 iteration is: 71.41894884173651\n",
            "The running loss at 7810 iteration is: 16.280993905092213\n",
            "The running loss at 7811 iteration is: 3.288367102652719\n",
            "The running loss at 7812 iteration is: 32.69163196068468\n",
            "The running loss at 7813 iteration is: 20.471626082413334\n",
            "The running loss at 7814 iteration is: 0.9823540124185512\n",
            "The running loss at 7815 iteration is: 9.202438016949943\n",
            "The running loss at 7816 iteration is: 20.51917851151228\n",
            "The running loss at 7817 iteration is: 15.861668619405949\n",
            "The running loss at 7818 iteration is: 1.372189847635909\n",
            "The running loss at 7819 iteration is: 5.071027068256307\n",
            "The running loss at 7820 iteration is: 17.902406807846997\n",
            "The running loss at 7821 iteration is: 14.500441653322568\n",
            "The running loss at 7822 iteration is: 4.936190718531734\n",
            "The running loss at 7823 iteration is: 0.4932877833632281\n",
            "The running loss at 7824 iteration is: 6.441929767513286\n",
            "The running loss at 7825 iteration is: 11.368381392314891\n",
            "The running loss at 7826 iteration is: 4.778212840456462\n",
            "The running loss at 7827 iteration is: 0.34217491278742324\n",
            "The running loss at 7828 iteration is: 3.7979448256412423\n",
            "The running loss at 7829 iteration is: 5.77319281809733\n",
            "The running loss at 7830 iteration is: 2.4561943366572456\n",
            "The running loss at 7831 iteration is: 0.4079616559091454\n",
            "The running loss at 7832 iteration is: 3.1816446575683703\n",
            "The running loss at 7833 iteration is: 3.734645374098344\n",
            "The running loss at 7834 iteration is: 0.6576917826793508\n",
            "The running loss at 7835 iteration is: 1.2088490868364867\n",
            "The running loss at 7836 iteration is: 3.068354869185671\n",
            "The running loss at 7837 iteration is: 1.2539737325778968\n",
            "The running loss at 7838 iteration is: 0.40249063126611057\n",
            "The running loss at 7839 iteration is: 1.91086143681254\n",
            "The running loss at 7840 iteration is: 1.4337232321204512\n",
            "The running loss at 7841 iteration is: 0.2733734883547801\n",
            "The running loss at 7842 iteration is: 1.0148964397032183\n",
            "The running loss at 7843 iteration is: 1.303514149297249\n",
            "The running loss at 7844 iteration is: 0.3993825464129873\n",
            "The running loss at 7845 iteration is: 0.5046622004488105\n",
            "The running loss at 7846 iteration is: 1.0411215403476406\n",
            "The running loss at 7847 iteration is: 0.5496805776594184\n",
            "The running loss at 7848 iteration is: 0.286711361771598\n",
            "The running loss at 7849 iteration is: 0.7596953844422842\n",
            "The running loss at 7850 iteration is: 0.6286287534913386\n",
            "The running loss at 7851 iteration is: 0.2403973889612108\n",
            "The running loss at 7852 iteration is: 0.5195567936433203\n",
            "The running loss at 7853 iteration is: 0.6179573597847507\n",
            "The running loss at 7854 iteration is: 0.27182143313452467\n",
            "The running loss at 7855 iteration is: 0.3500829147939652\n",
            "The running loss at 7856 iteration is: 0.5408081043153941\n",
            "The running loss at 7857 iteration is: 0.31940708731661527\n",
            "The running loss at 7858 iteration is: 0.25577542555483657\n",
            "The running loss at 7859 iteration is: 0.43385229182379476\n",
            "The running loss at 7860 iteration is: 0.3486775947666271\n",
            "The running loss at 7861 iteration is: 0.224204402938181\n",
            "The running loss at 7862 iteration is: 0.3333589822179207\n",
            "The running loss at 7863 iteration is: 0.3485021618262363\n",
            "The running loss at 7864 iteration is: 0.23117263497868965\n",
            "The running loss at 7865 iteration is: 0.2605162015005606\n",
            "The running loss at 7866 iteration is: 0.32199206248331025\n",
            "The running loss at 7867 iteration is: 0.25080890508177944\n",
            "The running loss at 7868 iteration is: 0.22261203997879753\n",
            "The running loss at 7869 iteration is: 0.2820139110797499\n",
            "The running loss at 7870 iteration is: 0.26334284879785297\n",
            "The running loss at 7871 iteration is: 0.2130690217732658\n",
            "The running loss at 7872 iteration is: 0.24337928102983336\n",
            "The running loss at 7873 iteration is: 0.26006833830866394\n",
            "The running loss at 7874 iteration is: 0.2185206806217644\n",
            "The running loss at 7875 iteration is: 0.21725212753069945\n",
            "The running loss at 7876 iteration is: 0.24428570255737708\n",
            "The running loss at 7877 iteration is: 0.22570623978055657\n",
            "The running loss at 7878 iteration is: 0.2061647708013128\n",
            "The running loss at 7879 iteration is: 0.22404342331000346\n",
            "The running loss at 7880 iteration is: 0.22644019469777274\n",
            "The running loss at 7881 iteration is: 0.20593720128893941\n",
            "The running loss at 7882 iteration is: 0.20782396897573743\n",
            "The running loss at 7883 iteration is: 0.21926305604866575\n",
            "The running loss at 7884 iteration is: 0.20897569772718133\n",
            "The running loss at 7885 iteration is: 0.199914723210727\n",
            "The running loss at 7886 iteration is: 0.20825417889242187\n",
            "The running loss at 7887 iteration is: 0.20894529292540792\n",
            "The running loss at 7888 iteration is: 0.19864677495676555\n",
            "The running loss at 7889 iteration is: 0.19883184834736328\n",
            "The running loss at 7890 iteration is: 0.20423686848487604\n",
            "The running loss at 7891 iteration is: 0.19936327973938994\n",
            "The running loss at 7892 iteration is: 0.19394025838014686\n",
            "The running loss at 7893 iteration is: 0.19723521715616013\n",
            "The running loss at 7894 iteration is: 0.198101179766804\n",
            "The running loss at 7895 iteration is: 0.19283450020869403\n",
            "The running loss at 7896 iteration is: 0.1913652334123986\n",
            "The running loss at 7897 iteration is: 0.19388980480299844\n",
            "The running loss at 7898 iteration is: 0.19225529085704302\n",
            "The running loss at 7899 iteration is: 0.1885099476328599\n",
            "The running loss at 7900 iteration is: 0.18884080992174226\n",
            "The running loss at 7901 iteration is: 0.1898426707603204\n",
            "The running loss at 7902 iteration is: 0.18740843219704253\n",
            "The running loss at 7903 iteration is: 0.1853541566219549\n",
            "The running loss at 7904 iteration is: 0.18600468080203222\n",
            "The running loss at 7905 iteration is: 0.18577136788193127\n",
            "The running loss at 7906 iteration is: 0.18359684712330912\n",
            "The running loss at 7907 iteration is: 0.1824651988713628\n",
            "The running loss at 7908 iteration is: 0.18281667867154835\n",
            "The running loss at 7909 iteration is: 0.1820297653941254\n",
            "The running loss at 7910 iteration is: 0.18031249842770045\n",
            "The running loss at 7911 iteration is: 0.17966304888351906\n",
            "The running loss at 7912 iteration is: 0.17968935224713056\n",
            "The running loss at 7913 iteration is: 0.1787505197731207\n",
            "The running loss at 7914 iteration is: 0.1773617325375695\n",
            "The running loss at 7915 iteration is: 0.17683903167394813\n",
            "The running loss at 7916 iteration is: 0.17655590341553026\n",
            "The running loss at 7917 iteration is: 0.1756571309874909\n",
            "The running loss at 7918 iteration is: 0.1745022200620069\n",
            "The running loss at 7919 iteration is: 0.17400987869583284\n",
            "The running loss at 7920 iteration is: 0.17361585807568514\n",
            "The running loss at 7921 iteration is: 0.17273706905945466\n",
            "The running loss at 7922 iteration is: 0.17179157031540077\n",
            "The running loss at 7923 iteration is: 0.17124787257109195\n",
            "The running loss at 7924 iteration is: 0.1707593142748811\n",
            "The running loss at 7925 iteration is: 0.16995780587161535\n",
            "The running loss at 7926 iteration is: 0.16912994083144403\n",
            "The running loss at 7927 iteration is: 0.16854671002245022\n",
            "The running loss at 7928 iteration is: 0.16801648321580304\n",
            "The running loss at 7929 iteration is: 0.1673282479709235\n",
            "The running loss at 7930 iteration is: 0.1665189605715162\n",
            "The running loss at 7931 iteration is: 0.16589696808326915\n",
            "The running loss at 7932 iteration is: 0.16538437766943104\n",
            "The running loss at 7933 iteration is: 0.16469992485321755\n",
            "The running loss at 7934 iteration is: 0.16401117921208133\n",
            "The running loss at 7935 iteration is: 0.1633605366683032\n",
            "The running loss at 7936 iteration is: 0.1628073800355077\n",
            "The running loss at 7937 iteration is: 0.16217794808013283\n",
            "The running loss at 7938 iteration is: 0.16149693573064003\n",
            "The running loss at 7939 iteration is: 0.16088497810297384\n",
            "The running loss at 7940 iteration is: 0.1602696185066518\n",
            "The running loss at 7941 iteration is: 0.15971361127769348\n",
            "The running loss at 7942 iteration is: 0.15910475497689958\n",
            "The running loss at 7943 iteration is: 0.15844536117045463\n",
            "The running loss at 7944 iteration is: 0.15783263803234024\n",
            "The running loss at 7945 iteration is: 0.15729020055056062\n",
            "The running loss at 7946 iteration is: 0.1567098493545269\n",
            "The running loss at 7947 iteration is: 0.15608812299799443\n",
            "The running loss at 7948 iteration is: 0.15548013879911973\n",
            "The running loss at 7949 iteration is: 0.15488722695052357\n",
            "The running loss at 7950 iteration is: 0.15431784065498644\n",
            "The running loss at 7951 iteration is: 0.15376004345758917\n",
            "The running loss at 7952 iteration is: 0.15317596202334116\n",
            "The running loss at 7953 iteration is: 0.15259652248179492\n",
            "The running loss at 7954 iteration is: 0.15203965056266205\n",
            "The running loss at 7955 iteration is: 0.15148728257548302\n",
            "The running loss at 7956 iteration is: 0.15090937598762594\n",
            "The running loss at 7957 iteration is: 0.15034299864404138\n",
            "The running loss at 7958 iteration is: 0.14977162386832554\n",
            "The running loss at 7959 iteration is: 0.14921476132195913\n",
            "The running loss at 7960 iteration is: 0.14866643515263675\n",
            "The running loss at 7961 iteration is: 0.14813882921882485\n",
            "The running loss at 7962 iteration is: 0.1475706793233375\n",
            "The running loss at 7963 iteration is: 0.14703533193748972\n",
            "The running loss at 7964 iteration is: 0.14647665373122765\n",
            "The running loss at 7965 iteration is: 0.1459609618975733\n",
            "The running loss at 7966 iteration is: 0.14541920379154513\n",
            "The running loss at 7967 iteration is: 0.1449122867682009\n",
            "The running loss at 7968 iteration is: 0.14434539361669396\n",
            "The running loss at 7969 iteration is: 0.14385549701908706\n",
            "The running loss at 7970 iteration is: 0.14331331734969543\n",
            "The running loss at 7971 iteration is: 0.14281041211778991\n",
            "The running loss at 7972 iteration is: 0.14226633920702259\n",
            "The running loss at 7973 iteration is: 0.14176924731916413\n",
            "The running loss at 7974 iteration is: 0.14126387828093517\n",
            "The running loss at 7975 iteration is: 0.1407139380264689\n",
            "The running loss at 7976 iteration is: 0.14020245954699742\n",
            "The running loss at 7977 iteration is: 0.13972456342510192\n",
            "The running loss at 7978 iteration is: 0.13923619692193245\n",
            "The running loss at 7979 iteration is: 0.13868058032491856\n",
            "The running loss at 7980 iteration is: 0.13820531199686242\n",
            "The running loss at 7981 iteration is: 0.1377293833014945\n",
            "The running loss at 7982 iteration is: 0.1372175352761224\n",
            "The running loss at 7983 iteration is: 0.13674948858017963\n",
            "The running loss at 7984 iteration is: 0.13623702057679746\n",
            "The running loss at 7985 iteration is: 0.13578210813148486\n",
            "The running loss at 7986 iteration is: 0.13526090884696898\n",
            "The running loss at 7987 iteration is: 0.13478728887931088\n",
            "The running loss at 7988 iteration is: 0.13430021035049416\n",
            "The running loss at 7989 iteration is: 0.13382629475194194\n",
            "The running loss at 7990 iteration is: 0.13338143130952015\n",
            "The running loss at 7991 iteration is: 0.13288831733158077\n",
            "The running loss at 7992 iteration is: 0.1324290283269287\n",
            "The running loss at 7993 iteration is: 0.1319435542428556\n",
            "The running loss at 7994 iteration is: 0.13152999198150184\n",
            "The running loss at 7995 iteration is: 0.13103164734207695\n",
            "The running loss at 7996 iteration is: 0.13057399122200866\n",
            "The running loss at 7997 iteration is: 0.13014199228043374\n",
            "The running loss at 7998 iteration is: 0.12967246598573123\n",
            "The running loss at 7999 iteration is: 0.1292253615868773\n",
            "The running loss at 8000 iteration is: 0.1287710954516915\n",
            "The running loss at 8001 iteration is: 0.12832002483927066\n",
            "The running loss at 8002 iteration is: 0.1278528260284845\n",
            "The running loss at 8003 iteration is: 0.12741787404256327\n",
            "The running loss at 8004 iteration is: 0.12698700986777314\n",
            "The running loss at 8005 iteration is: 0.12654560082378358\n",
            "The running loss at 8006 iteration is: 0.12608916283753122\n",
            "The running loss at 8007 iteration is: 0.12563963974707448\n",
            "The running loss at 8008 iteration is: 0.12524243650799496\n",
            "The running loss at 8009 iteration is: 0.12481767975486414\n",
            "The running loss at 8010 iteration is: 0.1243507289146136\n",
            "The running loss at 8011 iteration is: 0.12393942100116057\n",
            "The running loss at 8012 iteration is: 0.1235297942535255\n",
            "The running loss at 8013 iteration is: 0.12311600008985726\n",
            "The running loss at 8014 iteration is: 0.12269612644262705\n",
            "The running loss at 8015 iteration is: 0.12224488396189692\n",
            "The running loss at 8016 iteration is: 0.12181029859831446\n",
            "The running loss at 8017 iteration is: 0.12139324388089291\n",
            "The running loss at 8018 iteration is: 0.12099672689821385\n",
            "The running loss at 8019 iteration is: 0.12058542041079785\n",
            "The running loss at 8020 iteration is: 0.12017837809887792\n",
            "The running loss at 8021 iteration is: 0.11977322389178477\n",
            "The running loss at 8022 iteration is: 0.11936604992832239\n",
            "The running loss at 8023 iteration is: 0.11901791401711943\n",
            "The running loss at 8024 iteration is: 0.11859893682126804\n",
            "The running loss at 8025 iteration is: 0.11826904296340104\n",
            "The running loss at 8026 iteration is: 0.11794321433254264\n",
            "The running loss at 8027 iteration is: 0.11763409043450569\n",
            "The running loss at 8028 iteration is: 0.11749042066003318\n",
            "The running loss at 8029 iteration is: 0.11753131761865722\n",
            "The running loss at 8030 iteration is: 0.11793713080927797\n",
            "The running loss at 8031 iteration is: 0.11909767674421937\n",
            "The running loss at 8032 iteration is: 0.12175873346525334\n",
            "The running loss at 8033 iteration is: 0.12740939207993193\n",
            "The running loss at 8034 iteration is: 0.13945481846419988\n",
            "The running loss at 8035 iteration is: 0.1644946994794826\n",
            "The running loss at 8036 iteration is: 0.218553880350375\n",
            "The running loss at 8037 iteration is: 0.33225159372644864\n",
            "The running loss at 8038 iteration is: 0.5890422414577122\n",
            "The running loss at 8039 iteration is: 1.1299069250982925\n",
            "The running loss at 8040 iteration is: 2.4227219199488332\n",
            "The running loss at 8041 iteration is: 5.007809451346326\n",
            "The running loss at 8042 iteration is: 11.520463393432843\n",
            "The running loss at 8043 iteration is: 21.552269712555805\n",
            "The running loss at 8044 iteration is: 45.493533162396766\n",
            "The running loss at 8045 iteration is: 50.17726944701165\n",
            "The running loss at 8046 iteration is: 52.68333624434973\n",
            "The running loss at 8047 iteration is: 9.935761309678561\n",
            "The running loss at 8048 iteration is: 4.353058989562949\n",
            "The running loss at 8049 iteration is: 28.48138935024335\n",
            "The running loss at 8050 iteration is: 15.114164786170429\n",
            "The running loss at 8051 iteration is: 0.2932925581181719\n",
            "The running loss at 8052 iteration is: 10.535502368761295\n",
            "The running loss at 8053 iteration is: 15.654382852907395\n",
            "The running loss at 8054 iteration is: 6.709224502320354\n",
            "The running loss at 8055 iteration is: 0.46916142305214953\n",
            "The running loss at 8056 iteration is: 8.53864619520515\n",
            "The running loss at 8057 iteration is: 15.255674421466098\n",
            "The running loss at 8058 iteration is: 6.424806010261833\n",
            "The running loss at 8059 iteration is: 0.386307220446822\n",
            "The running loss at 8060 iteration is: 4.1653715583867115\n",
            "The running loss at 8061 iteration is: 8.57972318947945\n",
            "The running loss at 8062 iteration is: 7.003084634865526\n",
            "The running loss at 8063 iteration is: 1.186919318504341\n",
            "The running loss at 8064 iteration is: 1.417143637257621\n",
            "The running loss at 8065 iteration is: 5.472185616086627\n",
            "The running loss at 8066 iteration is: 4.551437289839106\n",
            "The running loss at 8067 iteration is: 1.0936833404692836\n",
            "The running loss at 8068 iteration is: 0.8053509949588645\n",
            "The running loss at 8069 iteration is: 3.1503320784794\n",
            "The running loss at 8070 iteration is: 3.0599049939210836\n",
            "The running loss at 8071 iteration is: 0.629542838733019\n",
            "The running loss at 8072 iteration is: 0.9260578149219664\n",
            "The running loss at 8073 iteration is: 2.5226633110303474\n",
            "The running loss at 8074 iteration is: 1.5052695183035547\n",
            "The running loss at 8075 iteration is: 0.30870170394075896\n",
            "The running loss at 8076 iteration is: 1.146885809599938\n",
            "The running loss at 8077 iteration is: 1.5862554723725255\n",
            "The running loss at 8078 iteration is: 0.627979701294486\n",
            "The running loss at 8079 iteration is: 0.4035553536769636\n",
            "The running loss at 8080 iteration is: 1.127217975577148\n",
            "The running loss at 8081 iteration is: 0.9353290764424945\n",
            "The running loss at 8082 iteration is: 0.29503037745526106\n",
            "The running loss at 8083 iteration is: 0.6374586140571104\n",
            "The running loss at 8084 iteration is: 0.9269314319070837\n",
            "The running loss at 8085 iteration is: 0.4222416408499404\n",
            "The running loss at 8086 iteration is: 0.3490994135352442\n",
            "The running loss at 8087 iteration is: 0.7132920687280961\n",
            "The running loss at 8088 iteration is: 0.5267558588008161\n",
            "The running loss at 8089 iteration is: 0.26590975297911157\n",
            "The running loss at 8090 iteration is: 0.4778529127781436\n",
            "The running loss at 8091 iteration is: 0.534415519471303\n",
            "The running loss at 8092 iteration is: 0.2966121596046435\n",
            "The running loss at 8093 iteration is: 0.3205581777858456\n",
            "The running loss at 8094 iteration is: 0.4666702693529049\n",
            "The running loss at 8095 iteration is: 0.34938396393626653\n",
            "The running loss at 8096 iteration is: 0.25681888564493066\n",
            "The running loss at 8097 iteration is: 0.372632765064463\n",
            "The running loss at 8098 iteration is: 0.3711777030826644\n",
            "The running loss at 8099 iteration is: 0.25542577835923386\n",
            "The running loss at 8100 iteration is: 0.29416428296945285\n",
            "The running loss at 8101 iteration is: 0.3520784905489503\n",
            "The running loss at 8102 iteration is: 0.27554570855124894\n",
            "The running loss at 8103 iteration is: 0.25066992963949986\n",
            "The running loss at 8104 iteration is: 0.3099827450737378\n",
            "The running loss at 8105 iteration is: 0.2885801425536867\n",
            "The running loss at 8106 iteration is: 0.2392393652033768\n",
            "The running loss at 8107 iteration is: 0.2681539491264561\n",
            "The running loss at 8108 iteration is: 0.28450036745357166\n",
            "The running loss at 8109 iteration is: 0.24475924579370129\n",
            "The running loss at 8110 iteration is: 0.24094893012349208\n",
            "The running loss at 8111 iteration is: 0.26719387323234145\n",
            "The running loss at 8112 iteration is: 0.25166877826531475\n",
            "The running loss at 8113 iteration is: 0.23046366062361764\n",
            "The running loss at 8114 iteration is: 0.2463584560902675\n",
            "The running loss at 8115 iteration is: 0.25096888259461864\n",
            "The running loss at 8116 iteration is: 0.23059374563199256\n",
            "The running loss at 8117 iteration is: 0.2305543197653765\n",
            "The running loss at 8118 iteration is: 0.24235147128557907\n",
            "The running loss at 8119 iteration is: 0.23289358661495024\n",
            "The running loss at 8120 iteration is: 0.22288111132146668\n",
            "The running loss at 8121 iteration is: 0.2306295633767015\n",
            "The running loss at 8122 iteration is: 0.23172515599263144\n",
            "The running loss at 8123 iteration is: 0.22134124925220713\n",
            "The running loss at 8124 iteration is: 0.22105622799954144\n",
            "The running loss at 8125 iteration is: 0.22634890035457098\n",
            "The running loss at 8126 iteration is: 0.22149100699517146\n",
            "The running loss at 8127 iteration is: 0.21583065947363497\n",
            "The running loss at 8128 iteration is: 0.21878991413001436\n",
            "The running loss at 8129 iteration is: 0.21932697308685362\n",
            "The running loss at 8130 iteration is: 0.21394434657383293\n",
            "The running loss at 8131 iteration is: 0.2127215691526958\n",
            "The running loss at 8132 iteration is: 0.21493597422859592\n",
            "The running loss at 8133 iteration is: 0.2126706098710578\n",
            "The running loss at 8134 iteration is: 0.20918548357425723\n",
            "The running loss at 8135 iteration is: 0.2098312827589523\n",
            "The running loss at 8136 iteration is: 0.21014936511710602\n",
            "The running loss at 8137 iteration is: 0.2072600989445948\n",
            "The running loss at 8138 iteration is: 0.20569927327277154\n",
            "The running loss at 8139 iteration is: 0.2063153965676494\n",
            "The running loss at 8140 iteration is: 0.20532270085721535\n",
            "The running loss at 8141 iteration is: 0.20302358074578686\n",
            "The running loss at 8142 iteration is: 0.20238951859577295\n",
            "The running loss at 8143 iteration is: 0.20247395747266483\n",
            "The running loss at 8144 iteration is: 0.20101130552615562\n",
            "The running loss at 8145 iteration is: 0.19935217343427816\n",
            "The running loss at 8146 iteration is: 0.19908433487418445\n",
            "The running loss at 8147 iteration is: 0.1986383453766317\n",
            "The running loss at 8148 iteration is: 0.19714489636928148\n",
            "The running loss at 8149 iteration is: 0.19604822256967325\n",
            "The running loss at 8150 iteration is: 0.19567733877900073\n",
            "The running loss at 8151 iteration is: 0.19495904322145566\n",
            "The running loss at 8152 iteration is: 0.19366387013971365\n",
            "The running loss at 8153 iteration is: 0.19275313570098987\n",
            "The running loss at 8154 iteration is: 0.19233595547171614\n",
            "The running loss at 8155 iteration is: 0.19143661897842845\n",
            "The running loss at 8156 iteration is: 0.19034323553711816\n",
            "The running loss at 8157 iteration is: 0.18959284520985756\n",
            "The running loss at 8158 iteration is: 0.18903041808920895\n",
            "The running loss at 8159 iteration is: 0.18810076948731042\n",
            "The running loss at 8160 iteration is: 0.18717370607958922\n",
            "The running loss at 8161 iteration is: 0.18645625460850945\n",
            "The running loss at 8162 iteration is: 0.18578565404980868\n",
            "The running loss at 8163 iteration is: 0.18492970392320907\n",
            "The running loss at 8164 iteration is: 0.1841197212436318\n",
            "The running loss at 8165 iteration is: 0.18339450317907038\n",
            "The running loss at 8166 iteration is: 0.18270478828304923\n",
            "The running loss at 8167 iteration is: 0.18185075402811007\n",
            "The running loss at 8168 iteration is: 0.18108153414815123\n",
            "The running loss at 8169 iteration is: 0.18041756137061288\n",
            "The running loss at 8170 iteration is: 0.17974121887619118\n",
            "The running loss at 8171 iteration is: 0.1789087652860267\n",
            "The running loss at 8172 iteration is: 0.17816062915911737\n",
            "The running loss at 8173 iteration is: 0.17748847655859049\n",
            "The running loss at 8174 iteration is: 0.1767788301537994\n",
            "The running loss at 8175 iteration is: 0.1760422125736777\n",
            "The running loss at 8176 iteration is: 0.17529113972239052\n",
            "The running loss at 8177 iteration is: 0.17461421239715025\n",
            "The running loss at 8178 iteration is: 0.17397024733372157\n",
            "The running loss at 8179 iteration is: 0.17326112353489642\n",
            "The running loss at 8180 iteration is: 0.17252094556426287\n",
            "The running loss at 8181 iteration is: 0.17181968459305508\n",
            "The running loss at 8182 iteration is: 0.17115188827636363\n",
            "The running loss at 8183 iteration is: 0.1704891893950759\n",
            "The running loss at 8184 iteration is: 0.16981368509723868\n",
            "The running loss at 8185 iteration is: 0.1691305426747604\n",
            "The running loss at 8186 iteration is: 0.16844772401251312\n",
            "The running loss at 8187 iteration is: 0.1677777144412495\n",
            "The running loss at 8188 iteration is: 0.16712532883432157\n",
            "The running loss at 8189 iteration is: 0.16647540831449614\n",
            "The running loss at 8190 iteration is: 0.16583675997676042\n",
            "The running loss at 8191 iteration is: 0.16515448761883103\n",
            "The running loss at 8192 iteration is: 0.16452862195474222\n",
            "The running loss at 8193 iteration is: 0.16384895394882354\n",
            "The running loss at 8194 iteration is: 0.16322821784333225\n",
            "The running loss at 8195 iteration is: 0.1625899052425951\n",
            "The running loss at 8196 iteration is: 0.1619689005701988\n",
            "The running loss at 8197 iteration is: 0.16130049067129645\n",
            "The running loss at 8198 iteration is: 0.1607111667049347\n",
            "The running loss at 8199 iteration is: 0.1600843413134786\n",
            "The running loss at 8200 iteration is: 0.15944379509685894\n",
            "The running loss at 8201 iteration is: 0.15885591732672114\n",
            "The running loss at 8202 iteration is: 0.15821323539308668\n",
            "The running loss at 8203 iteration is: 0.15762187655557086\n",
            "The running loss at 8204 iteration is: 0.1569972752274275\n",
            "The running loss at 8205 iteration is: 0.15641701071001787\n",
            "The running loss at 8206 iteration is: 0.15578897465423328\n",
            "The running loss at 8207 iteration is: 0.15520678949996836\n",
            "The running loss at 8208 iteration is: 0.15464055491269735\n",
            "The running loss at 8209 iteration is: 0.15406630136176483\n",
            "The running loss at 8210 iteration is: 0.1534390284328216\n",
            "The running loss at 8211 iteration is: 0.152855373992265\n",
            "The running loss at 8212 iteration is: 0.15228518111550318\n",
            "The running loss at 8213 iteration is: 0.1516946506360195\n",
            "The running loss at 8214 iteration is: 0.15113371036442483\n",
            "The running loss at 8215 iteration is: 0.15056845461657703\n",
            "The running loss at 8216 iteration is: 0.15000671616185582\n",
            "The running loss at 8217 iteration is: 0.14945301140928063\n",
            "The running loss at 8218 iteration is: 0.14886419135262827\n",
            "The running loss at 8219 iteration is: 0.14828744580213776\n",
            "The running loss at 8220 iteration is: 0.14773680720502197\n",
            "The running loss at 8221 iteration is: 0.14721282863918045\n",
            "The running loss at 8222 iteration is: 0.14664294775398679\n",
            "The running loss at 8223 iteration is: 0.1460984983136898\n",
            "The running loss at 8224 iteration is: 0.14557151826371878\n",
            "The running loss at 8225 iteration is: 0.14500985834397812\n",
            "The running loss at 8226 iteration is: 0.14447909140823625\n",
            "The running loss at 8227 iteration is: 0.14395779966730965\n",
            "The running loss at 8228 iteration is: 0.143404588668963\n",
            "The running loss at 8229 iteration is: 0.14290627673911263\n",
            "The running loss at 8230 iteration is: 0.14233347175332392\n",
            "The running loss at 8231 iteration is: 0.14184070761767084\n",
            "The running loss at 8232 iteration is: 0.14129054455090587\n",
            "The running loss at 8233 iteration is: 0.14079418180605122\n",
            "The running loss at 8234 iteration is: 0.1402918739366031\n",
            "The running loss at 8235 iteration is: 0.1397298842169303\n",
            "The running loss at 8236 iteration is: 0.1392251513028389\n",
            "The running loss at 8237 iteration is: 0.1387320416048025\n",
            "The running loss at 8238 iteration is: 0.13822338374431792\n",
            "The running loss at 8239 iteration is: 0.13773043269211996\n",
            "The running loss at 8240 iteration is: 0.13720757950876017\n",
            "The running loss at 8241 iteration is: 0.13671043240945913\n",
            "The running loss at 8242 iteration is: 0.13620629125017863\n",
            "The running loss at 8243 iteration is: 0.13570511214834086\n",
            "The running loss at 8244 iteration is: 0.13521831097311396\n",
            "The running loss at 8245 iteration is: 0.13472342782423039\n",
            "The running loss at 8246 iteration is: 0.13425704937170838\n",
            "The running loss at 8247 iteration is: 0.13379031682393328\n",
            "The running loss at 8248 iteration is: 0.13329552380048834\n",
            "The running loss at 8249 iteration is: 0.13281841818268236\n",
            "The running loss at 8250 iteration is: 0.1323407668374669\n",
            "The running loss at 8251 iteration is: 0.1318502484231988\n",
            "The running loss at 8252 iteration is: 0.1314008227058066\n",
            "The running loss at 8253 iteration is: 0.13089968578766548\n",
            "The running loss at 8254 iteration is: 0.1304564477919288\n",
            "The running loss at 8255 iteration is: 0.13002386173254754\n",
            "The running loss at 8256 iteration is: 0.1295364448837118\n",
            "The running loss at 8257 iteration is: 0.12906325366153001\n",
            "The running loss at 8258 iteration is: 0.1286252199882837\n",
            "The running loss at 8259 iteration is: 0.12814123287447027\n",
            "The running loss at 8260 iteration is: 0.127716918324611\n",
            "The running loss at 8261 iteration is: 0.1272424901082355\n",
            "The running loss at 8262 iteration is: 0.12679297974172307\n",
            "The running loss at 8263 iteration is: 0.12633294290385685\n",
            "The running loss at 8264 iteration is: 0.1258937208659549\n",
            "The running loss at 8265 iteration is: 0.1254796622182028\n",
            "The running loss at 8266 iteration is: 0.12501860645704632\n",
            "The running loss at 8267 iteration is: 0.12461028923857038\n",
            "The running loss at 8268 iteration is: 0.12417106266600124\n",
            "The running loss at 8269 iteration is: 0.12373345735271524\n",
            "The running loss at 8270 iteration is: 0.12332707506932018\n",
            "The running loss at 8271 iteration is: 0.12295361517377014\n",
            "The running loss at 8272 iteration is: 0.12258534351175672\n",
            "The running loss at 8273 iteration is: 0.12226663368990028\n",
            "The running loss at 8274 iteration is: 0.12201174529249686\n",
            "The running loss at 8275 iteration is: 0.12190562618247085\n",
            "The running loss at 8276 iteration is: 0.12201364418938923\n",
            "The running loss at 8277 iteration is: 0.12260373285015566\n",
            "The running loss at 8278 iteration is: 0.12397554633713236\n",
            "The running loss at 8279 iteration is: 0.12704694875944553\n",
            "The running loss at 8280 iteration is: 0.13324875219206112\n",
            "The running loss at 8281 iteration is: 0.14605696005014399\n",
            "The running loss at 8282 iteration is: 0.1715886430962864\n",
            "The running loss at 8283 iteration is: 0.2248629301487006\n",
            "The running loss at 8284 iteration is: 0.3325573312652115\n",
            "The running loss at 8285 iteration is: 0.5675675322478801\n",
            "The running loss at 8286 iteration is: 1.0451620264831\n",
            "The running loss at 8287 iteration is: 2.148458316966476\n",
            "The running loss at 8288 iteration is: 4.291592418082768\n",
            "The running loss at 8289 iteration is: 9.526520412973296\n",
            "The running loss at 8290 iteration is: 17.69531095977395\n",
            "The running loss at 8291 iteration is: 37.201057694960696\n",
            "The running loss at 8292 iteration is: 45.444053192358574\n",
            "The running loss at 8293 iteration is: 56.661113543839036\n",
            "The running loss at 8294 iteration is: 18.003290170739582\n",
            "The running loss at 8295 iteration is: 0.24938675780272945\n",
            "The running loss at 8296 iteration is: 15.801150349052177\n",
            "The running loss at 8297 iteration is: 19.706381979663064\n",
            "The running loss at 8298 iteration is: 6.481216638898219\n",
            "The running loss at 8299 iteration is: 1.2231725172306969\n",
            "The running loss at 8300 iteration is: 12.978873209224416\n",
            "The running loss at 8301 iteration is: 17.273387325017207\n",
            "The running loss at 8302 iteration is: 3.644746873773709\n",
            "The running loss at 8303 iteration is: 1.7771429057100134\n",
            "The running loss at 8304 iteration is: 11.480606114143612\n",
            "The running loss at 8305 iteration is: 11.592433787978704\n",
            "The running loss at 8306 iteration is: 4.684302540235548\n",
            "The running loss at 8307 iteration is: 0.3631621305426587\n",
            "The running loss at 8308 iteration is: 4.779827512721459\n",
            "The running loss at 8309 iteration is: 9.344305995556306\n",
            "The running loss at 8310 iteration is: 4.880457460344644\n",
            "The running loss at 8311 iteration is: 0.5173630724951142\n",
            "The running loss at 8312 iteration is: 2.0431514218067766\n",
            "The running loss at 8313 iteration is: 4.947078334749093\n",
            "The running loss at 8314 iteration is: 3.8933042553502175\n",
            "The running loss at 8315 iteration is: 0.6033215364624681\n",
            "The running loss at 8316 iteration is: 1.3076458054598552\n",
            "The running loss at 8317 iteration is: 3.5648601949308\n",
            "The running loss at 8318 iteration is: 2.1946055655121333\n",
            "The running loss at 8319 iteration is: 0.34814683132396074\n",
            "The running loss at 8320 iteration is: 1.2316103399660083\n",
            "The running loss at 8321 iteration is: 2.2105467280864053\n",
            "The running loss at 8322 iteration is: 1.1528090116215453\n",
            "The running loss at 8323 iteration is: 0.3029518773265882\n",
            "The running loss at 8324 iteration is: 1.2254515625694566\n",
            "The running loss at 8325 iteration is: 1.5461731818624926\n",
            "The running loss at 8326 iteration is: 0.5063115026385183\n",
            "The running loss at 8327 iteration is: 0.4628253623072964\n",
            "The running loss at 8328 iteration is: 1.174932399482903\n",
            "The running loss at 8329 iteration is: 0.8183181880022877\n",
            "The running loss at 8330 iteration is: 0.2735489239266277\n",
            "The running loss at 8331 iteration is: 0.62511047092371\n",
            "The running loss at 8332 iteration is: 0.8370138793585873\n",
            "The running loss at 8333 iteration is: 0.41092971648474663\n",
            "The running loss at 8334 iteration is: 0.31102211039992506\n",
            "The running loss at 8335 iteration is: 0.6336192743145405\n",
            "The running loss at 8336 iteration is: 0.5491514271982227\n",
            "The running loss at 8337 iteration is: 0.26291727596724007\n",
            "The running loss at 8338 iteration is: 0.4013774476118577\n",
            "The running loss at 8339 iteration is: 0.5428264950759656\n",
            "The running loss at 8340 iteration is: 0.333680638571601\n",
            "The running loss at 8341 iteration is: 0.2677659358725355\n",
            "The running loss at 8342 iteration is: 0.4280194322340726\n",
            "The running loss at 8343 iteration is: 0.38783963403455357\n",
            "The running loss at 8344 iteration is: 0.24876612650126387\n",
            "The running loss at 8345 iteration is: 0.3069852080064751\n",
            "The running loss at 8346 iteration is: 0.3750758652391889\n",
            "The running loss at 8347 iteration is: 0.2846205232904733\n",
            "The running loss at 8348 iteration is: 0.24386861651040365\n",
            "The running loss at 8349 iteration is: 0.3170340539557917\n",
            "The running loss at 8350 iteration is: 0.31039726854398864\n",
            "The running loss at 8351 iteration is: 0.23928938925815316\n",
            "The running loss at 8352 iteration is: 0.2586246683661986\n",
            "The running loss at 8353 iteration is: 0.2984423339640839\n",
            "The running loss at 8354 iteration is: 0.25784923133883925\n",
            "The running loss at 8355 iteration is: 0.23027148965011238\n",
            "The running loss at 8356 iteration is: 0.26327922173677415\n",
            "The running loss at 8357 iteration is: 0.2660655532927677\n",
            "The running loss at 8358 iteration is: 0.2310329306571719\n",
            "The running loss at 8359 iteration is: 0.23283045583176748\n",
            "The running loss at 8360 iteration is: 0.25418974012758655\n",
            "The running loss at 8361 iteration is: 0.24017548399354308\n",
            "The running loss at 8362 iteration is: 0.22139604613006408\n",
            "The running loss at 8363 iteration is: 0.23333120794844736\n",
            "The running loss at 8364 iteration is: 0.2401077038500501\n",
            "The running loss at 8365 iteration is: 0.2237366922592082\n",
            "The running loss at 8366 iteration is: 0.218688836363222\n",
            "The running loss at 8367 iteration is: 0.22921652703388035\n",
            "The running loss at 8368 iteration is: 0.2264256083669595\n",
            "The running loss at 8369 iteration is: 0.21504601049095326\n",
            "The running loss at 8370 iteration is: 0.21673330614709324\n",
            "The running loss at 8371 iteration is: 0.2222256829686996\n",
            "The running loss at 8372 iteration is: 0.21639377867782764\n",
            "The running loss at 8373 iteration is: 0.21029908096325017\n",
            "The running loss at 8374 iteration is: 0.2135614553798774\n",
            "The running loss at 8375 iteration is: 0.2149130659547201\n",
            "The running loss at 8376 iteration is: 0.20935943990177425\n",
            "The running loss at 8377 iteration is: 0.20683588275111944\n",
            "The running loss at 8378 iteration is: 0.20937386807963915\n",
            "The running loss at 8379 iteration is: 0.20857131293081294\n",
            "The running loss at 8380 iteration is: 0.20432648593007469\n",
            "The running loss at 8381 iteration is: 0.20349496258889585\n",
            "The running loss at 8382 iteration is: 0.204888896900063\n",
            "The running loss at 8383 iteration is: 0.20324083395359485\n",
            "The running loss at 8384 iteration is: 0.20032884737651002\n",
            "The running loss at 8385 iteration is: 0.20003842676543215\n",
            "The running loss at 8386 iteration is: 0.20052112620040205\n",
            "The running loss at 8387 iteration is: 0.19875714877535214\n",
            "The running loss at 8388 iteration is: 0.19669110314786917\n",
            "The running loss at 8389 iteration is: 0.1965328516580287\n",
            "The running loss at 8390 iteration is: 0.19642711952379083\n",
            "The running loss at 8391 iteration is: 0.19483034933374263\n",
            "The running loss at 8392 iteration is: 0.19329115682262504\n",
            "The running loss at 8393 iteration is: 0.19304563192945207\n",
            "The running loss at 8394 iteration is: 0.19257765180340758\n",
            "The running loss at 8395 iteration is: 0.19123258853695832\n",
            "The running loss at 8396 iteration is: 0.19003824084190424\n",
            "The running loss at 8397 iteration is: 0.18959329778941975\n",
            "The running loss at 8398 iteration is: 0.18905729822123593\n",
            "The running loss at 8399 iteration is: 0.18784656604901895\n",
            "The running loss at 8400 iteration is: 0.1868355640323021\n",
            "The running loss at 8401 iteration is: 0.18627728702844332\n",
            "The running loss at 8402 iteration is: 0.18568676899245637\n",
            "The running loss at 8403 iteration is: 0.18465496168113893\n",
            "The running loss at 8404 iteration is: 0.18367291422630574\n",
            "The running loss at 8405 iteration is: 0.18308226353654425\n",
            "The running loss at 8406 iteration is: 0.18246960513350538\n",
            "The running loss at 8407 iteration is: 0.1815418386130758\n",
            "The running loss at 8408 iteration is: 0.1806255275851194\n",
            "The running loss at 8409 iteration is: 0.17994977201455944\n",
            "The running loss at 8410 iteration is: 0.1793321999767863\n",
            "The running loss at 8411 iteration is: 0.17850453587168771\n",
            "The running loss at 8412 iteration is: 0.1776686998744681\n",
            "The running loss at 8413 iteration is: 0.17689755422563036\n",
            "The running loss at 8414 iteration is: 0.1763136303947579\n",
            "The running loss at 8415 iteration is: 0.17554077844715707\n",
            "The running loss at 8416 iteration is: 0.1747666590040313\n",
            "The running loss at 8417 iteration is: 0.17401766678207498\n",
            "The running loss at 8418 iteration is: 0.17334438260527052\n",
            "The running loss at 8419 iteration is: 0.1726644514072999\n",
            "The running loss at 8420 iteration is: 0.17195105605002775\n",
            "The running loss at 8421 iteration is: 0.17119183820711067\n",
            "The running loss at 8422 iteration is: 0.17051915919501007\n",
            "The running loss at 8423 iteration is: 0.16983180217107405\n",
            "The running loss at 8424 iteration is: 0.16915098391863845\n",
            "The running loss at 8425 iteration is: 0.16841097616159703\n",
            "The running loss at 8426 iteration is: 0.16776142277922035\n",
            "The running loss at 8427 iteration is: 0.16711045957302254\n",
            "The running loss at 8428 iteration is: 0.16642348091647333\n",
            "The running loss at 8429 iteration is: 0.16574579615353013\n",
            "The running loss at 8430 iteration is: 0.16507108550066926\n",
            "The running loss at 8431 iteration is: 0.16441129394821227\n",
            "The running loss at 8432 iteration is: 0.16378442581444635\n",
            "The running loss at 8433 iteration is: 0.16307280284392425\n",
            "The running loss at 8434 iteration is: 0.1624488683872454\n",
            "The running loss at 8435 iteration is: 0.1618271807319561\n",
            "The running loss at 8436 iteration is: 0.16114820400775612\n",
            "The running loss at 8437 iteration is: 0.1605357417372085\n",
            "The running loss at 8438 iteration is: 0.1598583777437706\n",
            "The running loss at 8439 iteration is: 0.15919457424295586\n",
            "The running loss at 8440 iteration is: 0.15862413937833575\n",
            "The running loss at 8441 iteration is: 0.15798812136126564\n",
            "The running loss at 8442 iteration is: 0.15733503352970277\n",
            "The running loss at 8443 iteration is: 0.15674249761802383\n",
            "The running loss at 8444 iteration is: 0.15609640417736267\n",
            "The running loss at 8445 iteration is: 0.15550237532703437\n",
            "The running loss at 8446 iteration is: 0.1548848957727036\n",
            "The running loss at 8447 iteration is: 0.15429075225500263\n",
            "The running loss at 8448 iteration is: 0.15370487378775344\n",
            "The running loss at 8449 iteration is: 0.15308627714280387\n",
            "The running loss at 8450 iteration is: 0.1524946953565465\n",
            "The running loss at 8451 iteration is: 0.1519169587960454\n",
            "The running loss at 8452 iteration is: 0.1513377196552381\n",
            "The running loss at 8453 iteration is: 0.15077521970707017\n",
            "The running loss at 8454 iteration is: 0.15019718108947386\n",
            "The running loss at 8455 iteration is: 0.1495420223613585\n",
            "The running loss at 8456 iteration is: 0.1489797021597559\n",
            "The running loss at 8457 iteration is: 0.14839408032066093\n",
            "The running loss at 8458 iteration is: 0.14784827262766967\n",
            "The running loss at 8459 iteration is: 0.14731156263830733\n",
            "The running loss at 8460 iteration is: 0.14675094655752305\n",
            "The running loss at 8461 iteration is: 0.14618454371218473\n",
            "The running loss at 8462 iteration is: 0.14559590368945066\n",
            "The running loss at 8463 iteration is: 0.1450480713845896\n",
            "The running loss at 8464 iteration is: 0.1445387416772168\n",
            "The running loss at 8465 iteration is: 0.14396078610643276\n",
            "The running loss at 8466 iteration is: 0.14339436520801993\n",
            "The running loss at 8467 iteration is: 0.14287164594569496\n",
            "The running loss at 8468 iteration is: 0.14230121034042775\n",
            "The running loss at 8469 iteration is: 0.14178216823215103\n",
            "The running loss at 8470 iteration is: 0.14126940607650146\n",
            "The running loss at 8471 iteration is: 0.1407143668152711\n",
            "The running loss at 8472 iteration is: 0.14018645662413917\n",
            "The running loss at 8473 iteration is: 0.13966950406222517\n",
            "The running loss at 8474 iteration is: 0.13916615576321512\n",
            "The running loss at 8475 iteration is: 0.1386210449755286\n",
            "The running loss at 8476 iteration is: 0.13811870319043212\n",
            "The running loss at 8477 iteration is: 0.13757888262081505\n",
            "The running loss at 8478 iteration is: 0.13709079770881347\n",
            "The running loss at 8479 iteration is: 0.13654748100130565\n",
            "The running loss at 8480 iteration is: 0.1360303364761502\n",
            "The running loss at 8481 iteration is: 0.13555445767045507\n",
            "The running loss at 8482 iteration is: 0.1350448941964332\n",
            "The running loss at 8483 iteration is: 0.1345561365118049\n",
            "The running loss at 8484 iteration is: 0.1340394522306765\n",
            "The running loss at 8485 iteration is: 0.13356011260681325\n",
            "The running loss at 8486 iteration is: 0.13304540339613236\n",
            "The running loss at 8487 iteration is: 0.13257679458389002\n",
            "The running loss at 8488 iteration is: 0.13207384313694076\n",
            "The running loss at 8489 iteration is: 0.13160610371982331\n",
            "The running loss at 8490 iteration is: 0.13114628073578644\n",
            "The running loss at 8491 iteration is: 0.1306694185490449\n",
            "The running loss at 8492 iteration is: 0.1301581583711679\n",
            "The running loss at 8493 iteration is: 0.12967214604428973\n",
            "The running loss at 8494 iteration is: 0.1292367938521279\n",
            "The running loss at 8495 iteration is: 0.12878410514031985\n",
            "The running loss at 8496 iteration is: 0.12828697198229194\n",
            "The running loss at 8497 iteration is: 0.1278671096740233\n",
            "The running loss at 8498 iteration is: 0.1273900858366252\n",
            "The running loss at 8499 iteration is: 0.12695764082229066\n",
            "The running loss at 8500 iteration is: 0.12654607904772427\n",
            "The running loss at 8501 iteration is: 0.1261559867962295\n",
            "The running loss at 8502 iteration is: 0.12578657784641653\n",
            "The running loss at 8503 iteration is: 0.12551845223140767\n",
            "The running loss at 8504 iteration is: 0.12538101223400783\n",
            "The running loss at 8505 iteration is: 0.125307034620965\n",
            "The running loss at 8506 iteration is: 0.1255501276488429\n",
            "The running loss at 8507 iteration is: 0.12627807528649396\n",
            "The running loss at 8508 iteration is: 0.12790712149855138\n",
            "The running loss at 8509 iteration is: 0.1311828113835027\n",
            "The running loss at 8510 iteration is: 0.1376186327933772\n",
            "The running loss at 8511 iteration is: 0.15039969710973466\n",
            "The running loss at 8512 iteration is: 0.17529692628234764\n",
            "The running loss at 8513 iteration is: 0.22603998609725062\n",
            "The running loss at 8514 iteration is: 0.3261380185177115\n",
            "The running loss at 8515 iteration is: 0.5394552666728393\n",
            "The running loss at 8516 iteration is: 0.9620806525233727\n",
            "The running loss at 8517 iteration is: 1.9141170596003354\n",
            "The running loss at 8518 iteration is: 3.7275124809977767\n",
            "The running loss at 8519 iteration is: 8.05662750798409\n",
            "The running loss at 8520 iteration is: 14.882758366200777\n",
            "The running loss at 8521 iteration is: 31.104998576219575\n",
            "The running loss at 8522 iteration is: 40.55880200219857\n",
            "The running loss at 8523 iteration is: 55.80466520600633\n",
            "The running loss at 8524 iteration is: 23.71957162782143\n",
            "The running loss at 8525 iteration is: 1.4367666169555955\n",
            "The running loss at 8526 iteration is: 7.461305891486079\n",
            "The running loss at 8527 iteration is: 19.498886361344848\n",
            "The running loss at 8528 iteration is: 14.282798981049279\n",
            "The running loss at 8529 iteration is: 0.4914536842419698\n",
            "The running loss at 8530 iteration is: 8.391591146861463\n",
            "The running loss at 8531 iteration is: 19.567515221646158\n",
            "The running loss at 8532 iteration is: 8.149005419069354\n",
            "The running loss at 8533 iteration is: 0.31513612810806935\n",
            "The running loss at 8534 iteration is: 6.327508812562452\n",
            "The running loss at 8535 iteration is: 11.588212567736363\n",
            "The running loss at 8536 iteration is: 8.644456259501084\n",
            "The running loss at 8537 iteration is: 1.1047065106623253\n",
            "The running loss at 8538 iteration is: 2.1593075238878234\n",
            "The running loss at 8539 iteration is: 7.8912714372292685\n",
            "The running loss at 8540 iteration is: 6.5418373514461585\n",
            "The running loss at 8541 iteration is: 1.8038389225082172\n",
            "The running loss at 8542 iteration is: 0.6444866681381293\n",
            "The running loss at 8543 iteration is: 3.847123882146311\n",
            "The running loss at 8544 iteration is: 4.996841749796381\n",
            "The running loss at 8545 iteration is: 1.5811148082064292\n",
            "The running loss at 8546 iteration is: 0.47692440222302873\n",
            "The running loss at 8547 iteration is: 2.6799697761411037\n",
            "The running loss at 8548 iteration is: 2.8717166778436063\n",
            "The running loss at 8549 iteration is: 0.9066930700392483\n",
            "The running loss at 8550 iteration is: 0.49053061993709673\n",
            "The running loss at 8551 iteration is: 1.893975588816901\n",
            "The running loss at 8552 iteration is: 1.8773639431713063\n",
            "The running loss at 8553 iteration is: 0.46027271039558587\n",
            "The running loss at 8554 iteration is: 0.6529312198505194\n",
            "The running loss at 8555 iteration is: 1.5846569818827485\n",
            "The running loss at 8556 iteration is: 0.9856944065766227\n",
            "The running loss at 8557 iteration is: 0.2753732998648938\n",
            "The running loss at 8558 iteration is: 0.7740269651799478\n",
            "The running loss at 8559 iteration is: 1.0676071224275865\n",
            "The running loss at 8560 iteration is: 0.49950919240978764\n",
            "The running loss at 8561 iteration is: 0.3104076913283283\n",
            "The running loss at 8562 iteration is: 0.7511285660617129\n",
            "The running loss at 8563 iteration is: 0.7141861229578216\n",
            "The running loss at 8564 iteration is: 0.2904973311853143\n",
            "The running loss at 8565 iteration is: 0.40387149155364754\n",
            "The running loss at 8566 iteration is: 0.6585531875098192\n",
            "The running loss at 8567 iteration is: 0.42562166236378235\n",
            "The running loss at 8568 iteration is: 0.25336901163415704\n",
            "The running loss at 8569 iteration is: 0.4490625124200098\n",
            "The running loss at 8570 iteration is: 0.4829636585255608\n",
            "The running loss at 8571 iteration is: 0.2828427339848263\n",
            "The running loss at 8572 iteration is: 0.2830024543110902\n",
            "The running loss at 8573 iteration is: 0.4174631380899034\n",
            "The running loss at 8574 iteration is: 0.3535380666046689\n",
            "The running loss at 8575 iteration is: 0.2395921541991984\n",
            "The running loss at 8576 iteration is: 0.3080664834834146\n",
            "The running loss at 8577 iteration is: 0.36220260183192976\n",
            "The running loss at 8578 iteration is: 0.2714351373227727\n",
            "The running loss at 8579 iteration is: 0.24009587452044973\n",
            "The running loss at 8580 iteration is: 0.3079601551195106\n",
            "The running loss at 8581 iteration is: 0.2994767913000699\n",
            "The running loss at 8582 iteration is: 0.2352518120034813\n",
            "The running loss at 8583 iteration is: 0.24827996589760684\n",
            "The running loss at 8584 iteration is: 0.2868074528974255\n",
            "The running loss at 8585 iteration is: 0.25722624904169883\n",
            "The running loss at 8586 iteration is: 0.2246276381035912\n",
            "The running loss at 8587 iteration is: 0.2495456604601957\n",
            "The running loss at 8588 iteration is: 0.26284956699130774\n",
            "The running loss at 8589 iteration is: 0.23219633112555596\n",
            "The running loss at 8590 iteration is: 0.22262623592590364\n",
            "The running loss at 8591 iteration is: 0.24361987639938948\n",
            "The running loss at 8592 iteration is: 0.24082657477365002\n",
            "The running loss at 8593 iteration is: 0.21934430812408298\n",
            "The running loss at 8594 iteration is: 0.221327375137614\n",
            "The running loss at 8595 iteration is: 0.23375634075246512\n",
            "The running loss at 8596 iteration is: 0.2255245084805051\n",
            "The running loss at 8597 iteration is: 0.21296246496807963\n",
            "The running loss at 8598 iteration is: 0.21832334061401096\n",
            "The running loss at 8599 iteration is: 0.22399954414462026\n",
            "The running loss at 8600 iteration is: 0.21525100934172425\n",
            "The running loss at 8601 iteration is: 0.20891942064728405\n",
            "The running loss at 8602 iteration is: 0.21392108935243154\n",
            "The running loss at 8603 iteration is: 0.21529556779044184\n",
            "The running loss at 8604 iteration is: 0.20827407386791696\n",
            "The running loss at 8605 iteration is: 0.20542176149322394\n",
            "The running loss at 8606 iteration is: 0.20895768634988596\n",
            "The running loss at 8607 iteration is: 0.20841786845672783\n",
            "The running loss at 8608 iteration is: 0.2032211543265035\n",
            "The running loss at 8609 iteration is: 0.20190681253587686\n",
            "The running loss at 8610 iteration is: 0.20399932684620362\n",
            "The running loss at 8611 iteration is: 0.2027339716366739\n",
            "The running loss at 8612 iteration is: 0.1990652073430087\n",
            "The running loss at 8613 iteration is: 0.19824445701002338\n",
            "The running loss at 8614 iteration is: 0.19932078878814266\n",
            "The running loss at 8615 iteration is: 0.1980210536811619\n",
            "The running loss at 8616 iteration is: 0.19528920852059048\n",
            "The running loss at 8617 iteration is: 0.19461948025033998\n",
            "The running loss at 8618 iteration is: 0.19494692230336783\n",
            "The running loss at 8619 iteration is: 0.19380789421765623\n",
            "The running loss at 8620 iteration is: 0.1917653727462516\n",
            "The running loss at 8621 iteration is: 0.19104833082111566\n",
            "The running loss at 8622 iteration is: 0.19104326388636997\n",
            "The running loss at 8623 iteration is: 0.19000658278880717\n",
            "The running loss at 8624 iteration is: 0.18843696642743307\n",
            "The running loss at 8625 iteration is: 0.18763067346744078\n",
            "The running loss at 8626 iteration is: 0.18736248756726984\n",
            "The running loss at 8627 iteration is: 0.1865028417028804\n",
            "The running loss at 8628 iteration is: 0.18515778718970705\n",
            "The running loss at 8629 iteration is: 0.18434255096947635\n",
            "The running loss at 8630 iteration is: 0.18389457121728883\n",
            "The running loss at 8631 iteration is: 0.18318545110689288\n",
            "The running loss at 8632 iteration is: 0.18203785285639845\n",
            "The running loss at 8633 iteration is: 0.1811054798453679\n",
            "The running loss at 8634 iteration is: 0.18049215234337063\n",
            "The running loss at 8635 iteration is: 0.1798956292718761\n",
            "The running loss at 8636 iteration is: 0.1790000605156616\n",
            "The running loss at 8637 iteration is: 0.17806275730872081\n",
            "The running loss at 8638 iteration is: 0.17731722749849463\n",
            "The running loss at 8639 iteration is: 0.17675624818496594\n",
            "The running loss at 8640 iteration is: 0.17597347313987535\n",
            "The running loss at 8641 iteration is: 0.17509681809558308\n",
            "The running loss at 8642 iteration is: 0.1743091343650285\n",
            "The running loss at 8643 iteration is: 0.17366845976329637\n",
            "The running loss at 8644 iteration is: 0.17303836019161195\n",
            "The running loss at 8645 iteration is: 0.17221790788185234\n",
            "The running loss at 8646 iteration is: 0.17139329251450128\n",
            "The running loss at 8647 iteration is: 0.17067190733399878\n",
            "The running loss at 8648 iteration is: 0.17006979906419098\n",
            "The running loss at 8649 iteration is: 0.16936517150586688\n",
            "The running loss at 8650 iteration is: 0.16860181871236213\n",
            "The running loss at 8651 iteration is: 0.16786297666640823\n",
            "The running loss at 8652 iteration is: 0.16713795433059184\n",
            "The running loss at 8653 iteration is: 0.16651447449859724\n",
            "The running loss at 8654 iteration is: 0.16581419390178842\n",
            "The running loss at 8655 iteration is: 0.1651326940902357\n",
            "The running loss at 8656 iteration is: 0.164420111714537\n",
            "The running loss at 8657 iteration is: 0.1637818396368891\n",
            "The running loss at 8658 iteration is: 0.16312555602589227\n",
            "The running loss at 8659 iteration is: 0.16245630710914302\n",
            "The running loss at 8660 iteration is: 0.1617546433777668\n",
            "The running loss at 8661 iteration is: 0.16112433610903246\n",
            "The running loss at 8662 iteration is: 0.16044967051191336\n",
            "The running loss at 8663 iteration is: 0.15982386531393178\n",
            "The running loss at 8664 iteration is: 0.15912531309006933\n",
            "The running loss at 8665 iteration is: 0.1584984016984954\n",
            "The running loss at 8666 iteration is: 0.1578329845583706\n",
            "The running loss at 8667 iteration is: 0.15721330261110322\n",
            "The running loss at 8668 iteration is: 0.1566155955586691\n",
            "The running loss at 8669 iteration is: 0.1559300091390435\n",
            "The running loss at 8670 iteration is: 0.15529624607563253\n",
            "The running loss at 8671 iteration is: 0.15470152857900274\n",
            "The running loss at 8672 iteration is: 0.15406551673726404\n",
            "The running loss at 8673 iteration is: 0.15346738462816442\n",
            "The running loss at 8674 iteration is: 0.15282295136988566\n",
            "The running loss at 8675 iteration is: 0.15222721166299147\n",
            "The running loss at 8676 iteration is: 0.15164230562458852\n",
            "The running loss at 8677 iteration is: 0.15104046576174976\n",
            "The running loss at 8678 iteration is: 0.15044057748931886\n",
            "The running loss at 8679 iteration is: 0.14985524269711464\n",
            "The running loss at 8680 iteration is: 0.14923743702440298\n",
            "The running loss at 8681 iteration is: 0.14866795184158538\n",
            "The running loss at 8682 iteration is: 0.14808043778082577\n",
            "The running loss at 8683 iteration is: 0.14749150510090098\n",
            "The running loss at 8684 iteration is: 0.14687658318719507\n",
            "The running loss at 8685 iteration is: 0.1463428141044326\n",
            "The running loss at 8686 iteration is: 0.1457778583852901\n",
            "The running loss at 8687 iteration is: 0.14517780217779186\n",
            "The running loss at 8688 iteration is: 0.14463215855959302\n",
            "The running loss at 8689 iteration is: 0.1440506164635597\n",
            "The running loss at 8690 iteration is: 0.14347234559212466\n",
            "The running loss at 8691 iteration is: 0.14291003924970724\n",
            "The running loss at 8692 iteration is: 0.1423888013861076\n",
            "The running loss at 8693 iteration is: 0.14181413976110777\n",
            "The running loss at 8694 iteration is: 0.14129160159537976\n",
            "The running loss at 8695 iteration is: 0.140713615027711\n",
            "The running loss at 8696 iteration is: 0.14015715439352855\n",
            "The running loss at 8697 iteration is: 0.13965080240155267\n",
            "The running loss at 8698 iteration is: 0.13908084540390306\n",
            "The running loss at 8699 iteration is: 0.13856679106531125\n",
            "The running loss at 8700 iteration is: 0.13804791477583\n",
            "The running loss at 8701 iteration is: 0.13751014047148932\n",
            "The running loss at 8702 iteration is: 0.1369795547783852\n",
            "The running loss at 8703 iteration is: 0.13647904531667543\n",
            "The running loss at 8704 iteration is: 0.13591112825369744\n",
            "The running loss at 8705 iteration is: 0.13541594978714\n",
            "The running loss at 8706 iteration is: 0.1349247327478339\n",
            "The running loss at 8707 iteration is: 0.1343953751104037\n",
            "The running loss at 8708 iteration is: 0.13389278939461124\n",
            "The running loss at 8709 iteration is: 0.13339530399649194\n",
            "The running loss at 8710 iteration is: 0.13288126918030838\n",
            "The running loss at 8711 iteration is: 0.13233846718673067\n",
            "The running loss at 8712 iteration is: 0.13187203216260898\n",
            "The running loss at 8713 iteration is: 0.1313747212858743\n",
            "The running loss at 8714 iteration is: 0.1308972387074594\n",
            "The running loss at 8715 iteration is: 0.13036702480148962\n",
            "The running loss at 8716 iteration is: 0.12990615664556834\n",
            "The running loss at 8717 iteration is: 0.12940885211018846\n",
            "The running loss at 8718 iteration is: 0.1289151535059666\n",
            "The running loss at 8719 iteration is: 0.12846503699140335\n",
            "The running loss at 8720 iteration is: 0.12800908844407527\n",
            "The running loss at 8721 iteration is: 0.12750931167259658\n",
            "The running loss at 8722 iteration is: 0.12706895786343936\n",
            "The running loss at 8723 iteration is: 0.12666889156674374\n",
            "The running loss at 8724 iteration is: 0.1263024349215748\n",
            "The running loss at 8725 iteration is: 0.12600858857953928\n",
            "The running loss at 8726 iteration is: 0.12586586734906988\n",
            "The running loss at 8727 iteration is: 0.1259508322964276\n",
            "The running loss at 8728 iteration is: 0.1264632189807253\n",
            "The running loss at 8729 iteration is: 0.12783619665795484\n",
            "The running loss at 8730 iteration is: 0.13087962847520282\n",
            "The running loss at 8731 iteration is: 0.1371967236311119\n",
            "The running loss at 8732 iteration is: 0.15025130091467376\n",
            "The running loss at 8733 iteration is: 0.17663609118369644\n",
            "The running loss at 8734 iteration is: 0.23234290883513103\n",
            "The running loss at 8735 iteration is: 0.3466002522395193\n",
            "The running loss at 8736 iteration is: 0.5991424689750617\n",
            "The running loss at 8737 iteration is: 1.11800703493701\n",
            "The running loss at 8738 iteration is: 2.331296075260324\n",
            "The running loss at 8739 iteration is: 4.7076201332133705\n",
            "The running loss at 8740 iteration is: 10.585216786572786\n",
            "The running loss at 8741 iteration is: 19.675758069230138\n",
            "The running loss at 8742 iteration is: 41.4061196118485\n",
            "The running loss at 8743 iteration is: 48.45023754160644\n",
            "The running loss at 8744 iteration is: 56.62624949577479\n",
            "The running loss at 8745 iteration is: 15.024020528916953\n",
            "The running loss at 8746 iteration is: 1.0756376806195838\n",
            "The running loss at 8747 iteration is: 21.06758725117571\n",
            "The running loss at 8748 iteration is: 20.409279091111983\n",
            "The running loss at 8749 iteration is: 4.921829205117142\n",
            "The running loss at 8750 iteration is: 2.1097183001596616\n",
            "The running loss at 8751 iteration is: 14.634023751784792\n",
            "The running loss at 8752 iteration is: 19.20139322319197\n",
            "The running loss at 8753 iteration is: 4.890067498141519\n",
            "The running loss at 8754 iteration is: 1.0064674351249243\n",
            "The running loss at 8755 iteration is: 9.953245982374296\n",
            "The running loss at 8756 iteration is: 13.085029998378731\n",
            "The running loss at 8757 iteration is: 8.231235704106536\n",
            "The running loss at 8758 iteration is: 0.8351726120792391\n",
            "The running loss at 8759 iteration is: 2.6713531540337567\n",
            "The running loss at 8760 iteration is: 8.574485444709559\n",
            "The running loss at 8761 iteration is: 6.595956085496038\n",
            "The running loss at 8762 iteration is: 1.5676124693047542\n",
            "The running loss at 8763 iteration is: 0.8454655504510495\n",
            "The running loss at 8764 iteration is: 4.231294006399105\n",
            "The running loss at 8765 iteration is: 4.80847043853522\n",
            "The running loss at 8766 iteration is: 1.156678737807298\n",
            "The running loss at 8767 iteration is: 0.7818035209615688\n",
            "The running loss at 8768 iteration is: 3.233632834839147\n",
            "The running loss at 8769 iteration is: 2.518211876891458\n",
            "The running loss at 8770 iteration is: 0.4572168348021753\n",
            "The running loss at 8771 iteration is: 0.9742788669844095\n",
            "The running loss at 8772 iteration is: 2.1750713897932643\n",
            "The running loss at 8773 iteration is: 1.2822827543362314\n",
            "The running loss at 8774 iteration is: 0.27939692128825133\n",
            "The running loss at 8775 iteration is: 1.1736223383884412\n",
            "The running loss at 8776 iteration is: 1.5666658943940688\n",
            "The running loss at 8777 iteration is: 0.5027715935475912\n",
            "The running loss at 8778 iteration is: 0.45803485058846305\n",
            "The running loss at 8779 iteration is: 1.1899074412619493\n",
            "The running loss at 8780 iteration is: 0.7925288778777829\n",
            "The running loss at 8781 iteration is: 0.257213345249185\n",
            "The running loss at 8782 iteration is: 0.6555745171679038\n",
            "The running loss at 8783 iteration is: 0.8272317190170961\n",
            "The running loss at 8784 iteration is: 0.3701462618196911\n",
            "The running loss at 8785 iteration is: 0.3218715708276112\n",
            "The running loss at 8786 iteration is: 0.6488630546029148\n",
            "The running loss at 8787 iteration is: 0.5143754111985864\n",
            "The running loss at 8788 iteration is: 0.24371524259225597\n",
            "The running loss at 8789 iteration is: 0.41840331526433094\n",
            "The running loss at 8790 iteration is: 0.5328971852813815\n",
            "The running loss at 8791 iteration is: 0.30396433354676167\n",
            "The running loss at 8792 iteration is: 0.26749369945102297\n",
            "The running loss at 8793 iteration is: 0.4331307401959864\n",
            "The running loss at 8794 iteration is: 0.3674228116021809\n",
            "The running loss at 8795 iteration is: 0.2328214162532572\n",
            "The running loss at 8796 iteration is: 0.30841665029175847\n",
            "The running loss at 8797 iteration is: 0.36677224526323243\n",
            "The running loss at 8798 iteration is: 0.2666247729335494\n",
            "The running loss at 8799 iteration is: 0.23503392514147725\n",
            "The running loss at 8800 iteration is: 0.31181549013119136\n",
            "The running loss at 8801 iteration is: 0.29807309183538266\n",
            "The running loss at 8802 iteration is: 0.22620228741173223\n",
            "The running loss at 8803 iteration is: 0.24976361182268778\n",
            "The running loss at 8804 iteration is: 0.2898440556534792\n",
            "The running loss at 8805 iteration is: 0.24707845167367745\n",
            "The running loss at 8806 iteration is: 0.21860521546455586\n",
            "The running loss at 8807 iteration is: 0.2527373252160845\n",
            "The running loss at 8808 iteration is: 0.25726713801168855\n",
            "The running loss at 8809 iteration is: 0.22108748408157233\n",
            "The running loss at 8810 iteration is: 0.22031457268134705\n",
            "The running loss at 8811 iteration is: 0.24342095111040665\n",
            "The running loss at 8812 iteration is: 0.23220676666887696\n",
            "The running loss at 8813 iteration is: 0.2106487227748421\n",
            "The running loss at 8814 iteration is: 0.22039146189607148\n",
            "The running loss at 8815 iteration is: 0.23097795519911887\n",
            "The running loss at 8816 iteration is: 0.21602686503490934\n",
            "The running loss at 8817 iteration is: 0.2066719483069118\n",
            "The running loss at 8818 iteration is: 0.2167553319172164\n",
            "The running loss at 8819 iteration is: 0.21836895408127027\n",
            "The running loss at 8820 iteration is: 0.20633669315236133\n",
            "The running loss at 8821 iteration is: 0.20400195207142857\n",
            "The running loss at 8822 iteration is: 0.2110622990861983\n",
            "The running loss at 8823 iteration is: 0.20883506007849795\n",
            "The running loss at 8824 iteration is: 0.200552502588224\n",
            "The running loss at 8825 iteration is: 0.20096124216786285\n",
            "The running loss at 8826 iteration is: 0.20499789845814445\n",
            "The running loss at 8827 iteration is: 0.20160410057523961\n",
            "The running loss at 8828 iteration is: 0.19639259979899573\n",
            "The running loss at 8829 iteration is: 0.1973731489011718\n",
            "The running loss at 8830 iteration is: 0.1993397482733052\n",
            "The running loss at 8831 iteration is: 0.19622606120869973\n",
            "The running loss at 8832 iteration is: 0.19279949754821885\n",
            "The running loss at 8833 iteration is: 0.19353324758733478\n",
            "The running loss at 8834 iteration is: 0.19431352986821362\n",
            "The running loss at 8835 iteration is: 0.19173382185641782\n",
            "The running loss at 8836 iteration is: 0.18935804260490202\n",
            "The running loss at 8837 iteration is: 0.18979215034108285\n",
            "The running loss at 8838 iteration is: 0.190003354828524\n",
            "The running loss at 8839 iteration is: 0.18790785361721535\n",
            "The running loss at 8840 iteration is: 0.18606487568500776\n",
            "The running loss at 8841 iteration is: 0.18607686305726914\n",
            "The running loss at 8842 iteration is: 0.18603500964382155\n",
            "The running loss at 8843 iteration is: 0.18449532590638099\n",
            "The running loss at 8844 iteration is: 0.18289652821835367\n",
            "The running loss at 8845 iteration is: 0.182550212118366\n",
            "The running loss at 8846 iteration is: 0.18234248293299854\n",
            "The running loss at 8847 iteration is: 0.18112972087714668\n",
            "The running loss at 8848 iteration is: 0.17979729708364883\n",
            "The running loss at 8849 iteration is: 0.1792406010684596\n",
            "The running loss at 8850 iteration is: 0.17890377876494495\n",
            "The running loss at 8851 iteration is: 0.17797390948127365\n",
            "The running loss at 8852 iteration is: 0.1768009708229163\n",
            "The running loss at 8853 iteration is: 0.17604750775672626\n",
            "The running loss at 8854 iteration is: 0.175625082814432\n",
            "The running loss at 8855 iteration is: 0.17486243080874617\n",
            "The running loss at 8856 iteration is: 0.17384121193395702\n",
            "The running loss at 8857 iteration is: 0.17302207959972035\n",
            "The running loss at 8858 iteration is: 0.17246973456934295\n",
            "The running loss at 8859 iteration is: 0.17186389411473213\n",
            "The running loss at 8860 iteration is: 0.1710231777365563\n",
            "The running loss at 8861 iteration is: 0.1701238113386631\n",
            "The running loss at 8862 iteration is: 0.16949836860648404\n",
            "The running loss at 8863 iteration is: 0.16886802188499575\n",
            "The running loss at 8864 iteration is: 0.1681654027219663\n",
            "The running loss at 8865 iteration is: 0.16737018046410387\n",
            "The running loss at 8866 iteration is: 0.1666319034200024\n",
            "The running loss at 8867 iteration is: 0.16600943671551502\n",
            "The running loss at 8868 iteration is: 0.16535476615125294\n",
            "The running loss at 8869 iteration is: 0.16465037591068776\n",
            "The running loss at 8870 iteration is: 0.1639153841102851\n",
            "The running loss at 8871 iteration is: 0.1632477719911964\n",
            "The running loss at 8872 iteration is: 0.16261713066468408\n",
            "The running loss at 8873 iteration is: 0.16193550436159537\n",
            "The running loss at 8874 iteration is: 0.16126631342794315\n",
            "The running loss at 8875 iteration is: 0.16057968077808352\n",
            "The running loss at 8876 iteration is: 0.15995164827758832\n",
            "The running loss at 8877 iteration is: 0.15929498470681694\n",
            "The running loss at 8878 iteration is: 0.15868744742686178\n",
            "The running loss at 8879 iteration is: 0.1580062202374714\n",
            "The running loss at 8880 iteration is: 0.15737161944229086\n",
            "The running loss at 8881 iteration is: 0.15670345875675137\n",
            "The running loss at 8882 iteration is: 0.15604439154453506\n",
            "The running loss at 8883 iteration is: 0.1554814671067506\n",
            "The running loss at 8884 iteration is: 0.15483473949816368\n",
            "The running loss at 8885 iteration is: 0.15421350177312804\n",
            "The running loss at 8886 iteration is: 0.15356096990441193\n",
            "The running loss at 8887 iteration is: 0.15297481212053973\n",
            "The running loss at 8888 iteration is: 0.15238624964848782\n",
            "The running loss at 8889 iteration is: 0.15174653824740425\n",
            "The running loss at 8890 iteration is: 0.15113203833738428\n",
            "The running loss at 8891 iteration is: 0.1505332569165228\n",
            "The running loss at 8892 iteration is: 0.14993641935858262\n",
            "The running loss at 8893 iteration is: 0.14933348563379983\n",
            "The running loss at 8894 iteration is: 0.14872153197806837\n",
            "The running loss at 8895 iteration is: 0.14816340758496097\n",
            "The running loss at 8896 iteration is: 0.14755999829750682\n",
            "The running loss at 8897 iteration is: 0.14697373946034814\n",
            "The running loss at 8898 iteration is: 0.14636989925191302\n",
            "The running loss at 8899 iteration is: 0.1458030232685095\n",
            "The running loss at 8900 iteration is: 0.14526945878756659\n",
            "The running loss at 8901 iteration is: 0.14470258401036626\n",
            "The running loss at 8902 iteration is: 0.144140184361886\n",
            "The running loss at 8903 iteration is: 0.143544880654479\n",
            "The running loss at 8904 iteration is: 0.14300847693548244\n",
            "The running loss at 8905 iteration is: 0.14244634936889938\n",
            "The running loss at 8906 iteration is: 0.14187786319339052\n",
            "The running loss at 8907 iteration is: 0.14134292470870555\n",
            "The running loss at 8908 iteration is: 0.1407786589566757\n",
            "The running loss at 8909 iteration is: 0.14025069528971393\n",
            "The running loss at 8910 iteration is: 0.13968390771578046\n",
            "The running loss at 8911 iteration is: 0.1391632631529932\n",
            "The running loss at 8912 iteration is: 0.13861532215010097\n",
            "The running loss at 8913 iteration is: 0.13808956996255337\n",
            "The running loss at 8914 iteration is: 0.13753971117824662\n",
            "The running loss at 8915 iteration is: 0.13698289813382503\n",
            "The running loss at 8916 iteration is: 0.13648991999164728\n",
            "The running loss at 8917 iteration is: 0.13598170932037454\n",
            "The running loss at 8918 iteration is: 0.13546571530377727\n",
            "The running loss at 8919 iteration is: 0.13495270565421782\n",
            "The running loss at 8920 iteration is: 0.13440535011654012\n",
            "The running loss at 8921 iteration is: 0.13389177306566097\n",
            "The running loss at 8922 iteration is: 0.13338449276233078\n",
            "The running loss at 8923 iteration is: 0.13286835240516495\n",
            "The running loss at 8924 iteration is: 0.1323672893866951\n",
            "The running loss at 8925 iteration is: 0.13184976729846581\n",
            "The running loss at 8926 iteration is: 0.13137458387666542\n",
            "The running loss at 8927 iteration is: 0.1308562582975282\n",
            "The running loss at 8928 iteration is: 0.1304037447518639\n",
            "The running loss at 8929 iteration is: 0.1299087227981702\n",
            "The running loss at 8930 iteration is: 0.12941633756618726\n",
            "The running loss at 8931 iteration is: 0.1289102661490902\n",
            "The running loss at 8932 iteration is: 0.1284256009531696\n",
            "The running loss at 8933 iteration is: 0.12796179902682991\n",
            "The running loss at 8934 iteration is: 0.1274947984078101\n",
            "The running loss at 8935 iteration is: 0.12703049274216655\n",
            "The running loss at 8936 iteration is: 0.12659310698984472\n",
            "The running loss at 8937 iteration is: 0.1261253092449352\n",
            "The running loss at 8938 iteration is: 0.1257423501946853\n",
            "The running loss at 8939 iteration is: 0.12532380508781596\n",
            "The running loss at 8940 iteration is: 0.12500744943572187\n",
            "The running loss at 8941 iteration is: 0.12480035528470829\n",
            "The running loss at 8942 iteration is: 0.12477639495325445\n",
            "The running loss at 8943 iteration is: 0.12511682401668184\n",
            "The running loss at 8944 iteration is: 0.12609198598102087\n",
            "The running loss at 8945 iteration is: 0.12828436238754276\n",
            "The running loss at 8946 iteration is: 0.1329234472820816\n",
            "The running loss at 8947 iteration is: 0.14229420793897016\n",
            "The running loss at 8948 iteration is: 0.16162629747179996\n",
            "The running loss at 8949 iteration is: 0.2005058642866249\n",
            "The running loss at 8950 iteration is: 0.2829491714850137\n",
            "The running loss at 8951 iteration is: 0.4514864167845321\n",
            "The running loss at 8952 iteration is: 0.8276368937068737\n",
            "The running loss at 8953 iteration is: 1.5932331385424612\n",
            "The running loss at 8954 iteration is: 3.4092324477576286\n",
            "The running loss at 8955 iteration is: 6.839485902885429\n",
            "The running loss at 8956 iteration is: 15.35116960568409\n",
            "The running loss at 8957 iteration is: 26.54153922251989\n",
            "The running loss at 8958 iteration is: 51.439916486500366\n",
            "The running loss at 8959 iteration is: 46.2892072496569\n",
            "The running loss at 8960 iteration is: 34.63766362529539\n",
            "The running loss at 8961 iteration is: 2.852338669238113\n",
            "The running loss at 8962 iteration is: 9.529523435495209\n",
            "The running loss at 8963 iteration is: 30.879344743484758\n",
            "The running loss at 8964 iteration is: 11.641521407798605\n",
            "The running loss at 8965 iteration is: 0.5532838692850618\n",
            "The running loss at 8966 iteration is: 13.785589679399681\n",
            "The running loss at 8967 iteration is: 16.336084768582428\n",
            "The running loss at 8968 iteration is: 6.4638102614930855\n",
            "The running loss at 8969 iteration is: 0.46714148305414127\n",
            "The running loss at 8970 iteration is: 8.288770594293995\n",
            "The running loss at 8971 iteration is: 15.49855125487528\n",
            "The running loss at 8972 iteration is: 7.271159265101386\n",
            "The running loss at 8973 iteration is: 0.5441061411227244\n",
            "The running loss at 8974 iteration is: 3.257881685116525\n",
            "The running loss at 8975 iteration is: 8.022148129252123\n",
            "The running loss at 8976 iteration is: 7.103337863765508\n",
            "The running loss at 8977 iteration is: 1.2801528384275753\n",
            "The running loss at 8978 iteration is: 1.3116953050131863\n",
            "The running loss at 8979 iteration is: 5.2746663518278565\n",
            "The running loss at 8980 iteration is: 4.203087249391672\n",
            "The running loss at 8981 iteration is: 0.8380908247270996\n",
            "The running loss at 8982 iteration is: 0.9971204233792443\n",
            "The running loss at 8983 iteration is: 3.203277869042176\n",
            "The running loss at 8984 iteration is: 2.5805305472680855\n",
            "The running loss at 8985 iteration is: 0.4039644506223475\n",
            "The running loss at 8986 iteration is: 1.2250462345632869\n",
            "The running loss at 8987 iteration is: 2.5339163706214567\n",
            "The running loss at 8988 iteration is: 1.1213161576905302\n",
            "The running loss at 8989 iteration is: 0.33253377119715954\n",
            "The running loss at 8990 iteration is: 1.436338055234159\n",
            "The running loss at 8991 iteration is: 1.4213793300728514\n",
            "The running loss at 8992 iteration is: 0.39417700269417494\n",
            "The running loss at 8993 iteration is: 0.5739701568731879\n",
            "The running loss at 8994 iteration is: 1.1847792212109776\n",
            "The running loss at 8995 iteration is: 0.7075329200129487\n",
            "The running loss at 8996 iteration is: 0.27154380110922993\n",
            "The running loss at 8997 iteration is: 0.7547104455381927\n",
            "The running loss at 8998 iteration is: 0.8393048272876655\n",
            "The running loss at 8999 iteration is: 0.31846167497362193\n",
            "The running loss at 9000 iteration is: 0.41015361034651043\n",
            "The running loss at 9001 iteration is: 0.7272356991628511\n",
            "The running loss at 9002 iteration is: 0.4447272869365744\n",
            "The running loss at 9003 iteration is: 0.25838191942876004\n",
            "The running loss at 9004 iteration is: 0.5081175636414254\n",
            "The running loss at 9005 iteration is: 0.4994125864771502\n",
            "The running loss at 9006 iteration is: 0.26251763391335897\n",
            "The running loss at 9007 iteration is: 0.32594164080836274\n",
            "The running loss at 9008 iteration is: 0.45613476635470224\n",
            "The running loss at 9009 iteration is: 0.32375298927633983\n",
            "The running loss at 9010 iteration is: 0.24277909980899853\n",
            "The running loss at 9011 iteration is: 0.3606912684408812\n",
            "The running loss at 9012 iteration is: 0.35892431054364266\n",
            "The running loss at 9013 iteration is: 0.24209817047421958\n",
            "The running loss at 9014 iteration is: 0.2729024002218514\n",
            "The running loss at 9015 iteration is: 0.33867972754845566\n",
            "The running loss at 9016 iteration is: 0.2710469755889372\n",
            "The running loss at 9017 iteration is: 0.23013873794264195\n",
            "The running loss at 9018 iteration is: 0.28650697861207536\n",
            "The running loss at 9019 iteration is: 0.2856632867914887\n",
            "The running loss at 9020 iteration is: 0.22959084566456398\n",
            "The running loss at 9021 iteration is: 0.23997303825620855\n",
            "The running loss at 9022 iteration is: 0.27237003678196814\n",
            "The running loss at 9023 iteration is: 0.24449854094637022\n",
            "The running loss at 9024 iteration is: 0.21965003586866347\n",
            "The running loss at 9025 iteration is: 0.24372423627417042\n",
            "The running loss at 9026 iteration is: 0.2495243516920619\n",
            "The running loss at 9027 iteration is: 0.22172679853430136\n",
            "The running loss at 9028 iteration is: 0.22018891121985212\n",
            "The running loss at 9029 iteration is: 0.23790449392347274\n",
            "The running loss at 9030 iteration is: 0.2286219203486267\n",
            "The running loss at 9031 iteration is: 0.21223681517632179\n",
            "The running loss at 9032 iteration is: 0.21969846053992995\n",
            "The running loss at 9033 iteration is: 0.22669768764700518\n",
            "The running loss at 9034 iteration is: 0.21485576115814992\n",
            "The running loss at 9035 iteration is: 0.20848250359246415\n",
            "The running loss at 9036 iteration is: 0.21624350730794595\n",
            "The running loss at 9037 iteration is: 0.21617933150482888\n",
            "The running loss at 9038 iteration is: 0.2066537317575728\n",
            "The running loss at 9039 iteration is: 0.2058565102747238\n",
            "The running loss at 9040 iteration is: 0.21079554359157343\n",
            "The running loss at 9041 iteration is: 0.2076231914661783\n",
            "The running loss at 9042 iteration is: 0.20155210375797314\n",
            "The running loss at 9043 iteration is: 0.2026955443017806\n",
            "The running loss at 9044 iteration is: 0.2049051102161838\n",
            "The running loss at 9045 iteration is: 0.20121032285585838\n",
            "The running loss at 9046 iteration is: 0.19770017574246446\n",
            "The running loss at 9047 iteration is: 0.19906481892129999\n",
            "The running loss at 9048 iteration is: 0.19951267471449577\n",
            "The running loss at 9049 iteration is: 0.19629899273872653\n",
            "The running loss at 9050 iteration is: 0.1942936665750557\n",
            "The running loss at 9051 iteration is: 0.1950535962095553\n",
            "The running loss at 9052 iteration is: 0.1946721581241253\n",
            "The running loss at 9053 iteration is: 0.19214955327323494\n",
            "The running loss at 9054 iteration is: 0.19090532314769404\n",
            "The running loss at 9055 iteration is: 0.1912487262379213\n",
            "The running loss at 9056 iteration is: 0.1905451105976149\n",
            "The running loss at 9057 iteration is: 0.18854723505289375\n",
            "The running loss at 9058 iteration is: 0.18754927603466123\n",
            "The running loss at 9059 iteration is: 0.18759638452781865\n",
            "The running loss at 9060 iteration is: 0.1867922068343622\n",
            "The running loss at 9061 iteration is: 0.1851808316493016\n",
            "The running loss at 9062 iteration is: 0.1842165879149456\n",
            "The running loss at 9063 iteration is: 0.18399025784456227\n",
            "The running loss at 9064 iteration is: 0.18321013289353602\n",
            "The running loss at 9065 iteration is: 0.1818926786123721\n",
            "The running loss at 9066 iteration is: 0.18101169790543453\n",
            "The running loss at 9067 iteration is: 0.1805900085474369\n",
            "The running loss at 9068 iteration is: 0.17988470056165462\n",
            "The running loss at 9069 iteration is: 0.17877719735733494\n",
            "The running loss at 9070 iteration is: 0.17785390639953083\n",
            "The running loss at 9071 iteration is: 0.17728220894254582\n",
            "The running loss at 9072 iteration is: 0.176663006868667\n",
            "The running loss at 9073 iteration is: 0.17577122770770617\n",
            "The running loss at 9074 iteration is: 0.1748629079095812\n",
            "The running loss at 9075 iteration is: 0.17420529768063014\n",
            "The running loss at 9076 iteration is: 0.17356433665537876\n",
            "The running loss at 9077 iteration is: 0.17280651264609453\n",
            "The running loss at 9078 iteration is: 0.1718913197633756\n",
            "The running loss at 9079 iteration is: 0.17120126687306933\n",
            "The running loss at 9080 iteration is: 0.170550590077028\n",
            "The running loss at 9081 iteration is: 0.16983095421259267\n",
            "The running loss at 9082 iteration is: 0.16904248888489817\n",
            "The running loss at 9083 iteration is: 0.16829164075860678\n",
            "The running loss at 9084 iteration is: 0.16765458753124884\n",
            "The running loss at 9085 iteration is: 0.16696063654089882\n",
            "The running loss at 9086 iteration is: 0.16625082255280207\n",
            "The running loss at 9087 iteration is: 0.16547981085977723\n",
            "The running loss at 9088 iteration is: 0.16481874387591414\n",
            "The running loss at 9089 iteration is: 0.1641832537309082\n",
            "The running loss at 9090 iteration is: 0.16353573626745055\n",
            "The running loss at 9091 iteration is: 0.16279941608570744\n",
            "The running loss at 9092 iteration is: 0.16211505567058773\n",
            "The running loss at 9093 iteration is: 0.16146421004361086\n",
            "The running loss at 9094 iteration is: 0.1607984800054086\n",
            "The running loss at 9095 iteration is: 0.16015580446553182\n",
            "The running loss at 9096 iteration is: 0.1594688499821695\n",
            "The running loss at 9097 iteration is: 0.15879231385249606\n",
            "The running loss at 9098 iteration is: 0.15814685619533098\n",
            "The running loss at 9099 iteration is: 0.1575477663743801\n",
            "The running loss at 9100 iteration is: 0.15689731833307125\n",
            "The running loss at 9101 iteration is: 0.15621665637989005\n",
            "The running loss at 9102 iteration is: 0.15558350797971093\n",
            "The running loss at 9103 iteration is: 0.15493253056413112\n",
            "The running loss at 9104 iteration is: 0.15434802097161968\n",
            "The running loss at 9105 iteration is: 0.15370565700461766\n",
            "The running loss at 9106 iteration is: 0.15307947477748618\n",
            "The running loss at 9107 iteration is: 0.1524431839341328\n",
            "The running loss at 9108 iteration is: 0.15182130086090534\n",
            "The running loss at 9109 iteration is: 0.15122914871455867\n",
            "The running loss at 9110 iteration is: 0.15064532509220277\n",
            "The running loss at 9111 iteration is: 0.14999101065952872\n",
            "The running loss at 9112 iteration is: 0.14940271151445947\n",
            "The running loss at 9113 iteration is: 0.14879939343710857\n",
            "The running loss at 9114 iteration is: 0.14822520741314119\n",
            "The running loss at 9115 iteration is: 0.14762293189209888\n",
            "The running loss at 9116 iteration is: 0.14702292443887852\n",
            "The running loss at 9117 iteration is: 0.14645091970990567\n",
            "The running loss at 9118 iteration is: 0.14586464098986232\n",
            "The running loss at 9119 iteration is: 0.14528346379479515\n",
            "The running loss at 9120 iteration is: 0.1446905050054832\n",
            "The running loss at 9121 iteration is: 0.14414053565287016\n",
            "The running loss at 9122 iteration is: 0.14357188257034315\n",
            "The running loss at 9123 iteration is: 0.14299116317692098\n",
            "The running loss at 9124 iteration is: 0.14244655123872266\n",
            "The running loss at 9125 iteration is: 0.14186655701727616\n",
            "The running loss at 9126 iteration is: 0.1412922761418832\n",
            "The running loss at 9127 iteration is: 0.14080560129656827\n",
            "The running loss at 9128 iteration is: 0.14023457269562678\n",
            "The running loss at 9129 iteration is: 0.13968806697200425\n",
            "The running loss at 9130 iteration is: 0.1390983048442505\n",
            "The running loss at 9131 iteration is: 0.13857461582010774\n",
            "The running loss at 9132 iteration is: 0.13805503892916463\n",
            "The running loss at 9133 iteration is: 0.13749486997359558\n",
            "The running loss at 9134 iteration is: 0.13697595269588067\n",
            "The running loss at 9135 iteration is: 0.1364578802375295\n",
            "The running loss at 9136 iteration is: 0.13589076340539977\n",
            "The running loss at 9137 iteration is: 0.1353747943558565\n",
            "The running loss at 9138 iteration is: 0.13487803798372433\n",
            "The running loss at 9139 iteration is: 0.13433620410030275\n",
            "The running loss at 9140 iteration is: 0.13382897406338362\n",
            "The running loss at 9141 iteration is: 0.13331267948924472\n",
            "The running loss at 9142 iteration is: 0.1328133976088212\n",
            "The running loss at 9143 iteration is: 0.13229002889705294\n",
            "The running loss at 9144 iteration is: 0.13180070612087802\n",
            "The running loss at 9145 iteration is: 0.1312713938614941\n",
            "The running loss at 9146 iteration is: 0.13077316841894493\n",
            "The running loss at 9147 iteration is: 0.13028331984997835\n",
            "The running loss at 9148 iteration is: 0.12978513272065964\n",
            "The running loss at 9149 iteration is: 0.1293144990434093\n",
            "The running loss at 9150 iteration is: 0.12881125866565427\n",
            "The running loss at 9151 iteration is: 0.12829575397469306\n",
            "The running loss at 9152 iteration is: 0.12781303760442067\n",
            "The running loss at 9153 iteration is: 0.12734789292689913\n",
            "The running loss at 9154 iteration is: 0.1268644447485791\n",
            "The running loss at 9155 iteration is: 0.12641709303157475\n",
            "The running loss at 9156 iteration is: 0.12597038441852748\n",
            "The running loss at 9157 iteration is: 0.1254764233794111\n",
            "The running loss at 9158 iteration is: 0.1250991187260102\n",
            "The running loss at 9159 iteration is: 0.12469726297925043\n",
            "The running loss at 9160 iteration is: 0.1243524400824038\n",
            "The running loss at 9161 iteration is: 0.12412654389748397\n",
            "The running loss at 9162 iteration is: 0.12407763188514216\n",
            "The running loss at 9163 iteration is: 0.12435621745835407\n",
            "The running loss at 9164 iteration is: 0.12535587073796178\n",
            "The running loss at 9165 iteration is: 0.12763719644885335\n",
            "The running loss at 9166 iteration is: 0.13256089351340897\n",
            "The running loss at 9167 iteration is: 0.14273583163710304\n",
            "The running loss at 9168 iteration is: 0.16415196882786628\n",
            "The running loss at 9169 iteration is: 0.20869420369827366\n",
            "The running loss at 9170 iteration is: 0.3058553579888387\n",
            "The running loss at 9171 iteration is: 0.5097330577653316\n",
            "The running loss at 9172 iteration is: 0.9779872198209892\n",
            "The running loss at 9173 iteration is: 1.9519853994842262\n",
            "The running loss at 9174 iteration is: 4.327715679382877\n",
            "The running loss at 9175 iteration is: 8.824214019905245\n",
            "The running loss at 9176 iteration is: 20.19241760289108\n",
            "The running loss at 9177 iteration is: 33.54792575839307\n",
            "The running loss at 9178 iteration is: 61.62281929035123\n",
            "The running loss at 9179 iteration is: 44.084769323523034\n",
            "The running loss at 9180 iteration is: 19.823974517928935\n",
            "The running loss at 9181 iteration is: 0.220799043887579\n",
            "The running loss at 9182 iteration is: 18.26154320768836\n",
            "The running loss at 9183 iteration is: 31.8649303588647\n",
            "The running loss at 9184 iteration is: 5.461640063976987\n",
            "The running loss at 9185 iteration is: 5.238561284507415\n",
            "The running loss at 9186 iteration is: 25.160370993350206\n",
            "The running loss at 9187 iteration is: 15.256779826413391\n",
            "The running loss at 9188 iteration is: 1.403163732459375\n",
            "The running loss at 9189 iteration is: 4.5843765517338575\n",
            "The running loss at 9190 iteration is: 14.261249649553442\n",
            "The running loss at 9191 iteration is: 14.373849602102382\n",
            "The running loss at 9192 iteration is: 2.570856916427727\n",
            "The running loss at 9193 iteration is: 1.9824167317258745\n",
            "The running loss at 9194 iteration is: 10.010630658729703\n",
            "The running loss at 9195 iteration is: 8.299997647458714\n",
            "The running loss at 9196 iteration is: 1.7915622635008934\n",
            "The running loss at 9197 iteration is: 1.2330294394210455\n",
            "The running loss at 9198 iteration is: 5.661227524647855\n",
            "The running loss at 9199 iteration is: 5.34200678126406\n",
            "The running loss at 9200 iteration is: 0.6897417557399658\n",
            "The running loss at 9201 iteration is: 1.8924344204065997\n",
            "The running loss at 9202 iteration is: 4.783915835652826\n",
            "The running loss at 9203 iteration is: 1.9874334924572385\n",
            "The running loss at 9204 iteration is: 0.390920502450743\n",
            "The running loss at 9205 iteration is: 2.616949238524647\n",
            "The running loss at 9206 iteration is: 2.354965517654731\n",
            "The running loss at 9207 iteration is: 0.40163719935062064\n",
            "The running loss at 9208 iteration is: 1.0530231855361498\n",
            "The running loss at 9209 iteration is: 1.9875459805380853\n",
            "The running loss at 9210 iteration is: 0.8244502476744769\n",
            "The running loss at 9211 iteration is: 0.379352553675942\n",
            "The running loss at 9212 iteration is: 1.3829554193421487\n",
            "The running loss at 9213 iteration is: 1.104475401372643\n",
            "The running loss at 9214 iteration is: 0.2587328821344994\n",
            "The running loss at 9215 iteration is: 0.8313097196651082\n",
            "The running loss at 9216 iteration is: 1.1113923286502563\n",
            "The running loss at 9217 iteration is: 0.36105969685707223\n",
            "The running loss at 9218 iteration is: 0.4526792361873283\n",
            "The running loss at 9219 iteration is: 0.916178234121032\n",
            "The running loss at 9220 iteration is: 0.48768850404961184\n",
            "The running loss at 9221 iteration is: 0.26907528551915905\n",
            "The running loss at 9222 iteration is: 0.6492124183150471\n",
            "The running loss at 9223 iteration is: 0.5484272100632593\n",
            "The running loss at 9224 iteration is: 0.23793518129822558\n",
            "The running loss at 9225 iteration is: 0.41689515481534906\n",
            "The running loss at 9226 iteration is: 0.5240877431309283\n",
            "The running loss at 9227 iteration is: 0.28568403826520183\n",
            "The running loss at 9228 iteration is: 0.27502169952336913\n",
            "The running loss at 9229 iteration is: 0.4399234873510071\n",
            "The running loss at 9230 iteration is: 0.3405422818826031\n",
            "The running loss at 9231 iteration is: 0.22463428314744405\n",
            "The running loss at 9232 iteration is: 0.33775839261592167\n",
            "The running loss at 9233 iteration is: 0.3572731146949494\n",
            "The running loss at 9234 iteration is: 0.23386058881631472\n",
            "The running loss at 9235 iteration is: 0.25762162385371484\n",
            "The running loss at 9236 iteration is: 0.32970120789888635\n",
            "The running loss at 9237 iteration is: 0.2607125180075318\n",
            "The running loss at 9238 iteration is: 0.21869170090478818\n",
            "The running loss at 9239 iteration is: 0.2788913522134038\n",
            "The running loss at 9240 iteration is: 0.27434276935083307\n",
            "The running loss at 9241 iteration is: 0.21645213281530487\n",
            "The running loss at 9242 iteration is: 0.2329278992305496\n",
            "The running loss at 9243 iteration is: 0.2637058090204826\n",
            "The running loss at 9244 iteration is: 0.22995447443450895\n",
            "The running loss at 9245 iteration is: 0.20977354737776932\n",
            "The running loss at 9246 iteration is: 0.23772477630840513\n",
            "The running loss at 9247 iteration is: 0.23730294299943078\n",
            "The running loss at 9248 iteration is: 0.2083151600627767\n",
            "The running loss at 9249 iteration is: 0.21331155247402364\n",
            "The running loss at 9250 iteration is: 0.22973579269288882\n",
            "The running loss at 9251 iteration is: 0.21502389040272496\n",
            "The running loss at 9252 iteration is: 0.20182284001761017\n",
            "The running loss at 9253 iteration is: 0.21338342343679156\n",
            "The running loss at 9254 iteration is: 0.21633462821347677\n",
            "The running loss at 9255 iteration is: 0.20224608212358847\n",
            "The running loss at 9256 iteration is: 0.2001543171820787\n",
            "The running loss at 9257 iteration is: 0.20875683930917627\n",
            "The running loss at 9258 iteration is: 0.20489485901115856\n",
            "The running loss at 9259 iteration is: 0.19582180145113126\n",
            "The running loss at 9260 iteration is: 0.19828715761990265\n",
            "The running loss at 9261 iteration is: 0.20222081793371746\n",
            "The running loss at 9262 iteration is: 0.19658116140775034\n",
            "The running loss at 9263 iteration is: 0.19205858126000935\n",
            "The running loss at 9264 iteration is: 0.19499520694194097\n",
            "The running loss at 9265 iteration is: 0.19570780887432387\n",
            "The running loss at 9266 iteration is: 0.19097299249689673\n",
            "The running loss at 9267 iteration is: 0.18903340668392885\n",
            "The running loss at 9268 iteration is: 0.1910955981267672\n",
            "The running loss at 9269 iteration is: 0.19035468888676196\n",
            "The running loss at 9270 iteration is: 0.18680926283791777\n",
            "The running loss at 9271 iteration is: 0.1859140736742763\n",
            "The running loss at 9272 iteration is: 0.18707520666507402\n",
            "The running loss at 9273 iteration is: 0.18596021952981834\n",
            "The running loss at 9274 iteration is: 0.18333634539236673\n",
            "The running loss at 9275 iteration is: 0.18272583230596431\n",
            "The running loss at 9276 iteration is: 0.1832133057955289\n",
            "The running loss at 9277 iteration is: 0.18209997002775077\n",
            "The running loss at 9278 iteration is: 0.18013795184401418\n",
            "The running loss at 9279 iteration is: 0.17951855142144904\n",
            "The running loss at 9280 iteration is: 0.17955667700623484\n",
            "The running loss at 9281 iteration is: 0.17860454294029965\n",
            "The running loss at 9282 iteration is: 0.1770443314939525\n",
            "The running loss at 9283 iteration is: 0.17636527712426323\n",
            "The running loss at 9284 iteration is: 0.17618370962458532\n",
            "The running loss at 9285 iteration is: 0.17537073327592584\n",
            "The running loss at 9286 iteration is: 0.17410105345234816\n",
            "The running loss at 9287 iteration is: 0.17329711109383292\n",
            "The running loss at 9288 iteration is: 0.1729228661634418\n",
            "The running loss at 9289 iteration is: 0.1722767536138767\n",
            "The running loss at 9290 iteration is: 0.17123659462862284\n",
            "The running loss at 9291 iteration is: 0.17038260366749433\n",
            "The running loss at 9292 iteration is: 0.16982586647663506\n",
            "The running loss at 9293 iteration is: 0.16923514643231516\n",
            "The running loss at 9294 iteration is: 0.16839839216507116\n",
            "The running loss at 9295 iteration is: 0.16756027657452746\n",
            "The running loss at 9296 iteration is: 0.16693756575757185\n",
            "The running loss at 9297 iteration is: 0.16634172200287045\n",
            "The running loss at 9298 iteration is: 0.16557712077759118\n",
            "The running loss at 9299 iteration is: 0.16484593120763055\n",
            "The running loss at 9300 iteration is: 0.16410105966751934\n",
            "The running loss at 9301 iteration is: 0.16353591162005215\n",
            "The running loss at 9302 iteration is: 0.16291196936993463\n",
            "The running loss at 9303 iteration is: 0.16213965799233251\n",
            "The running loss at 9304 iteration is: 0.16145184781071759\n",
            "The running loss at 9305 iteration is: 0.1608142454399273\n",
            "The running loss at 9306 iteration is: 0.1602187863532612\n",
            "The running loss at 9307 iteration is: 0.15957237275729755\n",
            "The running loss at 9308 iteration is: 0.1588525981934508\n",
            "The running loss at 9309 iteration is: 0.15818229228973538\n",
            "The running loss at 9310 iteration is: 0.15759570256477565\n",
            "The running loss at 9311 iteration is: 0.15699984612551707\n",
            "The running loss at 9312 iteration is: 0.15633299276204995\n",
            "The running loss at 9313 iteration is: 0.15567958591253292\n",
            "The running loss at 9314 iteration is: 0.15504166113952822\n",
            "The running loss at 9315 iteration is: 0.1543830619323916\n",
            "The running loss at 9316 iteration is: 0.1537907975167946\n",
            "The running loss at 9317 iteration is: 0.1531832597080432\n",
            "The running loss at 9318 iteration is: 0.15252012593193656\n",
            "The running loss at 9319 iteration is: 0.15192364011037457\n",
            "The running loss at 9320 iteration is: 0.15132292497686406\n",
            "The running loss at 9321 iteration is: 0.1507406162581877\n",
            "The running loss at 9322 iteration is: 0.1501397867659607\n",
            "The running loss at 9323 iteration is: 0.14953900722237318\n",
            "The running loss at 9324 iteration is: 0.14894937519993698\n",
            "The running loss at 9325 iteration is: 0.14835957524911414\n",
            "The running loss at 9326 iteration is: 0.1477771691707484\n",
            "The running loss at 9327 iteration is: 0.14716654935248044\n",
            "The running loss at 9328 iteration is: 0.14659154949143244\n",
            "The running loss at 9329 iteration is: 0.1460207435900904\n",
            "The running loss at 9330 iteration is: 0.1454307568584972\n",
            "The running loss at 9331 iteration is: 0.14486607704372576\n",
            "The running loss at 9332 iteration is: 0.14434746430379272\n",
            "The running loss at 9333 iteration is: 0.14375039806946607\n",
            "The running loss at 9334 iteration is: 0.1431632945522531\n",
            "The running loss at 9335 iteration is: 0.14261907259315984\n",
            "The running loss at 9336 iteration is: 0.14204963972998796\n",
            "The running loss at 9337 iteration is: 0.14153315846429768\n",
            "The running loss at 9338 iteration is: 0.1409525381227718\n",
            "The running loss at 9339 iteration is: 0.14040990423451885\n",
            "The running loss at 9340 iteration is: 0.13989960718467562\n",
            "The running loss at 9341 iteration is: 0.13932947175442104\n",
            "The running loss at 9342 iteration is: 0.1387805073408374\n",
            "The running loss at 9343 iteration is: 0.13825991874229887\n",
            "The running loss at 9344 iteration is: 0.13769634251791277\n",
            "The running loss at 9345 iteration is: 0.137183573971518\n",
            "The running loss at 9346 iteration is: 0.13664795763750107\n",
            "The running loss at 9347 iteration is: 0.13613259930087507\n",
            "The running loss at 9348 iteration is: 0.13561058498775355\n",
            "The running loss at 9349 iteration is: 0.13509479797555518\n",
            "The running loss at 9350 iteration is: 0.13457031465796035\n",
            "The running loss at 9351 iteration is: 0.13407144720206043\n",
            "The running loss at 9352 iteration is: 0.13354495606760383\n",
            "The running loss at 9353 iteration is: 0.13304299591596627\n",
            "The running loss at 9354 iteration is: 0.132534657553117\n",
            "The running loss at 9355 iteration is: 0.132036826393043\n",
            "The running loss at 9356 iteration is: 0.13152809987281688\n",
            "The running loss at 9357 iteration is: 0.1310426163649091\n",
            "The running loss at 9358 iteration is: 0.130524937861012\n",
            "The running loss at 9359 iteration is: 0.13004862524609295\n",
            "The running loss at 9360 iteration is: 0.12954409092138247\n",
            "The running loss at 9361 iteration is: 0.12907584707477596\n",
            "The running loss at 9362 iteration is: 0.12857667587091728\n",
            "The running loss at 9363 iteration is: 0.1280795553895872\n",
            "The running loss at 9364 iteration is: 0.12760341453432397\n",
            "The running loss at 9365 iteration is: 0.12712436401404015\n",
            "The running loss at 9366 iteration is: 0.1266585163190468\n",
            "The running loss at 9367 iteration is: 0.12615738058693138\n",
            "The running loss at 9368 iteration is: 0.12571989275438974\n",
            "The running loss at 9369 iteration is: 0.12524231255172646\n",
            "The running loss at 9370 iteration is: 0.12477444754362417\n",
            "The running loss at 9371 iteration is: 0.1243128477191799\n",
            "The running loss at 9372 iteration is: 0.12384045948064304\n",
            "The running loss at 9373 iteration is: 0.12341699505323242\n",
            "The running loss at 9374 iteration is: 0.12296523808582421\n",
            "The running loss at 9375 iteration is: 0.12250443278171211\n",
            "The running loss at 9376 iteration is: 0.12211903974918216\n",
            "The running loss at 9377 iteration is: 0.12174059104394869\n",
            "The running loss at 9378 iteration is: 0.12138279066109835\n",
            "The running loss at 9379 iteration is: 0.12114459490956425\n",
            "The running loss at 9380 iteration is: 0.12112821526618969\n",
            "The running loss at 9381 iteration is: 0.1215256293152364\n",
            "The running loss at 9382 iteration is: 0.12269834008119508\n",
            "The running loss at 9383 iteration is: 0.12562410166632984\n",
            "The running loss at 9384 iteration is: 0.13197666806427097\n",
            "The running loss at 9385 iteration is: 0.14578445458926628\n",
            "The running loss at 9386 iteration is: 0.17514499858366114\n",
            "The running loss at 9387 iteration is: 0.24022406828024634\n",
            "The running loss at 9388 iteration is: 0.38044762424484696\n",
            "The running loss at 9389 iteration is: 0.7059717136895388\n",
            "The running loss at 9390 iteration is: 1.4078314383835433\n",
            "The running loss at 9391 iteration is: 3.137544673982718\n",
            "The running loss at 9392 iteration is: 6.628250461231612\n",
            "The running loss at 9393 iteration is: 15.62678056835593\n",
            "The running loss at 9394 iteration is: 28.42600547852044\n",
            "The running loss at 9395 iteration is: 57.601340645051884\n",
            "The running loss at 9396 iteration is: 51.09951094717891\n",
            "The running loss at 9397 iteration is: 35.68290973774898\n",
            "The running loss at 9398 iteration is: 1.5301350175248536\n",
            "The running loss at 9399 iteration is: 15.368644305305475\n",
            "The running loss at 9400 iteration is: 37.217932437517405\n",
            "The running loss at 9401 iteration is: 7.744662015783842\n",
            "The running loss at 9402 iteration is: 4.966588569198275\n",
            "The running loss at 9403 iteration is: 27.176184502160694\n",
            "The running loss at 9404 iteration is: 15.28869424522457\n",
            "The running loss at 9405 iteration is: 0.793180153657765\n",
            "The running loss at 9406 iteration is: 6.913195242522149\n",
            "The running loss at 9407 iteration is: 16.03475751507474\n",
            "The running loss at 9408 iteration is: 13.032146755422733\n",
            "The running loss at 9409 iteration is: 1.3371360849902094\n",
            "The running loss at 9410 iteration is: 3.871773029235102\n",
            "The running loss at 9411 iteration is: 12.607296036245938\n",
            "The running loss at 9412 iteration is: 7.572279125823818\n",
            "The running loss at 9413 iteration is: 0.7071480383549003\n",
            "The running loss at 9414 iteration is: 2.917615497356971\n",
            "The running loss at 9415 iteration is: 6.727237025294139\n",
            "The running loss at 9416 iteration is: 3.95469093994569\n",
            "The running loss at 9417 iteration is: 0.32537528162296897\n",
            "The running loss at 9418 iteration is: 3.3388219545011153\n",
            "The running loss at 9419 iteration is: 4.861343286041902\n",
            "The running loss at 9420 iteration is: 0.9806926925374838\n",
            "The running loss at 9421 iteration is: 1.226871493970628\n",
            "The running loss at 9422 iteration is: 3.704483666698867\n",
            "The running loss at 9423 iteration is: 1.570165961128387\n",
            "The running loss at 9424 iteration is: 0.421390938682447\n",
            "The running loss at 9425 iteration is: 2.2804242358824083\n",
            "The running loss at 9426 iteration is: 1.6624840793357438\n",
            "The running loss at 9427 iteration is: 0.2776602522136368\n",
            "The running loss at 9428 iteration is: 1.2543523933666456\n",
            "The running loss at 9429 iteration is: 1.4781237350184822\n",
            "The running loss at 9430 iteration is: 0.3739908544441448\n",
            "The running loss at 9431 iteration is: 0.6582213094226107\n",
            "The running loss at 9432 iteration is: 1.1978405525035358\n",
            "The running loss at 9433 iteration is: 0.5079147839024903\n",
            "The running loss at 9434 iteration is: 0.36608288123658483\n",
            "The running loss at 9435 iteration is: 0.9120853698869166\n",
            "The running loss at 9436 iteration is: 0.5976020401243692\n",
            "The running loss at 9437 iteration is: 0.25742506688390815\n",
            "The running loss at 9438 iteration is: 0.6615337095759112\n",
            "The running loss at 9439 iteration is: 0.619592140734213\n",
            "The running loss at 9440 iteration is: 0.24606927598522577\n",
            "The running loss at 9441 iteration is: 0.46671530041078113\n",
            "The running loss at 9442 iteration is: 0.5816569537524099\n",
            "The running loss at 9443 iteration is: 0.27493799657687445\n",
            "The running loss at 9444 iteration is: 0.3327820353294027\n",
            "The running loss at 9445 iteration is: 0.5049889869287758\n",
            "The running loss at 9446 iteration is: 0.3094124722889532\n",
            "The running loss at 9447 iteration is: 0.2566085859767462\n",
            "The running loss at 9448 iteration is: 0.41382772944090884\n",
            "The running loss at 9449 iteration is: 0.3303778015669552\n",
            "The running loss at 9450 iteration is: 0.22707595166036718\n",
            "The running loss at 9451 iteration is: 0.32950314521434\n",
            "The running loss at 9452 iteration is: 0.3304764397163238\n",
            "The running loss at 9453 iteration is: 0.22789355804412734\n",
            "The running loss at 9454 iteration is: 0.2661174849535602\n",
            "The running loss at 9455 iteration is: 0.31138523613118424\n",
            "The running loss at 9456 iteration is: 0.24142120658786248\n",
            "The running loss at 9457 iteration is: 0.2288996851181192\n",
            "The running loss at 9458 iteration is: 0.28064800892476827\n",
            "The running loss at 9459 iteration is: 0.2526763512205302\n",
            "The running loss at 9460 iteration is: 0.21494189022908905\n",
            "The running loss at 9461 iteration is: 0.2486778858904839\n",
            "The running loss at 9462 iteration is: 0.25403791588157504\n",
            "The running loss at 9463 iteration is: 0.2156506076971472\n",
            "The running loss at 9464 iteration is: 0.22311278148432243\n",
            "The running loss at 9465 iteration is: 0.24357815110987624\n",
            "The running loss at 9466 iteration is: 0.2210536165335487\n",
            "The running loss at 9467 iteration is: 0.20927577573364517\n",
            "The running loss at 9468 iteration is: 0.2274215126281689\n",
            "The running loss at 9469 iteration is: 0.22341491559486207\n",
            "The running loss at 9470 iteration is: 0.2056524669492592\n",
            "The running loss at 9471 iteration is: 0.21213458748402647\n",
            "The running loss at 9472 iteration is: 0.21949055855403232\n",
            "The running loss at 9473 iteration is: 0.20723508882505798\n",
            "The running loss at 9474 iteration is: 0.20246842623713715\n",
            "The running loss at 9475 iteration is: 0.21082996811926907\n",
            "The running loss at 9476 iteration is: 0.20807342540704293\n",
            "The running loss at 9477 iteration is: 0.19908364637123002\n",
            "The running loss at 9478 iteration is: 0.20166095181099236\n",
            "The running loss at 9479 iteration is: 0.2051000977274143\n",
            "The running loss at 9480 iteration is: 0.1989734088370274\n",
            "The running loss at 9481 iteration is: 0.1955873663167952\n",
            "The running loss at 9482 iteration is: 0.19913128714923164\n",
            "The running loss at 9483 iteration is: 0.19823979830394262\n",
            "The running loss at 9484 iteration is: 0.19325058949842977\n",
            "The running loss at 9485 iteration is: 0.19315349759250516\n",
            "The running loss at 9486 iteration is: 0.19499134736992732\n",
            "The running loss at 9487 iteration is: 0.1924296005964955\n",
            "The running loss at 9488 iteration is: 0.18947206721278814\n",
            "The running loss at 9489 iteration is: 0.1902987902841297\n",
            "The running loss at 9490 iteration is: 0.19045344121608945\n",
            "The running loss at 9491 iteration is: 0.18781679889659497\n",
            "The running loss at 9492 iteration is: 0.18642449916325107\n",
            "The running loss at 9493 iteration is: 0.18699123076202417\n",
            "The running loss at 9494 iteration is: 0.18619952499042858\n",
            "The running loss at 9495 iteration is: 0.18413512525249812\n",
            "The running loss at 9496 iteration is: 0.18343041633957213\n",
            "The running loss at 9497 iteration is: 0.18356454453054089\n",
            "The running loss at 9498 iteration is: 0.18248905694680448\n",
            "The running loss at 9499 iteration is: 0.18090068701934853\n",
            "The running loss at 9500 iteration is: 0.1803469685933039\n",
            "The running loss at 9501 iteration is: 0.18011954909886574\n",
            "The running loss at 9502 iteration is: 0.1790932774939234\n",
            "The running loss at 9503 iteration is: 0.1778483609415156\n",
            "The running loss at 9504 iteration is: 0.17733367312867215\n",
            "The running loss at 9505 iteration is: 0.17691942280757664\n",
            "The running loss at 9506 iteration is: 0.17596051863654572\n",
            "The running loss at 9507 iteration is: 0.1749474086741515\n",
            "The running loss at 9508 iteration is: 0.17435524470789618\n",
            "The running loss at 9509 iteration is: 0.1738280978558237\n",
            "The running loss at 9510 iteration is: 0.17295132720676848\n",
            "The running loss at 9511 iteration is: 0.17206450073730625\n",
            "The running loss at 9512 iteration is: 0.1714420060513978\n",
            "The running loss at 9513 iteration is: 0.17088430235372604\n",
            "The running loss at 9514 iteration is: 0.17010291668063368\n",
            "The running loss at 9515 iteration is: 0.16928877921737764\n",
            "The running loss at 9516 iteration is: 0.16864169927630748\n",
            "The running loss at 9517 iteration is: 0.1680708164366226\n",
            "The running loss at 9518 iteration is: 0.16735957912757296\n",
            "The running loss at 9519 iteration is: 0.16651760619543554\n",
            "The running loss at 9520 iteration is: 0.16584333678894522\n",
            "The running loss at 9521 iteration is: 0.16528366678081258\n",
            "The running loss at 9522 iteration is: 0.16464751478756845\n",
            "The running loss at 9523 iteration is: 0.16387240497109845\n",
            "The running loss at 9524 iteration is: 0.16316647159661887\n",
            "The running loss at 9525 iteration is: 0.1625453767260655\n",
            "The running loss at 9526 iteration is: 0.1619493562978524\n",
            "The running loss at 9527 iteration is: 0.16126698346689755\n",
            "The running loss at 9528 iteration is: 0.16059168933226692\n",
            "The running loss at 9529 iteration is: 0.1599427168496668\n",
            "The running loss at 9530 iteration is: 0.15931187696489496\n",
            "The running loss at 9531 iteration is: 0.1586935489251631\n",
            "The running loss at 9532 iteration is: 0.15806068382947658\n",
            "The running loss at 9533 iteration is: 0.15737345548979836\n",
            "The running loss at 9534 iteration is: 0.1567899055561516\n",
            "The running loss at 9535 iteration is: 0.156195573398025\n",
            "The running loss at 9536 iteration is: 0.15556692606744396\n",
            "The running loss at 9537 iteration is: 0.15489734710655065\n",
            "The running loss at 9538 iteration is: 0.1542949778713191\n",
            "The running loss at 9539 iteration is: 0.15370401238196893\n",
            "The running loss at 9540 iteration is: 0.15308783422532932\n",
            "The running loss at 9541 iteration is: 0.15249202274838217\n",
            "The running loss at 9542 iteration is: 0.15187180279383555\n",
            "The running loss at 9543 iteration is: 0.15128378861709962\n",
            "The running loss at 9544 iteration is: 0.15068158829603912\n",
            "The running loss at 9545 iteration is: 0.15012393164032853\n",
            "The running loss at 9546 iteration is: 0.14953852360391812\n",
            "The running loss at 9547 iteration is: 0.14895105690060764\n",
            "The running loss at 9548 iteration is: 0.1483506429259844\n",
            "The running loss at 9549 iteration is: 0.1477671812265784\n",
            "The running loss at 9550 iteration is: 0.14720509674488835\n",
            "The running loss at 9551 iteration is: 0.1466370173832003\n",
            "The running loss at 9552 iteration is: 0.14603584998491284\n",
            "The running loss at 9553 iteration is: 0.1455177545654745\n",
            "The running loss at 9554 iteration is: 0.14494346237234432\n",
            "The running loss at 9555 iteration is: 0.14439394298376673\n",
            "The running loss at 9556 iteration is: 0.1438103791438231\n",
            "The running loss at 9557 iteration is: 0.1432753199492728\n",
            "The running loss at 9558 iteration is: 0.14268061867852289\n",
            "The running loss at 9559 iteration is: 0.14215954034529138\n",
            "The running loss at 9560 iteration is: 0.1416165819796628\n",
            "The running loss at 9561 iteration is: 0.1410958646029007\n",
            "The running loss at 9562 iteration is: 0.1405368238654751\n",
            "The running loss at 9563 iteration is: 0.13999590082134733\n",
            "The running loss at 9564 iteration is: 0.13946076327151713\n",
            "The running loss at 9565 iteration is: 0.1389595893014217\n",
            "The running loss at 9566 iteration is: 0.1384094271058523\n",
            "The running loss at 9567 iteration is: 0.1378976969942525\n",
            "The running loss at 9568 iteration is: 0.13737879390326835\n",
            "The running loss at 9569 iteration is: 0.13686750129510172\n",
            "The running loss at 9570 iteration is: 0.13632754992212934\n",
            "The running loss at 9571 iteration is: 0.13582290065896158\n",
            "The running loss at 9572 iteration is: 0.13529122451781556\n",
            "The running loss at 9573 iteration is: 0.1347819224871842\n",
            "The running loss at 9574 iteration is: 0.1342701416950367\n",
            "The running loss at 9575 iteration is: 0.13373295331352342\n",
            "The running loss at 9576 iteration is: 0.13325161543806646\n",
            "The running loss at 9577 iteration is: 0.13277882686653644\n",
            "The running loss at 9578 iteration is: 0.13226646294171027\n",
            "The running loss at 9579 iteration is: 0.13176934240062899\n",
            "The running loss at 9580 iteration is: 0.13129310891695545\n",
            "The running loss at 9581 iteration is: 0.13079283136121808\n",
            "The running loss at 9582 iteration is: 0.13027594311054477\n",
            "The running loss at 9583 iteration is: 0.12981998958393384\n",
            "The running loss at 9584 iteration is: 0.12931709499495234\n",
            "The running loss at 9585 iteration is: 0.12886642442931232\n",
            "The running loss at 9586 iteration is: 0.1283744890325094\n",
            "The running loss at 9587 iteration is: 0.12790857118311916\n",
            "The running loss at 9588 iteration is: 0.127447135652358\n",
            "The running loss at 9589 iteration is: 0.12696070328319292\n",
            "The running loss at 9590 iteration is: 0.1264820731718635\n",
            "The running loss at 9591 iteration is: 0.12599627820366266\n",
            "The running loss at 9592 iteration is: 0.12556528667326636\n",
            "The running loss at 9593 iteration is: 0.12510680638677973\n",
            "The running loss at 9594 iteration is: 0.12463819692970382\n",
            "The running loss at 9595 iteration is: 0.12415611361106185\n",
            "The running loss at 9596 iteration is: 0.1237177673541543\n",
            "The running loss at 9597 iteration is: 0.12326560866492778\n",
            "The running loss at 9598 iteration is: 0.12283032182233602\n",
            "The running loss at 9599 iteration is: 0.12236965460481943\n",
            "The running loss at 9600 iteration is: 0.12193922822283736\n",
            "The running loss at 9601 iteration is: 0.12150899928215259\n",
            "The running loss at 9602 iteration is: 0.12107888284611706\n",
            "The running loss at 9603 iteration is: 0.12062053552228441\n",
            "The running loss at 9604 iteration is: 0.12025179535409782\n",
            "The running loss at 9605 iteration is: 0.11987796148757382\n",
            "The running loss at 9606 iteration is: 0.11958891432065905\n",
            "The running loss at 9607 iteration is: 0.11945174963959797\n",
            "The running loss at 9608 iteration is: 0.1194864701875747\n",
            "The running loss at 9609 iteration is: 0.1200283089819115\n",
            "The running loss at 9610 iteration is: 0.12148283338693187\n",
            "The running loss at 9611 iteration is: 0.12490926319344238\n",
            "The running loss at 9612 iteration is: 0.13242452420902007\n",
            "The running loss at 9613 iteration is: 0.14885760019056185\n",
            "The running loss at 9614 iteration is: 0.1839912431788185\n",
            "The running loss at 9615 iteration is: 0.2623850256716785\n",
            "The running loss at 9616 iteration is: 0.43211143310106187\n",
            "The running loss at 9617 iteration is: 0.8301915058399598\n",
            "The running loss at 9618 iteration is: 1.6878931357010194\n",
            "The running loss at 9619 iteration is: 3.820270160611919\n",
            "The running loss at 9620 iteration is: 8.047467041831268\n",
            "The running loss at 9621 iteration is: 18.9557726903277\n",
            "The running loss at 9622 iteration is: 32.91815965743442\n",
            "The running loss at 9623 iteration is: 62.98619663883439\n",
            "The running loss at 9624 iteration is: 46.64435428749538\n",
            "The running loss at 9625 iteration is: 21.73675156735009\n",
            "The running loss at 9626 iteration is: 0.23615164622221585\n",
            "The running loss at 9627 iteration is: 20.334613793281164\n",
            "The running loss at 9628 iteration is: 31.754951808202517\n",
            "The running loss at 9629 iteration is: 2.9705425550402227\n",
            "The running loss at 9630 iteration is: 10.806220098208557\n",
            "The running loss at 9631 iteration is: 31.49115609348066\n",
            "The running loss at 9632 iteration is: 9.668585465063185\n",
            "The running loss at 9633 iteration is: 1.4740749061207858\n",
            "The running loss at 9634 iteration is: 17.116130641345727\n",
            "The running loss at 9635 iteration is: 15.945831431498894\n",
            "The running loss at 9636 iteration is: 4.03042878458424\n",
            "The running loss at 9637 iteration is: 1.559383129082444\n",
            "The running loss at 9638 iteration is: 10.444270938078292\n",
            "The running loss at 9639 iteration is: 12.04862962412553\n",
            "The running loss at 9640 iteration is: 1.852717525496098\n",
            "The running loss at 9641 iteration is: 2.6413716688329294\n",
            "The running loss at 9642 iteration is: 9.411058078163487\n",
            "The running loss at 9643 iteration is: 4.628217621946092\n",
            "The running loss at 9644 iteration is: 0.34113552797281993\n",
            "The running loss at 9645 iteration is: 4.155797930011216\n",
            "The running loss at 9646 iteration is: 4.601158096886784\n",
            "The running loss at 9647 iteration is: 0.8297687867994911\n",
            "The running loss at 9648 iteration is: 1.390076977294488\n",
            "The running loss at 9649 iteration is: 3.484124760971224\n",
            "The running loss at 9650 iteration is: 1.5311302553567732\n",
            "The running loss at 9651 iteration is: 0.46379352602998647\n",
            "The running loss at 9652 iteration is: 2.400961620651234\n",
            "The running loss at 9653 iteration is: 1.818151631989424\n",
            "The running loss at 9654 iteration is: 0.27845172411290364\n",
            "The running loss at 9655 iteration is: 1.594685254887986\n",
            "The running loss at 9656 iteration is: 1.7690955478234462\n",
            "The running loss at 9657 iteration is: 0.31406246907448127\n",
            "The running loss at 9658 iteration is: 1.0399438727789736\n",
            "The running loss at 9659 iteration is: 1.5475187532473407\n",
            "The running loss at 9660 iteration is: 0.38820048573854304\n",
            "The running loss at 9661 iteration is: 0.6741811678690433\n",
            "The running loss at 9662 iteration is: 1.267998587988816\n",
            "The running loss at 9663 iteration is: 0.44807587774308716\n",
            "The running loss at 9664 iteration is: 0.44436748089542605\n",
            "The running loss at 9665 iteration is: 0.9929031202882582\n",
            "The running loss at 9666 iteration is: 0.4821341510405532\n",
            "The running loss at 9667 iteration is: 0.31269481991314935\n",
            "The running loss at 9668 iteration is: 0.7512120782505913\n",
            "The running loss at 9669 iteration is: 0.49004191033390343\n",
            "The running loss at 9670 iteration is: 0.2506004478864482\n",
            "The running loss at 9671 iteration is: 0.5563976442958735\n",
            "The running loss at 9672 iteration is: 0.4750457696639332\n",
            "The running loss at 9673 iteration is: 0.23409907482158543\n",
            "The running loss at 9674 iteration is: 0.41160145553468847\n",
            "The running loss at 9675 iteration is: 0.44231552625790943\n",
            "The running loss at 9676 iteration is: 0.2428759594456236\n",
            "The running loss at 9677 iteration is: 0.3134760733956159\n",
            "The running loss at 9678 iteration is: 0.3982265245770731\n",
            "The running loss at 9679 iteration is: 0.2608742669605861\n",
            "The running loss at 9680 iteration is: 0.2544152620002437\n",
            "The running loss at 9681 iteration is: 0.34858126073603596\n",
            "The running loss at 9682 iteration is: 0.27612633572572703\n",
            "The running loss at 9683 iteration is: 0.22618731270306122\n",
            "The running loss at 9684 iteration is: 0.30050654970789686\n",
            "The running loss at 9685 iteration is: 0.2817637690163008\n",
            "The running loss at 9686 iteration is: 0.2186400822584473\n",
            "The running loss at 9687 iteration is: 0.25982543408745395\n",
            "The running loss at 9688 iteration is: 0.2757598865809161\n",
            "The running loss at 9689 iteration is: 0.22203715452606132\n",
            "The running loss at 9690 iteration is: 0.23081197958127744\n",
            "The running loss at 9691 iteration is: 0.2605345572457838\n",
            "The running loss at 9692 iteration is: 0.22800093878117328\n",
            "The running loss at 9693 iteration is: 0.21446734000946324\n",
            "The running loss at 9694 iteration is: 0.24093810218603945\n",
            "The running loss at 9695 iteration is: 0.2306363396162198\n",
            "The running loss at 9696 iteration is: 0.20862724308438724\n",
            "The running loss at 9697 iteration is: 0.22271940413106364\n",
            "The running loss at 9698 iteration is: 0.22792383594705407\n",
            "The running loss at 9699 iteration is: 0.20897421373133762\n",
            "The running loss at 9700 iteration is: 0.20932817448051205\n",
            "The running loss at 9701 iteration is: 0.22009789333329577\n",
            "The running loss at 9702 iteration is: 0.2104804895676487\n",
            "The running loss at 9703 iteration is: 0.20244350540254338\n",
            "The running loss at 9704 iteration is: 0.21044823671107193\n",
            "The running loss at 9705 iteration is: 0.2098923547488861\n",
            "The running loss at 9706 iteration is: 0.2003187130535561\n",
            "The running loss at 9707 iteration is: 0.2019152147953425\n",
            "The running loss at 9708 iteration is: 0.2060052519537327\n",
            "The running loss at 9709 iteration is: 0.20011930218755772\n",
            "The running loss at 9710 iteration is: 0.19644052753535898\n",
            "The running loss at 9711 iteration is: 0.20009598516152832\n",
            "The running loss at 9712 iteration is: 0.19909150892523758\n",
            "The running loss at 9713 iteration is: 0.1941000138794741\n",
            "The running loss at 9714 iteration is: 0.19438960079515782\n",
            "The running loss at 9715 iteration is: 0.19603937222205378\n",
            "The running loss at 9716 iteration is: 0.19304319269586742\n",
            "The running loss at 9717 iteration is: 0.19052360311322922\n",
            "The running loss at 9718 iteration is: 0.1916715418692469\n",
            "The running loss at 9719 iteration is: 0.1913258779804149\n",
            "The running loss at 9720 iteration is: 0.1885028813780595\n",
            "The running loss at 9721 iteration is: 0.18772542810696352\n",
            "The running loss at 9722 iteration is: 0.18840129089620672\n",
            "The running loss at 9723 iteration is: 0.18703500859168773\n",
            "The running loss at 9724 iteration is: 0.18501363443387672\n",
            "The running loss at 9725 iteration is: 0.18486501324840227\n",
            "The running loss at 9726 iteration is: 0.18483859370350236\n",
            "The running loss at 9727 iteration is: 0.18319860030471735\n",
            "The running loss at 9728 iteration is: 0.18193642310104483\n",
            "The running loss at 9729 iteration is: 0.181873619376524\n",
            "The running loss at 9730 iteration is: 0.1813174085299956\n",
            "The running loss at 9731 iteration is: 0.17988246120597637\n",
            "The running loss at 9732 iteration is: 0.17900783559300093\n",
            "The running loss at 9733 iteration is: 0.17875499626914385\n",
            "The running loss at 9734 iteration is: 0.17804053061002623\n",
            "The running loss at 9735 iteration is: 0.17682771413581538\n",
            "The running loss at 9736 iteration is: 0.17615505573349052\n",
            "The running loss at 9737 iteration is: 0.17573111302763658\n",
            "The running loss at 9738 iteration is: 0.17490837856135213\n",
            "The running loss at 9739 iteration is: 0.17391798918155046\n",
            "The running loss at 9740 iteration is: 0.17329458292876204\n",
            "The running loss at 9741 iteration is: 0.1728092186356773\n",
            "The running loss at 9742 iteration is: 0.17203628930957837\n",
            "The running loss at 9743 iteration is: 0.17112680133957536\n",
            "The running loss at 9744 iteration is: 0.17046684300730233\n",
            "The running loss at 9745 iteration is: 0.16993940556791828\n",
            "The running loss at 9746 iteration is: 0.16918477596059836\n",
            "The running loss at 9747 iteration is: 0.16836764372317692\n",
            "The running loss at 9748 iteration is: 0.1677467959510349\n",
            "The running loss at 9749 iteration is: 0.16718281218371936\n",
            "The running loss at 9750 iteration is: 0.16647013336931435\n",
            "The running loss at 9751 iteration is: 0.16571764117332907\n",
            "The running loss at 9752 iteration is: 0.16501972623387676\n",
            "The running loss at 9753 iteration is: 0.16445172648038225\n",
            "The running loss at 9754 iteration is: 0.16381545606716014\n",
            "The running loss at 9755 iteration is: 0.16313588505805032\n",
            "The running loss at 9756 iteration is: 0.1624395967113012\n",
            "The running loss at 9757 iteration is: 0.16184273423519135\n",
            "The running loss at 9758 iteration is: 0.16123425743697678\n",
            "The running loss at 9759 iteration is: 0.16056275566319206\n",
            "The running loss at 9760 iteration is: 0.15993045865663977\n",
            "The running loss at 9761 iteration is: 0.15926253624931927\n",
            "The running loss at 9762 iteration is: 0.15869078553255503\n",
            "The running loss at 9763 iteration is: 0.1580684196288212\n",
            "The running loss at 9764 iteration is: 0.15740595565840423\n",
            "The running loss at 9765 iteration is: 0.15681345188469586\n",
            "The running loss at 9766 iteration is: 0.15619773828879602\n",
            "The running loss at 9767 iteration is: 0.15557980328966178\n",
            "The running loss at 9768 iteration is: 0.15498837869156987\n",
            "The running loss at 9769 iteration is: 0.15433057590884217\n",
            "The running loss at 9770 iteration is: 0.15375750027778756\n",
            "The running loss at 9771 iteration is: 0.15318400916851063\n",
            "The running loss at 9772 iteration is: 0.1525736469745204\n",
            "The running loss at 9773 iteration is: 0.15198409523658188\n",
            "The running loss at 9774 iteration is: 0.151357854289186\n",
            "The running loss at 9775 iteration is: 0.15078034125721765\n",
            "The running loss at 9776 iteration is: 0.1502316730171327\n",
            "The running loss at 9777 iteration is: 0.14965354920067275\n",
            "The running loss at 9778 iteration is: 0.1490898728943828\n",
            "The running loss at 9779 iteration is: 0.14849899568624456\n",
            "The running loss at 9780 iteration is: 0.14791386597608963\n",
            "The running loss at 9781 iteration is: 0.14736769407998218\n",
            "The running loss at 9782 iteration is: 0.1467823334076022\n",
            "The running loss at 9783 iteration is: 0.14622902827576975\n",
            "The running loss at 9784 iteration is: 0.14569419790441002\n",
            "The running loss at 9785 iteration is: 0.14511479745514932\n",
            "The running loss at 9786 iteration is: 0.14457237187396643\n",
            "The running loss at 9787 iteration is: 0.14405927341370284\n",
            "The running loss at 9788 iteration is: 0.14345762382154656\n",
            "The running loss at 9789 iteration is: 0.14294598203327885\n",
            "The running loss at 9790 iteration is: 0.14239910356602015\n",
            "The running loss at 9791 iteration is: 0.14185243540199338\n",
            "The running loss at 9792 iteration is: 0.14133885397736648\n",
            "The running loss at 9793 iteration is: 0.14081125643003561\n",
            "The running loss at 9794 iteration is: 0.14023500551572946\n",
            "The running loss at 9795 iteration is: 0.1397162601630053\n",
            "The running loss at 9796 iteration is: 0.1391731978490353\n",
            "The running loss at 9797 iteration is: 0.1386579643023169\n",
            "The running loss at 9798 iteration is: 0.13813365437698605\n",
            "The running loss at 9799 iteration is: 0.13765370432655105\n",
            "The running loss at 9800 iteration is: 0.1371271689723634\n",
            "The running loss at 9801 iteration is: 0.13661328968207198\n",
            "The running loss at 9802 iteration is: 0.13608303360331353\n",
            "The running loss at 9803 iteration is: 0.135592618301632\n",
            "The running loss at 9804 iteration is: 0.13510554022599983\n",
            "The running loss at 9805 iteration is: 0.1345916532816934\n",
            "The running loss at 9806 iteration is: 0.13406305535010546\n",
            "The running loss at 9807 iteration is: 0.133597424832624\n",
            "The running loss at 9808 iteration is: 0.1330771345260274\n",
            "The running loss at 9809 iteration is: 0.13259680408344576\n",
            "The running loss at 9810 iteration is: 0.13211660364366978\n",
            "The running loss at 9811 iteration is: 0.13160691509629643\n",
            "The running loss at 9812 iteration is: 0.13113196778443745\n",
            "The running loss at 9813 iteration is: 0.13063098113584443\n",
            "The running loss at 9814 iteration is: 0.1301683673161575\n",
            "The running loss at 9815 iteration is: 0.12968693393508252\n",
            "The running loss at 9816 iteration is: 0.1292174798078613\n",
            "The running loss at 9817 iteration is: 0.12875475743765535\n",
            "The running loss at 9818 iteration is: 0.12824950283558312\n",
            "The running loss at 9819 iteration is: 0.12779754501432447\n",
            "The running loss at 9820 iteration is: 0.12734622986943814\n",
            "The running loss at 9821 iteration is: 0.12685432008218905\n",
            "The running loss at 9822 iteration is: 0.1263820648658922\n",
            "The running loss at 9823 iteration is: 0.1259441755858687\n",
            "The running loss at 9824 iteration is: 0.12548807522708721\n",
            "The running loss at 9825 iteration is: 0.12504292224836835\n",
            "The running loss at 9826 iteration is: 0.12458791769509703\n",
            "The running loss at 9827 iteration is: 0.12414905855669049\n",
            "The running loss at 9828 iteration is: 0.12370475953596508\n",
            "The running loss at 9829 iteration is: 0.12321675423386642\n",
            "The running loss at 9830 iteration is: 0.1227866521514438\n",
            "The running loss at 9831 iteration is: 0.12233282455408631\n",
            "The running loss at 9832 iteration is: 0.12193107601948006\n",
            "The running loss at 9833 iteration is: 0.12153152351718502\n",
            "The running loss at 9834 iteration is: 0.12110908067990739\n",
            "The running loss at 9835 iteration is: 0.12073490581888853\n",
            "The running loss at 9836 iteration is: 0.12036963044457885\n",
            "The running loss at 9837 iteration is: 0.12007277093122885\n",
            "The running loss at 9838 iteration is: 0.11989729090685902\n",
            "The running loss at 9839 iteration is: 0.11983118098029168\n",
            "The running loss at 9840 iteration is: 0.12009086500107181\n",
            "The running loss at 9841 iteration is: 0.1209003694869419\n",
            "The running loss at 9842 iteration is: 0.12286036887410198\n",
            "The running loss at 9843 iteration is: 0.12707060514511098\n",
            "The running loss at 9844 iteration is: 0.13577520511576782\n",
            "The running loss at 9845 iteration is: 0.15389787000875685\n",
            "The running loss at 9846 iteration is: 0.19107008306952794\n",
            "The running loss at 9847 iteration is: 0.27105132151472694\n",
            "The running loss at 9848 iteration is: 0.4371458927690675\n",
            "The running loss at 9849 iteration is: 0.8130514405065602\n",
            "The running loss at 9850 iteration is: 1.5910916703109952\n",
            "The running loss at 9851 iteration is: 3.4583672369712097\n",
            "The running loss at 9852 iteration is: 7.033742305860268\n",
            "The running loss at 9853 iteration is: 15.962513296988604\n",
            "The running loss at 9854 iteration is: 27.529430931531707\n",
            "The running loss at 9855 iteration is: 52.50042736970126\n",
            "The running loss at 9856 iteration is: 43.868715353595945\n",
            "The running loss at 9857 iteration is: 27.053600944374264\n",
            "The running loss at 9858 iteration is: 0.773302134689508\n",
            "The running loss at 9859 iteration is: 13.820910226789321\n",
            "The running loss at 9860 iteration is: 29.419152493259315\n",
            "The running loss at 9861 iteration is: 4.992426068553686\n",
            "The running loss at 9862 iteration is: 5.800518151396442\n",
            "The running loss at 9863 iteration is: 23.164097108186056\n",
            "The running loss at 9864 iteration is: 8.845994796656015\n",
            "The running loss at 9865 iteration is: 0.7133552202125972\n",
            "The running loss at 9866 iteration is: 11.774848799287517\n",
            "The running loss at 9867 iteration is: 12.341118311714599\n",
            "The running loss at 9868 iteration is: 3.4516588256400573\n",
            "The running loss at 9869 iteration is: 1.1182598065855252\n",
            "The running loss at 9870 iteration is: 8.014728764194663\n",
            "The running loss at 9871 iteration is: 10.056142038319463\n",
            "The running loss at 9872 iteration is: 2.3218223056824647\n",
            "The running loss at 9873 iteration is: 1.2010205466345185\n",
            "The running loss at 9874 iteration is: 6.421433351866386\n",
            "The running loss at 9875 iteration is: 5.354257603861042\n",
            "The running loss at 9876 iteration is: 1.0197497225719199\n",
            "The running loss at 9877 iteration is: 1.2736545353856743\n",
            "The running loss at 9878 iteration is: 4.0881548158346055\n",
            "The running loss at 9879 iteration is: 3.0866178081657267\n",
            "The running loss at 9880 iteration is: 0.3951362648723852\n",
            "The running loss at 9881 iteration is: 1.781156534878903\n",
            "The running loss at 9882 iteration is: 3.2221122492658965\n",
            "The running loss at 9883 iteration is: 1.0860104092602458\n",
            "The running loss at 9884 iteration is: 0.5519196935570486\n",
            "The running loss at 9885 iteration is: 2.124673337504044\n",
            "The running loss at 9886 iteration is: 1.479963184606158\n",
            "The running loss at 9887 iteration is: 0.3125843172102734\n",
            "The running loss at 9888 iteration is: 1.092451323106747\n",
            "The running loss at 9889 iteration is: 1.4021432552273592\n",
            "The running loss at 9890 iteration is: 0.4722299090254157\n",
            "The running loss at 9891 iteration is: 0.5191317711086588\n",
            "The running loss at 9892 iteration is: 1.1059385714332874\n",
            "The running loss at 9893 iteration is: 0.6514341510508316\n",
            "The running loss at 9894 iteration is: 0.3085000233999867\n",
            "The running loss at 9895 iteration is: 0.7866983387390444\n",
            "The running loss at 9896 iteration is: 0.7193801388015402\n",
            "The running loss at 9897 iteration is: 0.28510139726758477\n",
            "The running loss at 9898 iteration is: 0.5353521952975802\n",
            "The running loss at 9899 iteration is: 0.6814490723378193\n",
            "The running loss at 9900 iteration is: 0.3257700101864016\n",
            "The running loss at 9901 iteration is: 0.37237911716278826\n",
            "The running loss at 9902 iteration is: 0.5832705831710696\n",
            "The running loss at 9903 iteration is: 0.3676346454168231\n",
            "The running loss at 9904 iteration is: 0.2881837233109102\n",
            "The running loss at 9905 iteration is: 0.4704979105453862\n",
            "The running loss at 9906 iteration is: 0.38671763961848615\n",
            "The running loss at 9907 iteration is: 0.25943793474174254\n",
            "The running loss at 9908 iteration is: 0.3721613005546955\n",
            "The running loss at 9909 iteration is: 0.3798002758660979\n",
            "The running loss at 9910 iteration is: 0.2615701061416586\n",
            "The running loss at 9911 iteration is: 0.3026375934958569\n",
            "The running loss at 9912 iteration is: 0.3543955870737733\n",
            "The running loss at 9913 iteration is: 0.2740232000037069\n",
            "The running loss at 9914 iteration is: 0.2625111245496256\n",
            "The running loss at 9915 iteration is: 0.32012257869227834\n",
            "The running loss at 9916 iteration is: 0.2835364835938742\n",
            "The running loss at 9917 iteration is: 0.24535755981219384\n",
            "The running loss at 9918 iteration is: 0.2861675316789044\n",
            "The running loss at 9919 iteration is: 0.2839626704268703\n",
            "The running loss at 9920 iteration is: 0.24231284566368447\n",
            "The running loss at 9921 iteration is: 0.2592848232062723\n",
            "The running loss at 9922 iteration is: 0.2755870906580773\n",
            "The running loss at 9923 iteration is: 0.2447956552895996\n",
            "The running loss at 9924 iteration is: 0.24150865959807094\n",
            "The running loss at 9925 iteration is: 0.26157357578980756\n",
            "The running loss at 9926 iteration is: 0.2469400716780601\n",
            "The running loss at 9927 iteration is: 0.23263804541124186\n",
            "The running loss at 9928 iteration is: 0.24654027213132693\n",
            "The running loss at 9929 iteration is: 0.24558710740299297\n",
            "The running loss at 9930 iteration is: 0.22983904775187075\n",
            "The running loss at 9931 iteration is: 0.23400867874384187\n",
            "The running loss at 9932 iteration is: 0.2402220990532781\n",
            "The running loss at 9933 iteration is: 0.22959162603281902\n",
            "The running loss at 9934 iteration is: 0.22579036480632642\n",
            "The running loss at 9935 iteration is: 0.23255293296805138\n",
            "The running loss at 9936 iteration is: 0.228791813915445\n",
            "The running loss at 9937 iteration is: 0.22157575678233435\n",
            "The running loss at 9938 iteration is: 0.22458628563485966\n",
            "The running loss at 9939 iteration is: 0.22580606284666385\n",
            "The running loss at 9940 iteration is: 0.2196488475695283\n",
            "The running loss at 9941 iteration is: 0.2184266591685914\n",
            "The running loss at 9942 iteration is: 0.22112216887538316\n",
            "The running loss at 9943 iteration is: 0.21812395611948354\n",
            "The running loss at 9944 iteration is: 0.214460441320343\n",
            "The running loss at 9945 iteration is: 0.21571961635281348\n",
            "The running loss at 9946 iteration is: 0.2155391039280543\n",
            "The running loss at 9947 iteration is: 0.2120317521992025\n",
            "The running loss at 9948 iteration is: 0.21115684176636149\n",
            "The running loss at 9949 iteration is: 0.21190931006743646\n",
            "The running loss at 9950 iteration is: 0.2099716330140537\n",
            "The running loss at 9951 iteration is: 0.2077933188333391\n",
            "The running loss at 9952 iteration is: 0.20795932031925804\n",
            "The running loss at 9953 iteration is: 0.20753310653315052\n",
            "The running loss at 9954 iteration is: 0.20538382303180258\n",
            "The running loss at 9955 iteration is: 0.20430235182723913\n",
            "The running loss at 9956 iteration is: 0.20430323900218322\n",
            "The running loss at 9957 iteration is: 0.2031014909015416\n",
            "The running loss at 9958 iteration is: 0.2014362194453536\n",
            "The running loss at 9959 iteration is: 0.20091021675820128\n",
            "The running loss at 9960 iteration is: 0.20047528368513365\n",
            "The running loss at 9961 iteration is: 0.19904416445862966\n",
            "The running loss at 9962 iteration is: 0.19791988953915465\n",
            "The running loss at 9963 iteration is: 0.1974945504916014\n",
            "The running loss at 9964 iteration is: 0.19671533835143237\n",
            "The running loss at 9965 iteration is: 0.19538595861784155\n",
            "The running loss at 9966 iteration is: 0.19453155618837042\n",
            "The running loss at 9967 iteration is: 0.19398310999716661\n",
            "The running loss at 9968 iteration is: 0.1930682975110117\n",
            "The running loss at 9969 iteration is: 0.19200093258497128\n",
            "The running loss at 9970 iteration is: 0.19123942874235106\n",
            "The running loss at 9971 iteration is: 0.1905804905067383\n",
            "The running loss at 9972 iteration is: 0.1896632265991809\n",
            "The running loss at 9973 iteration is: 0.1886692839571712\n",
            "The running loss at 9974 iteration is: 0.18796237782253983\n",
            "The running loss at 9975 iteration is: 0.18723153960754083\n",
            "The running loss at 9976 iteration is: 0.1863326295428466\n",
            "The running loss at 9977 iteration is: 0.18547867551312183\n",
            "The running loss at 9978 iteration is: 0.18477034807424786\n",
            "The running loss at 9979 iteration is: 0.184031684943244\n",
            "The running loss at 9980 iteration is: 0.18317933941609385\n",
            "The running loss at 9981 iteration is: 0.18231429671611898\n",
            "The running loss at 9982 iteration is: 0.18164832977141104\n",
            "The running loss at 9983 iteration is: 0.1808722815568324\n",
            "The running loss at 9984 iteration is: 0.18009917955943083\n",
            "The running loss at 9985 iteration is: 0.17932709780080922\n",
            "The running loss at 9986 iteration is: 0.17858704580063778\n",
            "The running loss at 9987 iteration is: 0.177871483838845\n",
            "The running loss at 9988 iteration is: 0.1771141963319687\n",
            "The running loss at 9989 iteration is: 0.17631053170527158\n",
            "The running loss at 9990 iteration is: 0.17559991490504562\n",
            "The running loss at 9991 iteration is: 0.17491152546825472\n",
            "The running loss at 9992 iteration is: 0.17416260742610343\n",
            "The running loss at 9993 iteration is: 0.1734191215493518\n",
            "The running loss at 9994 iteration is: 0.1727310904872741\n",
            "The running loss at 9995 iteration is: 0.17199075917133597\n",
            "The running loss at 9996 iteration is: 0.17127244886439558\n",
            "The running loss at 9997 iteration is: 0.17056879365994174\n",
            "The running loss at 9998 iteration is: 0.16986871359735692\n",
            "The running loss at 9999 iteration is: 0.16919240266448357\n",
            "The running loss at 10000 iteration is: 0.16849587714653302\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8813876647108546e-05\n"
          ]
        }
      ],
      "source": [
        "run_train(lr=0.03, num_e= 10000, isOn=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SYadZRcw0TVB"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uBsYrR_QHC0q"
      },
      "outputs": [],
      "source": [
        "# save weights\n",
        "# filepath = '/content/drive/My Drive/Colab Notebooks/Project_PINNs/weight_working_ex01.txt'\n",
        "# torch.save(model.state_dict(), filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1gm6SynA4Ufq"
      },
      "outputs": [],
      "source": [
        "# #Later to restore:\n",
        "# model.load_state_dict(torch.load(filepath))\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CDTic9mj5Xec"
      },
      "outputs": [],
      "source": [
        "def evaluate_np(x_data_np):\n",
        "    u_hat = []\n",
        "    for x in x_data_np:\n",
        "        x_tensor = torch.tensor(np.array([x]), requires_grad= True)\n",
        "        temp = model(x_tensor.float())\n",
        "        u_hat.append(temp.clone().item())\n",
        "        # u_hat = torch.hstack([u_hat, temp])\n",
        "    return u_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9uoUTEaZ7DJz"
      },
      "outputs": [],
      "source": [
        "def evaluate_error_np(x_data_np):\n",
        "    y_pred = evaluate_np(x_data_np)\n",
        "    y_true = u_star_func(x_data_np)\n",
        "    return np.mean(np.square(y_true -  y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "EQ7z_1NV4_LG",
        "outputId": "97f53937-685c-4ce9-a912-da8bdb2d8a1f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "6.294729403435639e-06"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_init_np2 = np.arange(start=a, stop=b+step/2, step=step/2)\n",
        "x_init2 = torch.tensor(x_init_np2, requires_grad= True)\n",
        "# print(x_init)\n",
        "# print(x_init2.shape)\n",
        "y_pred2 = evaluate_np(x_init_np2)\n",
        "\n",
        "draw_result(x_init_np2, y_pred2, 'solution-NN')\n",
        "evaluate_error_np(x_init_np2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "UU5oKmgB9yiq",
        "outputId": "e8b9d532-b403-4730-9732-1bfe8d83397f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "draw_result(x_init_np2, u_star_func(x_init_np2), 'solution-NN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fc5-liOp8099"
      },
      "outputs": [],
      "source": [
        "step_size = []\n",
        "error = []\n",
        "for k in [1,2,3,4,5,6,7]:\n",
        "    x_vec = np.arange(start=a, stop=b+step/k, step=step/k)\n",
        "    # x_vec_tensor = torch.tensor(x_vec, requires_grad= True)\n",
        "    y_pred = evaluate_np(x_vec)\n",
        "    error.append(evaluate_error_np(x_vec))\n",
        "    step_size.append(step/k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "erpPFfAS9-jC",
        "outputId": "6388f53a-9943-424b-d3d4-84cbf5eeef18"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.01, 0.005, 0.0033333333333333335, 0.0025, 0.002, 0.0016666666666666668, 0.0014285714285714286]\n"
          ]
        }
      ],
      "source": [
        "draw_result(step_size[::-1], error[::-1], 'error')\n",
        "print(step_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oreTdjj9rjCW"
      },
      "source": [
        "# Appendix: Understanding pytorch gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUxBjI6nrnpA"
      },
      "source": [
        "We create two vectors\n",
        "\n",
        "$$a = \\begin{bmatrix} {2\\\\3}\n",
        "\\end{bmatrix}, \\qquad\\text{and}\\qquad b = \\begin{bmatrix} {6\\\\4}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Let us consider the function\n",
        "\n",
        "$$ Q(x,y) = 3x^3 - y^2$$\n",
        "\n",
        "The convention here is that, if \n",
        "$$\\mathbf{x} = \\begin{bmatrix} {x_1\\\\x_2\\\\ \\vdots\\\\ x_n}\n",
        "\\end{bmatrix} \\qquad\\text{and}\\qquad \\mathbf{y} = \\begin{bmatrix} {y_1\\\\y_2\\\\ \\vdots\\\\ y_n}\n",
        "\\end{bmatrix}$$\n",
        "then\n",
        "the action of $Q$ on $\\mathbf{x}, \\mathbf{y}$ are component-wise, i.e.,\n",
        "\n",
        "$$ Q(\\mathbf{x},\\mathbf{y}) = \\begin{bmatrix} {Q(x_1,y_1)\\\\ Q(x_2,y_2)\\\\ \\vdots\\\\ Q(x_n,y_n)}\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "In `pytorch`, this is saved as a tensor object."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oreTdjj9rjCW"
      ],
      "name": "Working example 03.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
